@inproceedings{Good2001,
author = {Good, Michael},
booktitle = {XML Conference hosted by IDEAlliance},
doi = {10.1.1.118.5431},
file = {:home/a108210/Dropbox/mendeley sync/2001\_MusicXML An Internet-Friendly Format for Sheet Music\_Good.pdf:pdf},
title = {{MusicXML: An Internet-Friendly Format for Sheet Music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.118.5431},
year = {2001}
}
@misc{Joachims2008,
annote = {cornell lib },
author = {Joachims, Thorsten},
title = {{SVM\^{}hmm: Sequence Tagging with Structural Support Vector Machines}},
url = {http://www.cs.cornell.edu/people/tj/svm\_light/svm\_hmm.html},
year = {2008}
}
@article{Joachims2009,
annote = {SVM-HMM original paper},
author = {Joachims, Thorsten and Finley, Thomas and Yu, CNJ},
doi = {10.1007/s10994-009-5108-8},
file = {:home/a108210/Dropbox/mendeley sync/2009\_Cutting-plane training of structural SVMs\_Joachims, Finley, Yu.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {structural svms,structured output predic-,support vector machines,tion,training algorithms},
month = may,
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5108-8 http://link.springer.com/article/10.1007/s10994-009-5108-8},
volume = {77},
year = {2009}
}
@book{Selfridge-Field1997,
abstract = {"This volume, likely to become a standard reference work, describes an extraordinary number of approaches to the representation of musical information for purposes of computer processing. It is a considerable achievement, for it sorts and orders work in a confusing and sometimes embattled field, analyzing each encoding method in logical sequence and in light of the specific purposes for which it was designed." -- Raymond Erickson, Dean of Arts and Humanities, Queens College, CUNY; author of "DARMS: A Reference Manual" The establishment of the Musical Instrument Digital Interface (MIDI) in the late 1980s allowed hobbyists and musicians to experiment with sound control in ways that previously had been possible only in research studios. MIDI is now the most prevalent representation of music, but what it represents is based on hardware control protocols for sound synthesis. Programs that support sound input for graphics output necessarily span a gamut of representational categories. What is most likely to be lost is any sense of the musical work. Thus, for those involved in pedagogy, analysis, simulation, notation, and music theory, the nature of the representation matters a great deal. An understanding of the data requirements of different applications is fundamental to the creation of interchange codes. The contributors to "Beyond MIDI" present a broad range of schemes, illustrating a wide variety of approaches to music representation. Generally, each chapter describes the history and intended purposes of the code, a description of the representation of the primary attributes of music (pitch, duration, articulation, ornamentation, dynamics, andtimbre), a description of the file organization, some mention of existing data in the format, resources for further information, and at least one encoded example. The book also shows how intended applications influence the kinds of musical information that are encoded. Contributors: David Bainbridge, Ulf Berggren, Roger D. Boyle, Donald Byrd, David Cooper, Edmund Correia, Jr., David Cottle, Tim Crawford, J. Stephen Dydo, Brent A. Field, Roger Firman, John Gibson, Cindy Grande, Lippold Haken, Thomas Hall, David Halperin, Philip Hazel, Walter B. Hewlett, John Howard, David Huron, Werner Icking, David Jaffe, Bettye Krolick, Max V. Mathews, Toshiaki Matsushima, Steven R. Newcomb, Kia-Chuan Ng, Kjell E. Nordli, Sile O'Modhrain, Perry Roland, Helmut Schaffrath, Bill Schottstaedt, Eleanor Selfrdige-Field, Peer Sitter, Donald Sloan, Leland Smith, Andranick Tanguiane, Lynn M. Trowbridge, Frans Wiering.},
author = {Selfridge-Field, Eleanor},
isbn = {0262193949},
publisher = {MIT Press},
title = {{Beyond MIDI: The Handbook of Musical Codes}},
url = {http://books.google.com.tw/books/about/Beyond\_MIDI.html?id=Xm3J9DG9EFcC\&pgis=1},
year = {1997}
}
@misc{KernScores,
title = {{KernScores}},
url = {http://kern.ccarh.org/},
howpublished= {http://kern.ccarh.org/}
}
@misc{LilyPond,
title = {{LilyPond}},
url = {http://www.lilypond.org},
howpublished = {http://www.lilypond.org}
}
@inproceedings{Lyu2012,
author = {Lyu, Shing Hermes and Jeng, Shyh-kang},
booktitle = {workshop on Computer Music and Audio Technology},
title = {{COMPUTER EXPRESSIVE MUSIC PERFORMANCE BY PHRASE-WISE MODELING}},
year = {2012}
}
@misc{zenph,
booktitle = {Zenph Music},
title = {{Rachmianinoff - Plays Rachmaninoff}},
url = {https://www.zenph.com/rachmaninoff-plays-rachmaninoff},
howpublished= {https://www.zenph.com/rachmaninoff-plays-rachmaninoff},
urldate = {2014/2/11},
year = {2009}
}
@incollection{THEBOOK,
author = {Kirke, Alexis and Miranda, Eduardo R.},
booktitle = {Guide to Computing for Expressive Music Performance},
editor = {Kirke, Alexis and Miranda, Eduardo R.},
isbn = {978-1-4471-4123-5},
pages = {1--47},
publisher = {Springer},
title = {{An Overview of Computer Systems for Expressive Music Performance}},
year = {2013}
}
@inproceedings{RenCon,
address = {Hamatsu, Japan},
annote = {RenCon
},
author = {Hiraga, R and Bresin, R and Hirata, K and KH, RenCon},
booktitle = {Proceedings of 2004 new interfaces for musical expression conference},
editor = {Nagashima, Y and Lyons, M},
pages = {120-123},
publisher = {ACM Press},
title = {{Turing test for musical expression proceedings of international conference on new interfaces for musical expression}},
year = {2004}
}

@article{18,
abstract = {Presented is a model of rubato, implemented in Lisp, in which expression is viewed as the mapping of musical structure into the variables of expression. The basic idea is that the performer uses ?phrase final lengthening? as a device to reflect some internal representation of the phrase structure. The representation is based on Lardahl and Jackendoff's time-span reduction. The basic heuristic in the model is recursive involving look-ahead and planning at a number of levels. The planned phrasings are superposed beat by beat and the output from the program is a list of durations which could easily be adapted to be sent to a synthesiser given a suitable system.
Presented is a model of rubato, implemented in Lisp, in which expression is viewed as the mapping of musical structure into the variables of expression. The basic idea is that the performer uses ?phrase final lengthening? as a device to reflect some internal representation of the phrase structure. The representation is based on Lardahl and Jackendoff's time-span reduction. The basic heuristic in the model is recursive involving look-ahead and planning at a number of levels. The planned phrasings are superposed beat by beat and the output from the program is a list of durations which could easily be adapted to be sent to a synthesiser given a suitable system.},
author = {Todd, Neil P. McAngus},
doi = {10.1080/07494468900640061},
issn = {0749-4467},
journal = {Contemporary Music Review},
month = jan,
number = {1},
pages = {69--88},
publisher = {Routledge},
title = {{A computational model of rubato}},
url = {http://dx.doi.org/10.1080/07494468900640061},
volume = {3},
year = {1989}
}
@article{31,
abstract = {Abstract Emotions are a key part of creative endeavours, and a core problem for computational models of creativity. In this paper we discuss an affective computing architecture for the dynamic modification of music with a view to predictablyaffectinginducedmusicalemotions. Extending previous work on the modification of perceived emotions in music, our system architecture aims to provide reliable control of both perceived and induced musical emotions: its emotionality. A rule-based system is used to modify a subset of musical features at two processing levels, namely score and performance. The interactive model leverages sensed listener affect by adapting the emotionality of the music modifications in real-time to assist the listener in reaching a desired emotional state.
Abstract Emotions are a key part of creative endeavours, and a core problem for computational models of creativity. In this paper we discuss an affective computing architecture for the dynamic modification of music with a view to predictablyaffectinginducedmusicalemotions. Extending previous work on the modification of perceived emotions in music, our system architecture aims to provide reliable control of both perceived and induced musical emotions: its emotionality. A rule-based system is used to modify a subset of musical features at two processing levels, namely score and performance. The interactive model leverages sensed listener affect by adapting the emotionality of the music modifications in real-time to assist the listener in reaching a desired emotional state.},
annote = {31},
author = {Livingstone, Steven R. and M\"{u}hlberger, Ralf and Brown, Andrew R. and Loch, Andrew},
doi = {10.1080/14626260701253606},
issn = {1462-6268},
journal = {Digital Creativity},
month = mar,
number = {1},
pages = {43--53},
publisher = {Routledge},
title = {{Controlling musical emotionality: an affective computational architecture for influencing musical emotions}},
url = {http://dx.doi.org/10.1080/14626260701253606},
volume = {18},
year = {2007}
}
@book{29,
abstract = {The only things truly universal in music are those that are based on biological and/or perceptual facts. Tuning Timbre Spectrum Scale focuses on perceptions of consonance and dissonance, which are defined in the Harvard Dictionary of Music: "Consonance is used to describe the agreeable effect produced by certain intervals as against the disagreeable effect produced others. Consonance and dissonance are the very foundation of harmonic music... consonance represents the element of smoothness and repose, while dissonance represents the no less important elements of roughness and irregularity.” Tuning Timbre Spectrum Scale begins by asking (and answering) the question: How can we build a device to measure consonance and dissonance? The remainder of the book describes the impact of such a "dissonance meter” on music theory, on synthesizer design, on the construction of musical scales and tunings, on the design of musical instruments, and introduces related compositional techniques and new methods of musicological analyses. A new chapter contains a detailed explanation of how the software works. It incorporates several important simplifications over the full presentation in the current Chapter 7 in order to allow it to function in real time. Another new chapter describes the various ways that the software can be used. New sections throughout the book bring it up to date with the current state of the subject. Tuning Timbre Spectrum Scale offers a unique analysis of the relationship between the structure of sound and the structure of scale and will be useful to musicians and composers who use inharmonic tones and sounds. This includes a large percentage of people composing and performing with modern musical synthesizers. It will be of use to arrangers, musicologists, and others interested in musical analysis. Tuning Timbre Spectrum Scale provides a unique approach to working with environmental sounds, and there are clear applications for the use of inharmonic sounds in film scoring. The book will also be of interest to engineers and others interested in the design of audio devices such as musical synthesizers, special effects devices, and keyboards.},
annote = {29},
author = {Sethares, William A.},
isbn = {1852337974},
publisher = {Springer},
title = {{Tuning, Timbre, Spectrum, Scale}},
url = {http://books.google.com.tw/books/about/Tuning\_Timbre\_Spectrum\_Scale.html?id=KChoKKhjOb0C\&pgis=1},
year = {2005}
}
@misc{sibelius,
title = {{Sibelius}},
url = {http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share},
howpublished = {http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share}
}
@article{17,
author = {Friberg, Anders and Bresin, Roberto and Sundberg, Johan},
doi = {10.2478/v10053-008-0052-x},
file = {:home/a108210/文件/Mendeley Desktop/2006 - Overview of the KTH rule system for musical performance - Friberg, Bresin, Sundberg.pdf:pdf},
issn = {1895-1171},
journal = {Advances in Cognitive Psychology},
keywords = {expression,music performance modeling,rule system},
month = jan,
number = {2},
pages = {145--161},
title = {{Overview of the KTH rule system for musical performance}},
url = {http://versita.metapress.com/openurl.asp?genre=article\&id=doi:10.2478/v10053-008-0052-x},
volume = {2},
year = {2006}
}

@article{19,
abstract = {A computational model of musical dynamics is proposed that complements an earlier model of expressive timing. The model, implemented in the artificial intelligence language LISP, is based on the observation that a musical phrase is often indicated by a crescendo/decrescendo shape. The functional form of this shape is derived by making two main assumptions. First, that musical dynamics and tempo are coupled, that is, ‘‘the faster the louder, the slower the softer.’’ This tempo/dynamics coupling, it is suggested, may be a characteristic of some classical and romantic styles perhaps exemplified by performances of Chopin. Second, that the tempo change is governed by analogy to physical movement. The allusion of musical expression to physical motion is further extended by the introduction of the concepts of energy and mass. The utility of the model, in addition to giving an insight into the nature of musical expression, is that it provides a basis for a method of performance style analysis.},
author = {{McAngus Todd}, Neil P.},
doi = {10.1121/1.402843},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
month = jun,
number = {6},
pages = {3540},
publisher = {Acoustical Society of America},
title = {{The dynamics of dynamics: A model of musical expression}},
url = {http://scitation.aip.org/content/asa/journal/jasa/91/6/10.1121/1.402843},
volume = {91},
year = {1992}
}
@article{20,
abstract = {The notion that there is an intimate relationship between musical motion and physical movement is an old one and can be traced back to antiquity. Recently this idea has again received some attention, particularly in relation to musical expression. To use a modern metaphor, one can consider expressive performance to be analogous to the problems of kinematics and trajectory planning in robotics. The trajectories referred to, however, are not those of the performers limbs in physical space, but those of an abstract movement relative to a metrical grid associated with a musical score. Recent studies have attempted to substantiate this idea by comparing a model of motion with timing measurements of the final ritardandi from actual performances. This study extends these earlier analyses to include the accelerandi as well as the ritardandi from complete performances. One conclusion is that the variation of tempo in music can be reasonably compared with velocity in the equations of elementary mechanics. Further, it is suggested that the origin of metrical space, upon which the motion concept rests, lies in the way the auditory system processes rhythm.},
annote = {20
},
author = {Todd, Neil P. McAngus},
doi = {10.1121/1.412067},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
month = mar,
number = {3},
pages = {1940},
publisher = {Acoustical Society of America},
title = {{The kinematics of musical expression}},
url = {http://scitation.aip.org/content/asa/journal/jasa/97/3/10.1121/1.412067},
volume = {97},
year = {1995}
}
@article{21,
author = {Clynes, M},
journal = {Journal For The Integrated Study Of Artificial Intelligence},
title = {Generative principles of musical thought: Integration of microstructure with structure},
url = {http://ivizlab.sfu.ca/arya/Papers/Others/Generative Principles of Musical Thought.pdf},
year = {1986}
}
@article{22,
author = {Clynes, M},
journal = {Cognition},
title = {{Microstructural musical linguistics: composers' pulses are liked most by the best musicians}},
url = {http://www.sciencedirect.com/science/article/pii/001002779400650A},
year = {1995}
}

@article{26,
author = {Mazzola, G and Zahorka, O},
journal = {Computer Music Journal},
number = {1},
pages = {40--52},
title = {{Tempo curves revisited: Hierarchies of performance fields}},
url = {http://www.jstor.org/stable/3680521},
volume = {18},
year = {1994}
}
@book{27,
address = {Basel/Boston},
author = {Mazzola, G},
isbn = {3764357312},
publisher = {Birkh\"{a}user},
title = {{The Topos of Music: Geometric Logic of Concepts, Theory, and Performance}},
url = {http://www.amazon.com/The-Topos-Music-Geometric-Performance/dp/3764357312},
year = {2002}
}
@inproceedings{25,
address = {Ann Arbor, Michigan},
author = {Dannenberg, Roger B. and Pellerin, Hank and Derenyi, Itsvan},
booktitle = {Proceedings of the 1998 international computer music conference},
editor = {1998, October},
pages = {57--61},
publisher = {International Computer Music Association},
title = {{A Study of Trumpet Envelopes}},
url = {http://repository.cmu.edu/compsci/501},
year = {1998}
}
@article{24,
abstract = {Abstract Convincing synthesis of wind instruments requires more than the reproduction of individual tones. Since the player exerts continuous control over amplitude, frequency, and other parameters, it is not adequate to store simple templates for individual tones and string them together to make phrases. Transitions are important, and the details of a tone are affected by context. To address these problems, we present an approach to music synthesis that relies on a performance model to generate musical control signals and an instrument model to generate appropriate time?varying spectra. This approach is carefully designed to facilitate model construction from recorded examples of acoustic performances. We report on our experience developing a system to synthesize trumpet performances from a symbolic score input.
Abstract Convincing synthesis of wind instruments requires more than the reproduction of individual tones. Since the player exerts continuous control over amplitude, frequency, and other parameters, it is not adequate to store simple templates for individual tones and string them together to make phrases. Transitions are important, and the details of a tone are affected by context. To address these problems, we present an approach to music synthesis that relies on a performance model to generate musical control signals and an instrument model to generate appropriate time?varying spectra. This approach is carefully designed to facilitate model construction from recorded examples of acoustic performances. We report on our experience developing a system to synthesize trumpet performances from a symbolic score input.},
author = {Dannenberg, Roger B. and Derenyi, Istvan},
doi = {10.1080/09298219808570747},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = sep,
number = {3},
pages = {211--238},
publisher = {Routledge},
title = {{Combining instrument and performance models for high-quality music synthesis}},
url = {http://dx.doi.org/10.1080/09298219808570747},
volume = {27},
year = {1998}
}
@article{23,
abstract = {The development and implementation of an expert system that determines the tempo and articulations of Bach fugues are described. The rules in the knowledge base are based on the expertise of two professional performers. The system's input is a numeric representation of the fugue. The system processes the input using a transition graph, a data structure consisting of nodes where data is stored and edges that connect the nodes. The transition graph recognizes rhythmic patterns in the input. Once the system identifies a pattern, it applies a specific rule or performs a procedure. System output consists of a listing of tempo and articulation instructions. To validate the expert system, its output was compared with versions of fugues edited by one of the two experts used in developing the system. In tests with six fugues, the expert system generated the same editing instructions 85 to 90\% of the time.},
annote = {23

      },
author = {Johnson, M.L.},
doi = {10.1109/2.84832},
issn = {0018-9162},
journal = {Computer},
month = jul,
number = {7},
pages = {30--34},
shorttitle = {Computer},
title = {{Toward an expert system for expressive musical performance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=84832},
volume = {24},
year = {1991}
}
@inproceedings{32,
annote = {32},
author = {Katayose, H. and Fukuoka, T. and Takami, K. and Inokuchi, S.},
booktitle = {Proceedings of 10th International Conference on Pattern Recognition},
year={1990},
doi = {10.1109/ICPR.1990.118216},
isbn = {0-8186-2062-5},
language = {English},
pages = {780--784},
publisher = {IEEE Comput. Soc. Press},
title = {{Expression extraction in virtuoso music performances}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=118216},
volume = {i}
}
@article{33,
abstract = {33},
annote = {33},
author = {Katayose, H and Fukuoka, T and Takami, K and Inokuchi, S},
journal = {Journal of Information Processing Society of Japan},
number = {38},
pages = {1473--1481},
title = {{Extraction of expression parameters with multiple regression analysis}},
year = {1997}
}
@inproceedings{Ishikawa2000,
abstract = {34},
address = {Berlin},
author = {Ishikawa, Osamu and Aono, Yushi and Katayose, Haruhiro and Inokuchi, Seiji},
booktitle = {International Computer Music Conference Proceedings},
pages = {348--351},
publisher = {International Computer Music Association},
title = {{Extraction of Musical Performance Rules Using a Modified Algorithm of Multiple Regression Analysis}},
volume = {2000},
year = {2000}
}
@article{35,
author = {Canazza, Sergio and {De Poli}, Giovanni and Drioli, Carlo and Rod\`{a}, Antonio and Vidolin, Alvise},
issn = {1070-986X},
journal = {IEEE MultiMedia},
month = jul,
number = {3},
pages = {79--83},
publisher = {IEEE Computer Society Press},
title = {{Audio Morphing Different Expressive Intentions for Multimedia Systems}},
url = {http://dl.acm.org/citation.cfm?id=614667.614998},
volume = {7},
year = {2000}
}
@article{36,
author = {Canazza, S. and Vidolin, A. and {De Poli}, G. and Drioli, C. and Rod\`{a}, A.},
isbn = {0-7695-1284-4},
month = nov,
pages = {116},
publisher = {IEEE Computer Society},
title = {{Expressive Morphing for Interactive Performance of Musical Scores}},
url = {http://dl.acm.org/citation.cfm?id=882508.885257},
year = {2001}
}
@article{37,
abstract = {Expressiveness is not an extravagance: instead, expressiveness plays a critical role in rational decision-making, in perception, in human interaction, in human emotions and in human intelligence. These facts, combined with the development of new informatics systems able to recognize and understand different kinds of signals, open new areas for research. A new model is suggested for computer understanding of sensory expressive intentions of a human performer and both theoretical and practical applications are described for human-computer interaction, perceptual information retrieval, creative arts and entertainment. Recent studies demonstrated that by opportunely modifying systematic deviations introduced by the musician it is possible to convey different sensitive contents, such as expressive intentions and/or emotions. We present an space, that can be used as a user interface. It represents, at an abstract level, the expressive content and the interaction between the performer and an expressive synthesizer.
Expressiveness is not an extravagance: instead, expressiveness plays a critical role in rational decision-making, in perception, in human interaction, in human emotions and in human intelligence. These facts, combined with the development of new informatics systems able to recognize and understand different kinds of signals, open new areas for research. A new model is suggested for computer understanding of sensory expressive intentions of a human performer and both theoretical and practical applications are described for human-computer interaction, perceptual information retrieval, creative arts and entertainment. Recent studies demonstrated that by opportunely modifying systematic deviations introduced by the musician it is possible to convey different sensitive contents, such as expressive intentions and/or emotions. We present an space, that can be used as a user interface. It represents, at an abstract level, the expressive content and the interaction between the performer and an expressive synthesizer.},
author = {Canazza, Sergio and {De Poli}, Giovanni and Rod\`{a}, Antonio and Vidolin, Alvise},
doi = {10.1076/jnmr.32.3.281.16862},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = sep,
number = {3},
pages = {281--294},
publisher = {Routledge},
title = {{An Abstract Control Space for Communication of Sensory Expressive Intentions in Music Performance}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.3.281.16862},
volume = {32},
year = {2003}
}
@inproceedings{39,
address = {L'Aquila, Italy},
author = {Camurri, A and Dillon, Roberto and Saron, A},
booktitle = {Proceedings of 13th colloquium on musical informatics},
title = {{An experiment on analysis and synthesis of musical expressivity}},
year = {2000}
}

@inproceedings{57,
author = {Dorard, L and Hardoon, DR and Shawe-Taylor, J},
booktitle = {Proceedings of the Music, Brain and Cognition Workshop -- Neural Information Processing Systems},
publisher = {Whistler, Canada},
title = {{Can style be learned? A machine learning approach towards ‘performing’as famous pianists.}},
year = {2007}
}
@inproceedings{52,
author = {Raphael, C},
booktitle = {Proceedings of the 8th Int. Workshop on Artificial Intelligence and Statistics},
editor = {Jaakkola, T and Richardson, T},
file = {::},
pages = {113--120},
publisher = {Morgan Kaufmann, San Francisco},
title = {{Can the computer learn to play music expressively?}},
url = {http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/raphael\_perf.pdf},
year = {2001}
}
@article{53,
author = {Raphael, C},
journal = {Neural Information Processing Systems},
number = {14},
pages = {1433--1440},
title = {{A Bayesian Network for Real-Time Musical Accompaniment.}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AP01.ps.gz},
year = {2001}
}
@inproceedings{54,
address = {Acapulco, Mexico},
author = {Raphael, C},
booktitle = {Proceedings of 2003 International Joint conference on Artifical Intelligence (Working Notes of IJCAI-03 Rencon Workshop)},
editor = {Gottob, G and Walsh, T},
file = {::},
pages = {5--10},
publisher = {Morgan Kaufmann, San Francisco},
title = {{Orchestra in a box: A system for real-time musical accompaniment}},
url = {http://renconmusic.org/ijcai2003/IJCAI\_Rencon2003\_FINALLAST.pdf\#page=11},
year = {2003}
}
@phdthesis{55,
author = {Grindlay, GC},
school = {University of Santa Cruz, CA},
title = {{Modeling expressive musical performance with Hidden Markov Models}},
type = {PhD Thesis},
url = {http://scholar.google.com.tw/scholar?hl=zh-TW\&as\_sdt=0,5\&q=grindlay+gc+modelling+expressive+musical+performance+with+hidden+markov+models\#0},
year = {2005}
}

@inproceedings{89,
address = {Jinan, China},
author = {Zhang, Q and Miranda, ER},
booktitle = {Proceedings of the 6th International Conference on Intelligent Systems Design and Applications},
editor = {Chen, Y and Abraham, A},
pages = {1189--1194},
publisher = {IEEE Computer Society, Washington, DC},
title = {{Towards an evolution model of expressive music performance}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4021833},
year = {2006}
}
@inproceedings{88,
address = {Clearwater Beach, FL, USA},
author = {Ramirez, R and Hazan, A},
booktitle = {Proceedings of 18th international Florida Artificial Intelligence Research Society Sonference (AI in Music and Art)},
pages = {86--91},
publisher = {AAAI Press, Menlo Park},
title = {{Modeling Expressive Music Performance in Jazz.}},
url = {http://www.aaai.org/Papers/FLAIRS/2005/Flairs05-015.pdf},
year = {2005}
}
@inproceedings{82,
address = {New Orleans, USA},
author = {Wright, M and Berdahl, E},
booktitle = {Proceedings of the 2006 International Computer Music Conference},
editor = {Zannos, I},
file = {::},
pages = {572--575},
publisher = {ICMA, San Francisco},
title = {{Towards machine learning of expressive microtiming in Brazilian drumming}},
url = {https://ccrma.stanford.edu/~eberdahl/Projects/Microtiming/ML-Microtiming-ICMC.pdf},
year = {2006}
}

@article{42,
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez},
doi = {10.1023/A:1008311209823},
issn = {1573-7497},
journal = {Journal of Applied Intelligence},
language = {en},
month = jan,
number = {1},
pages = {115--129},
publisher = {Kluwer Academic Publishers},
title = {{An Interactive Case-Based Reasoning Approach for Generating Expressive Music}},
url = {http://link.springer.com/article/10.1023/A\%3A1008311209823},
volume = {14},
year = {2001}
}
@article{60,
author = {Miranda, ER and Kirke, A and Zhang, Q},
file = {::},
journal = {Computer Music Journal},
number = {1},
pages = {80--96},
title = {{Artificial evolution of expressive performance of music: An imitative multi-agent systems approach}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/comj.2010.34.1.80},
volume = {34},
year = {2010}
}
@inproceedings{59,
address = {London, UK},
author = {Ramirez, R and Hazan, A},
booktitle = {Proceedings of 2007 annual conference on Genetic and evolutionary computation},
file = {::},
publisher = {ACM Press, New York},
title = {{Inducing a generative expressive performance model using a sequential-covering genetic algorithm}},
url = {http://dl.acm.org/citation.cfm?id=1277374},
year = {2007}
}
@inproceedings{42,
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez},
booktitle = {Proceedings of the Workshop on Current Research Directions in Computer Music},
pages = {17--22},
title = {{The SaxEx system for expressive music synthesis: A progress report}},
year = {2001}
}
@inproceedings{40,
address = {Thessalonikia, Greece},
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez and Serra, Xavier},
booktitle = {Proceedings of 1997 International Computer Music Conference},
editor = {Cook, PR},
pages = {329--336},
publisher = {ICMA, San Francisco},
title = {{X. Serra, 1997.“SaxEx: a case-based reasoning system for generating expressive musical performances”}},
year = {1997}
}
@article{41,
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez and Serra, Xavier},
journal = {Journal of New Music Research},
number = {3},
pages = {194--210},
publisher = {Taylor \& Francis},
title = {{Saxex: A case-based reasoning system for generating expressive musical performances}},
volume = {27},
year = {1998}
}
