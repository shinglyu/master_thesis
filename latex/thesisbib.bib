@inproceedings{Good2001,
author = {Good, Michael},
booktitle = {XML Conference hosted by IDEAlliance},
doi = {10.1.1.118.5431},
file = {:home/a108210/Dropbox/mendeley sync/2001\_MusicXML An Internet-Friendly Format for Sheet Music\_Good.pdf:pdf},
title = {{MusicXML: An Internet-Friendly Format for Sheet Music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.118.5431},
year = {2001}
}
@misc{Joachims2008,
annote = {cornell lib },
author = {Joachims, Thorsten},
title = {{SVM\^{}hmm: Sequence Tagging with Structural Support Vector Machines}},
url = {http://www.cs.cornell.edu/people/tj/svm\_light/svm\_hmm.html},
year = {2008}
}
@article{Joachims2009,
annote = {SVM-HMM original paper},
author = {Joachims, Thorsten and Finley, Thomas and Yu, CNJ},
doi = {10.1007/s10994-009-5108-8},
file = {:home/a108210/Dropbox/mendeley sync/2009\_Cutting-plane training of structural SVMs\_Joachims, Finley, Yu.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {structural svms,structured output predic-,support vector machines,tion,training algorithms},
month = may,
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5108-8 http://link.springer.com/article/10.1007/s10994-009-5108-8},
volume = {77},
year = {2009}
}
@book{Selfridge-Field1997,
abstract = {"This volume, likely to become a standard reference work, describes an extraordinary number of approaches to the representation of musical information for purposes of computer processing. It is a considerable achievement, for it sorts and orders work in a confusing and sometimes embattled field, analyzing each encoding method in logical sequence and in light of the specific purposes for which it was designed." -- Raymond Erickson, Dean of Arts and Humanities, Queens College, CUNY; author of "DARMS: A Reference Manual" The establishment of the Musical Instrument Digital Interface (MIDI) in the late 1980s allowed hobbyists and musicians to experiment with sound control in ways that previously had been possible only in research studios. MIDI is now the most prevalent representation of music, but what it represents is based on hardware control protocols for sound synthesis. Programs that support sound input for graphics output necessarily span a gamut of representational categories. What is most likely to be lost is any sense of the musical work. Thus, for those involved in pedagogy, analysis, simulation, notation, and music theory, the nature of the representation matters a great deal. An understanding of the data requirements of different applications is fundamental to the creation of interchange codes. The contributors to "Beyond MIDI" present a broad range of schemes, illustrating a wide variety of approaches to music representation. Generally, each chapter describes the history and intended purposes of the code, a description of the representation of the primary attributes of music (pitch, duration, articulation, ornamentation, dynamics, andtimbre), a description of the file organization, some mention of existing data in the format, resources for further information, and at least one encoded example. The book also shows how intended applications influence the kinds of musical information that are encoded. Contributors: David Bainbridge, Ulf Berggren, Roger D. Boyle, Donald Byrd, David Cooper, Edmund Correia, Jr., David Cottle, Tim Crawford, J. Stephen Dydo, Brent A. Field, Roger Firman, John Gibson, Cindy Grande, Lippold Haken, Thomas Hall, David Halperin, Philip Hazel, Walter B. Hewlett, John Howard, David Huron, Werner Icking, David Jaffe, Bettye Krolick, Max V. Mathews, Toshiaki Matsushima, Steven R. Newcomb, Kia-Chuan Ng, Kjell E. Nordli, Sile O'Modhrain, Perry Roland, Helmut Schaffrath, Bill Schottstaedt, Eleanor Selfrdige-Field, Peer Sitter, Donald Sloan, Leland Smith, Andranick Tanguiane, Lynn M. Trowbridge, Frans Wiering.},
author = {Selfridge-Field, Eleanor},
isbn = {0262193949},
publisher = {MIT Press},
title = {{Beyond MIDI: The Handbook of Musical Codes}},
url = {http://books.google.com.tw/books/about/Beyond\_MIDI.html?id=Xm3J9DG9EFcC\&pgis=1},
year = {1997}
}
@misc{KernScores,
title = {{KernScores}},
url = {http://kern.ccarh.org/},
howpublished= {http://kern.ccarh.org/}
}
@misc{LilyPond,
title = {{LilyPond}},
url = {http://www.lilypond.org},
howpublished = {http://www.lilypond.org}
}
@inproceedings{Lyu2012,
author = {Lyu, Shing Hermes and Jeng, Shyh-kang},
booktitle = {workshop on Computer Music and Audio Technology},
title = {{COMPUTER EXPRESSIVE MUSIC PERFORMANCE BY PHRASE-WISE MODELING}},
year = {2012}
}
@misc{zenph,
booktitle = {Zenph Music},
title = {{Rachmianinoff - Plays Rachmaninoff}},
url = {https://www.zenph.com/rachmaninoff-plays-rachmaninoff},
howpublished= {https://www.zenph.com/rachmaninoff-plays-rachmaninoff},
urldate = {2014/2/11},
year = {2009}
}
@incollection{THEBOOK,
author = {Kirke, Alexis and Miranda, Eduardo R.},
booktitle = {Guide to Computing for Expressive Music Performance},
editor = {Kirke, Alexis and Miranda, Eduardo R.},
isbn = {978-1-4471-4123-5},
pages = {1--47},
publisher = {Springer},
title = {{An Overview of Computer Systems for Expressive Music Performance}},
year = {2013}
}
@inproceedings{RenCon,
address = {Hamatsu, Japan},
annote = {RenCon
},
author = {Hiraga, R and Bresin, R and Hirata, K and KH, RenCon},
booktitle = {Proceedings of 2004 new interfaces for musical expression conference},
editor = {Nagashima, Y and Lyons, M},
pages = {120-123},
publisher = {ACM Press},
title = {{Turing test for musical expression proceedings of international conference on new interfaces for musical expression}},
year = {2004}
}

@article{18,
abstract = {Presented is a model of rubato, implemented in Lisp, in which expression is viewed as the mapping of musical structure into the variables of expression. The basic idea is that the performer uses ?phrase final lengthening? as a device to reflect some internal representation of the phrase structure. The representation is based on Lardahl and Jackendoff's time-span reduction. The basic heuristic in the model is recursive involving look-ahead and planning at a number of levels. The planned phrasings are superposed beat by beat and the output from the program is a list of durations which could easily be adapted to be sent to a synthesiser given a suitable system.
Presented is a model of rubato, implemented in Lisp, in which expression is viewed as the mapping of musical structure into the variables of expression. The basic idea is that the performer uses ?phrase final lengthening? as a device to reflect some internal representation of the phrase structure. The representation is based on Lardahl and Jackendoff's time-span reduction. The basic heuristic in the model is recursive involving look-ahead and planning at a number of levels. The planned phrasings are superposed beat by beat and the output from the program is a list of durations which could easily be adapted to be sent to a synthesiser given a suitable system.},
author = {Todd, Neil P. McAngus},
doi = {10.1080/07494468900640061},
issn = {0749-4467},
journal = {Contemporary Music Review},
month = jan,
number = {1},
pages = {69--88},
publisher = {Routledge},
title = {{A computational model of rubato}},
url = {http://dx.doi.org/10.1080/07494468900640061},
volume = {3},
year = {1989}
}
@article{31,
abstract = {Abstract Emotions are a key part of creative endeavours, and a core problem for computational models of creativity. In this paper we discuss an affective computing architecture for the dynamic modification of music with a view to predictablyaffectinginducedmusicalemotions. Extending previous work on the modification of perceived emotions in music, our system architecture aims to provide reliable control of both perceived and induced musical emotions: its emotionality. A rule-based system is used to modify a subset of musical features at two processing levels, namely score and performance. The interactive model leverages sensed listener affect by adapting the emotionality of the music modifications in real-time to assist the listener in reaching a desired emotional state.
Abstract Emotions are a key part of creative endeavours, and a core problem for computational models of creativity. In this paper we discuss an affective computing architecture for the dynamic modification of music with a view to predictablyaffectinginducedmusicalemotions. Extending previous work on the modification of perceived emotions in music, our system architecture aims to provide reliable control of both perceived and induced musical emotions: its emotionality. A rule-based system is used to modify a subset of musical features at two processing levels, namely score and performance. The interactive model leverages sensed listener affect by adapting the emotionality of the music modifications in real-time to assist the listener in reaching a desired emotional state.},
annote = {31},
author = {Livingstone, Steven R. and M\"{u}hlberger, Ralf and Brown, Andrew R. and Loch, Andrew},
doi = {10.1080/14626260701253606},
issn = {1462-6268},
journal = {Digital Creativity},
month = mar,
number = {1},
pages = {43--53},
publisher = {Routledge},
title = {{Controlling musical emotionality: an affective computational architecture for influencing musical emotions}},
url = {http://dx.doi.org/10.1080/14626260701253606},
volume = {18},
year = {2007}
}
@book{29,
abstract = {The only things truly universal in music are those that are based on biological and/or perceptual facts. Tuning Timbre Spectrum Scale focuses on perceptions of consonance and dissonance, which are defined in the Harvard Dictionary of Music: "Consonance is used to describe the agreeable effect produced by certain intervals as against the disagreeable effect produced others. Consonance and dissonance are the very foundation of harmonic music... consonance represents the element of smoothness and repose, while dissonance represents the no less important elements of roughness and irregularity.” Tuning Timbre Spectrum Scale begins by asking (and answering) the question: How can we build a device to measure consonance and dissonance? The remainder of the book describes the impact of such a "dissonance meter” on music theory, on synthesizer design, on the construction of musical scales and tunings, on the design of musical instruments, and introduces related compositional techniques and new methods of musicological analyses. A new chapter contains a detailed explanation of how the software works. It incorporates several important simplifications over the full presentation in the current Chapter 7 in order to allow it to function in real time. Another new chapter describes the various ways that the software can be used. New sections throughout the book bring it up to date with the current state of the subject. Tuning Timbre Spectrum Scale offers a unique analysis of the relationship between the structure of sound and the structure of scale and will be useful to musicians and composers who use inharmonic tones and sounds. This includes a large percentage of people composing and performing with modern musical synthesizers. It will be of use to arrangers, musicologists, and others interested in musical analysis. Tuning Timbre Spectrum Scale provides a unique approach to working with environmental sounds, and there are clear applications for the use of inharmonic sounds in film scoring. The book will also be of interest to engineers and others interested in the design of audio devices such as musical synthesizers, special effects devices, and keyboards.},
annote = {29},
author = {Sethares, William A.},
isbn = {1852337974},
publisher = {Springer},
title = {{Tuning, Timbre, Spectrum, Scale}},
url = {http://books.google.com.tw/books/about/Tuning\_Timbre\_Spectrum\_Scale.html?id=KChoKKhjOb0C\&pgis=1},
year = {2005}
}
@misc{sibelius,
title = {{Sibelius}},
url = {http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share},
howpublished = {http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share}
}
@article{17,
author = {Friberg, Anders and Bresin, Roberto and Sundberg, Johan},
doi = {10.2478/v10053-008-0052-x},
file = {:home/a108210/文件/Mendeley Desktop/2006 - Overview of the KTH rule system for musical performance - Friberg, Bresin, Sundberg.pdf:pdf},
issn = {1895-1171},
journal = {Advances in Cognitive Psychology},
keywords = {expression,music performance modeling,rule system},
month = jan,
number = {2},
pages = {145--161},
title = {{Overview of the KTH rule system for musical performance}},
url = {http://versita.metapress.com/openurl.asp?genre=article\&id=doi:10.2478/v10053-008-0052-x},
volume = {2},
year = {2006}
}

@article{19,
abstract = {A computational model of musical dynamics is proposed that complements an earlier model of expressive timing. The model, implemented in the artificial intelligence language LISP, is based on the observation that a musical phrase is often indicated by a crescendo/decrescendo shape. The functional form of this shape is derived by making two main assumptions. First, that musical dynamics and tempo are coupled, that is, ‘‘the faster the louder, the slower the softer.’’ This tempo/dynamics coupling, it is suggested, may be a characteristic of some classical and romantic styles perhaps exemplified by performances of Chopin. Second, that the tempo change is governed by analogy to physical movement. The allusion of musical expression to physical motion is further extended by the introduction of the concepts of energy and mass. The utility of the model, in addition to giving an insight into the nature of musical expression, is that it provides a basis for a method of performance style analysis.},
author = {{McAngus Todd}, Neil P.},
doi = {10.1121/1.402843},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
month = jun,
number = {6},
pages = {3540},
publisher = {Acoustical Society of America},
title = {{The dynamics of dynamics: A model of musical expression}},
url = {http://scitation.aip.org/content/asa/journal/jasa/91/6/10.1121/1.402843},
volume = {91},
year = {1992}
}
@article{20,
abstract = {The notion that there is an intimate relationship between musical motion and physical movement is an old one and can be traced back to antiquity. Recently this idea has again received some attention, particularly in relation to musical expression. To use a modern metaphor, one can consider expressive performance to be analogous to the problems of kinematics and trajectory planning in robotics. The trajectories referred to, however, are not those of the performers limbs in physical space, but those of an abstract movement relative to a metrical grid associated with a musical score. Recent studies have attempted to substantiate this idea by comparing a model of motion with timing measurements of the final ritardandi from actual performances. This study extends these earlier analyses to include the accelerandi as well as the ritardandi from complete performances. One conclusion is that the variation of tempo in music can be reasonably compared with velocity in the equations of elementary mechanics. Further, it is suggested that the origin of metrical space, upon which the motion concept rests, lies in the way the auditory system processes rhythm.},
annote = {20
},
author = {Todd, Neil P. McAngus},
doi = {10.1121/1.412067},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
month = mar,
number = {3},
pages = {1940},
publisher = {Acoustical Society of America},
title = {{The kinematics of musical expression}},
url = {http://scitation.aip.org/content/asa/journal/jasa/97/3/10.1121/1.412067},
volume = {97},
year = {1995}
}
@article{21,
author = {Clynes, M},
journal = {Journal For The Integrated Study Of Artificial Intelligence},
title = {Generative principles of musical thought: Integration of microstructure with structure},
url = {http://ivizlab.sfu.ca/arya/Papers/Others/Generative Principles of Musical Thought.pdf},
year = {1986}
}
@article{22,
author = {Clynes, M},
journal = {Cognition},
title = {{Microstructural musical linguistics: composers' pulses are liked most by the best musicians}},
url = {http://www.sciencedirect.com/science/article/pii/001002779400650A},
year = {1995}
}

@article{26,
author = {Mazzola, G and Zahorka, O},
journal = {Computer Music Journal},
number = {1},
pages = {40--52},
title = {{Tempo curves revisited: Hierarchies of performance fields}},
url = {http://www.jstor.org/stable/3680521},
volume = {18},
year = {1994}
}
@book{27,
address = {Basel/Boston},
author = {Mazzola, G},
isbn = {3764357312},
publisher = {Birkh\"{a}user},
title = {{The Topos of Music: Geometric Logic of Concepts, Theory, and Performance}},
url = {http://www.amazon.com/The-Topos-Music-Geometric-Performance/dp/3764357312},
year = {2002}
}
@inproceedings{25,
address = {Ann Arbor, Michigan},
author = {Dannenberg, Roger B. and Pellerin, Hank and Derenyi, Itsvan},
booktitle = {Proceedings of the 1998 international computer music conference},
editor = {1998, October},
pages = {57--61},
publisher = {International Computer Music Association},
title = {{A Study of Trumpet Envelopes}},
url = {http://repository.cmu.edu/compsci/501},
year = {1998}
}
@article{24,
abstract = {Abstract Convincing synthesis of wind instruments requires more than the reproduction of individual tones. Since the player exerts continuous control over amplitude, frequency, and other parameters, it is not adequate to store simple templates for individual tones and string them together to make phrases. Transitions are important, and the details of a tone are affected by context. To address these problems, we present an approach to music synthesis that relies on a performance model to generate musical control signals and an instrument model to generate appropriate time?varying spectra. This approach is carefully designed to facilitate model construction from recorded examples of acoustic performances. We report on our experience developing a system to synthesize trumpet performances from a symbolic score input.
Abstract Convincing synthesis of wind instruments requires more than the reproduction of individual tones. Since the player exerts continuous control over amplitude, frequency, and other parameters, it is not adequate to store simple templates for individual tones and string them together to make phrases. Transitions are important, and the details of a tone are affected by context. To address these problems, we present an approach to music synthesis that relies on a performance model to generate musical control signals and an instrument model to generate appropriate time?varying spectra. This approach is carefully designed to facilitate model construction from recorded examples of acoustic performances. We report on our experience developing a system to synthesize trumpet performances from a symbolic score input.},
author = {Dannenberg, Roger B. and Derenyi, Istvan},
doi = {10.1080/09298219808570747},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = sep,
number = {3},
pages = {211--238},
publisher = {Routledge},
title = {{Combining instrument and performance models for high-quality music synthesis}},
url = {http://dx.doi.org/10.1080/09298219808570747},
volume = {27},
year = {1998}
}
@article{23,
abstract = {The development and implementation of an expert system that determines the tempo and articulations of Bach fugues are described. The rules in the knowledge base are based on the expertise of two professional performers. The system's input is a numeric representation of the fugue. The system processes the input using a transition graph, a data structure consisting of nodes where data is stored and edges that connect the nodes. The transition graph recognizes rhythmic patterns in the input. Once the system identifies a pattern, it applies a specific rule or performs a procedure. System output consists of a listing of tempo and articulation instructions. To validate the expert system, its output was compared with versions of fugues edited by one of the two experts used in developing the system. In tests with six fugues, the expert system generated the same editing instructions 85 to 90\% of the time.},
annote = {23

      },
author = {Johnson, M.L.},
doi = {10.1109/2.84832},
issn = {0018-9162},
journal = {Computer},
month = jul,
number = {7},
pages = {30--34},
shorttitle = {Computer},
title = {{Toward an expert system for expressive musical performance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=84832},
volume = {24},
year = {1991}
}
@inproceedings{32,
annote = {32},
author = {Katayose, H. and Fukuoka, T. and Takami, K. and Inokuchi, S.},
booktitle = {Proceedings of 10th International Conference on Pattern Recognition},
year={1990},
doi = {10.1109/ICPR.1990.118216},
isbn = {0-8186-2062-5},
language = {English},
pages = {780--784},
publisher = {IEEE Comput. Soc. Press},
title = {{Expression extraction in virtuoso music performances}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=118216},
volume = {i}
}
@article{33,
abstract = {33},
annote = {33},
author = {Katayose, H and Fukuoka, T and Takami, K and Inokuchi, S},
journal = {Journal of Information Processing Society of Japan},
number = {38},
pages = {1473--1481},
title = {{Extraction of expression parameters with multiple regression analysis}},
year = {1997}
}
@article{35,
author = {Canazza, Sergio and {De Poli}, Giovanni and Drioli, Carlo and Rod\`{a}, Antonio and Vidolin, Alvise},
issn = {1070-986X},
journal = {IEEE MultiMedia},
month = jul,
number = {3},
pages = {79--83},
publisher = {IEEE Computer Society Press},
title = {{Audio Morphing Different Expressive Intentions for Multimedia Systems}},
url = {http://dl.acm.org/citation.cfm?id=614667.614998},
volume = {7},
year = {2000}
}
@article{36,
author = {Canazza, S. and Vidolin, A. and {De Poli}, G. and Drioli, C. and Rod\`{a}, A.},
isbn = {0-7695-1284-4},
month = nov,
pages = {116},
publisher = {IEEE Computer Society},
title = {{Expressive Morphing for Interactive Performance of Musical Scores}},
url = {http://dl.acm.org/citation.cfm?id=882508.885257},
year = {2001}
}
@article{37,
abstract = {Expressiveness is not an extravagance: instead, expressiveness plays a critical role in rational decision-making, in perception, in human interaction, in human emotions and in human intelligence. These facts, combined with the development of new informatics systems able to recognize and understand different kinds of signals, open new areas for research. A new model is suggested for computer understanding of sensory expressive intentions of a human performer and both theoretical and practical applications are described for human-computer interaction, perceptual information retrieval, creative arts and entertainment. Recent studies demonstrated that by opportunely modifying systematic deviations introduced by the musician it is possible to convey different sensitive contents, such as expressive intentions and/or emotions. We present an space, that can be used as a user interface. It represents, at an abstract level, the expressive content and the interaction between the performer and an expressive synthesizer.
Expressiveness is not an extravagance: instead, expressiveness plays a critical role in rational decision-making, in perception, in human interaction, in human emotions and in human intelligence. These facts, combined with the development of new informatics systems able to recognize and understand different kinds of signals, open new areas for research. A new model is suggested for computer understanding of sensory expressive intentions of a human performer and both theoretical and practical applications are described for human-computer interaction, perceptual information retrieval, creative arts and entertainment. Recent studies demonstrated that by opportunely modifying systematic deviations introduced by the musician it is possible to convey different sensitive contents, such as expressive intentions and/or emotions. We present an space, that can be used as a user interface. It represents, at an abstract level, the expressive content and the interaction between the performer and an expressive synthesizer.},
author = {Canazza, Sergio and {De Poli}, Giovanni and Rod\`{a}, Antonio and Vidolin, Alvise},
doi = {10.1076/jnmr.32.3.281.16862},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = sep,
number = {3},
pages = {281--294},
publisher = {Routledge},
title = {{An Abstract Control Space for Communication of Sensory Expressive Intentions in Music Performance}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.3.281.16862},
volume = {32},
year = {2003}
}
@inproceedings{39,
address = {L'Aquila, Italy},
author = {Camurri, A and Dillon, Roberto and Saron, A},
booktitle = {Proceedings of 13th colloquium on musical informatics},
title = {{An experiment on analysis and synthesis of musical expressivity}},
year = {2000}
}

@inproceedings{57,
author = {Dorard, L and Hardoon, DR and Shawe-Taylor, J},
booktitle = {Proceedings of the Music, Brain and Cognition Workshop -- Neural Information Processing Systems},
publisher = {Whistler, Canada},
title = {{Can style be learned? A machine learning approach towards ‘performing’as famous pianists.}},
year = {2007}
}
@inproceedings{52,
author = {Raphael, C},
booktitle = {Proceedings of the 8th Int. Workshop on Artificial Intelligence and Statistics},
editor = {Jaakkola, T and Richardson, T},
file = {::},
pages = {113--120},
publisher = {Morgan Kaufmann, San Francisco},
title = {{Can the computer learn to play music expressively?}},
url = {http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/raphael\_perf.pdf},
year = {2001}
}
@article{53,
author = {Raphael, C},
journal = {Neural Information Processing Systems},
number = {14},
pages = {1433--1440},
title = {{A Bayesian Network for Real-Time Musical Accompaniment.}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AP01.ps.gz},
year = {2001}
}
@inproceedings{54,
address = {Acapulco, Mexico},
author = {Raphael, C},
booktitle = {Proceedings of 2003 International Joint conference on Artifical Intelligence (Working Notes of IJCAI-03 Rencon Workshop)},
editor = {Gottob, G and Walsh, T},
file = {::},
pages = {5--10},
publisher = {Morgan Kaufmann, San Francisco},
title = {{Orchestra in a box: A system for real-time musical accompaniment}},
url = {http://renconmusic.org/ijcai2003/IJCAI\_Rencon2003\_FINALLAST.pdf\#page=11},
year = {2003}
}
@phdthesis{55,
author = {Grindlay, GC},
school = {University of Santa Cruz, CA},
title = {{Modeling expressive musical performance with Hidden Markov Models}},
type = {PhD Thesis},
url = {http://scholar.google.com.tw/scholar?hl=zh-TW\&as\_sdt=0,5\&q=grindlay+gc+modelling+expressive+musical+performance+with+hidden+markov+models\#0},
year = {2005}
}

@inproceedings{89,
address = {Jinan, China},
author = {Zhang, Q and Miranda, ER},
booktitle = {Proceedings of the 6th International Conference on Intelligent Systems Design and Applications},
editor = {Chen, Y and Abraham, A},
pages = {1189--1194},
publisher = {IEEE Computer Society, Washington, DC},
title = {{Towards an evolution model of expressive music performance}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4021833},
year = {2006}
}
@inproceedings{88,
address = {Clearwater Beach, FL, USA},
author = {Ramirez, R and Hazan, A},
booktitle = {Proceedings of 18th international Florida Artificial Intelligence Research Society Sonference (AI in Music and Art)},
pages = {86--91},
publisher = {AAAI Press, Menlo Park},
title = {{Modeling Expressive Music Performance in Jazz.}},
url = {http://www.aaai.org/Papers/FLAIRS/2005/Flairs05-015.pdf},
year = {2005}
}
@inproceedings{82,
address = {New Orleans, USA},
author = {Wright, M and Berdahl, E},
booktitle = {Proceedings of the 2006 International Computer Music Conference},
editor = {Zannos, I},
file = {::},
pages = {572--575},
publisher = {ICMA, San Francisco},
title = {{Towards machine learning of expressive microtiming in Brazilian drumming}},
url = {https://ccrma.stanford.edu/~eberdahl/Projects/Microtiming/ML-Microtiming-ICMC.pdf},
year = {2006}
}

@article{42,
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez},
doi = {10.1023/A:1008311209823},
issn = {1573-7497},
journal = {Journal of Applied Intelligence},
language = {en},
month = jan,
number = {1},
pages = {115--129},
publisher = {Kluwer Academic Publishers},
title = {{An Interactive Case-Based Reasoning Approach for Generating Expressive Music}},
url = {http://link.springer.com/article/10.1023/A\%3A1008311209823},
volume = {14},
year = {2001}
}
@article{60,
author = {Miranda, ER and Kirke, A and Zhang, Q},
file = {::},
journal = {Computer Music Journal},
number = {1},
pages = {80--96},
title = {{Artificial evolution of expressive performance of music: An imitative multi-agent systems approach}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/comj.2010.34.1.80},
volume = {34},
year = {2010}
}
@inproceedings{59,
address = {London, UK},
author = {Ramirez, R and Hazan, A},
booktitle = {Proceedings of 2007 annual conference on Genetic and evolutionary computation},
file = {::},
publisher = {ACM Press, New York},
title = {{Inducing a generative expressive performance model using a sequential-covering genetic algorithm}},
url = {http://dl.acm.org/citation.cfm?id=1277374},
year = {2007}
}
@inproceedings{42,
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez},
booktitle = {Proceedings of the Workshop on Current Research Directions in Computer Music},
pages = {17--22},
title = {{The SaxEx system for expressive music synthesis: A progress report}},
year = {2001}
}
@inproceedings{40,
address = {Thessalonikia, Greece},
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez and Serra, Xavier},
booktitle = {Proceedings of 1997 International Computer Music Conference},
editor = {Cook, PR},
pages = {329--336},
publisher = {ICMA, San Francisco},
title = {{X. Serra, 1997.“SaxEx: a case-based reasoning system for generating expressive musical performances”}},
year = {1997}
}
@article{41,
author = {Arcos, Josep Llu\'{\i}s and {De M\'{a}ntaras}, Ramon L\'{o}pez and Serra, Xavier},
journal = {Journal of New Music Research},
number = {3},
pages = {194--210},
publisher = {Taylor \& Francis},
title = {{Saxex: A case-based reasoning system for generating expressive musical performances}},
volume = {27},
year = {1998}
}
@inproceedings{45,
address = {Kyoto, Japan},
author = {Hirata, K and Hiraga, R},
booktitle = {Proceedings of the ICAD 2002 Rencon Workshop on performance rendering systems},
file = {::},
pages = {40--46},
title = {{Ha-Hi-Hun: Performance rendering system of high controllability}},
url = {http://www.fun.ac.jp/~hirata/Papers/icad2002-rencon-ws.pdf},
year = {2002}
}
@inproceedings{44,
address = {Acapulco, Mexico},
author = {Suzuki, T},
booktitle = {Proceedings of 2003 International Joint Conference on Artificial Intelligence (working Notes of RenCon Workshop)},
editor = {Gottlob, G and Walsh, T},
file = {::},
publisher = {Morgan Kaufmann, Los Altos},
title = {{Kagurame phase-II}},
url = {http://renconmusic.org/ijcai2003/IJCAI\_Rencon2003\_FINALLAST.pdf\#page=84},
year = {2003}
}
@inproceedings{43,
address = {Stockholm, Sweden},
author = {Suzuki, T and Tokunaga, T and Tanaka, H},
booktitle = {Proceedings of the 16th International Joint Conference on Artificial Intelligence},
file = {::},
pages = {642--648},
publisher = {Morgan Kaufmann, San Francisco},
title = {{A case based approach to the generation of musical expression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.1437\&rep=rep1\&type=pdf},
year = {1999}
}
@article{svm2005,
author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
file = {:home/a108210/文件/Mendeley Desktop/2005 - Large Margin Methods for Structured and Interdependent Output Variables - Tsochantaridis et al.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1453--1484},
title = {{Large Margin Methods for Structured and Interdependent Output Variables}},
volume = {6},
year = {2005}
}
@inproceedings{svm2003,
address = {Washington DC, USA},
author = {Altun, Yasemin and Tsochantaridis, Ioannis and Hofmann, Thomas},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:home/a108210/文件/Mendeley Desktop/2003 - Hidden Markov Support Vector Machines - Altun, Tsochantaridis, Hofmann.pdf:pdf},
pages = {3--10},
title = {{Hidden Markov Support Vector Machines}},
volume = {3},
year = {2003}
}
@article{svm2009,
annote = {SVM-HMM original paper},
author = {Joachims, Thorsten and Finley, Thomas and Yu, Chun-Nam John},
doi = {10.1007/s10994-009-5108-8},
file = {:home/a108210/文件/Mendeley Desktop/2009 - Cutting-plane training of structural SVMs - Joachims, Finley, Yu.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {structural svms,structured output predic-,support vector machines,tion,training algorithms},
month = may,
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5108-8 http://link.springer.com/article/10.1007/s10994-009-5108-8},
volume = {77},
year = {2009}
}
@book{Clementi1915,
address = {Paris},
author = {Clementi, Muzio},
edition = {Plate D. \& C. 9318},
publisher = {Durand \& Cie.},
title = {{SONATINES pour Piano a 2 mains Op. 36 VOLUME I [Musical Score]}},
year = {1915}
}
@inproceedings{34,
address = {Berlin, Germany},
author = {Ishikawa, Osamu and Aono, Yushi and Katayose, Haruhiro and Inokuchi, Seiji},
booktitle = {International Computer Music Conference Proceedings},
pages = {348--351},
publisher = {International Computer Music Association, San Francisco},
title = {{Extraction of Musical Performance Rules Using a Modified Algorithm of Multiple Regression Analysis}},
year = {2000}
}
@article{38,
abstract = {Abstract This article briefly summarises the author's research on automatic performance, started at CSC (Centro di Sonologia Computazionale, University of Padua) and continued at TMH?KTH (Speech, Music Hearing Department at the Royal Institute of Technology, Stockholm). The focus is on the evolution of the architecture of an artificial neural networks (ANNs) framework, from the first simple model, able to learn the KTH performance rules, to the final one, that accurately simulates the style of a real pianist performer, including time and loudness deviations. The task was to analyse and synthesise the performance process of a professional pianist, playing on a Disklavier. An automatic analysis extracts all performance parameters of the pianist, starting from the KTH rule system. The system possesses good generalisation properties: applying the same ANN, it is possible to perform different scores in the performing style used for the training of the networks. Brief descriptions of the program Melodia and of the two Java applets Japer and Jalisper are given in the Appendix. In Melodia, developed at the CSC, the user can run either rules or ANNs, and study their different effects. Japer and Jalisper, developed at TMH, implement in real time on the web the performance rules developed at TMH plus new features achieved by using ANNs.
Abstract This article briefly summarises the author's research on automatic performance, started at CSC (Centro di Sonologia Computazionale, University of Padua) and continued at TMH?KTH (Speech, Music Hearing Department at the Royal Institute of Technology, Stockholm). The focus is on the evolution of the architecture of an artificial neural networks (ANNs) framework, from the first simple model, able to learn the KTH performance rules, to the final one, that accurately simulates the style of a real pianist performer, including time and loudness deviations. The task was to analyse and synthesise the performance process of a professional pianist, playing on a Disklavier. An automatic analysis extracts all performance parameters of the pianist, starting from the KTH rule system. The system possesses good generalisation properties: applying the same ANN, it is possible to perform different scores in the performing style used for the training of the networks. Brief descriptions of the program Melodia and of the two Java applets Japer and Jalisper are given in the Appendix. In Melodia, developed at the CSC, the user can run either rules or ANNs, and study their different effects. Japer and Jalisper, developed at TMH, implement in real time on the web the performance rules developed at TMH plus new features achieved by using ANNs.},
author = {Bresin, Roberto},
doi = {10.1080/09298219808570748},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = sep,
number = {3},
pages = {239--270},
publisher = {Routledge},
title = {{Artificial neural networks based models for automatic performance of musical scores}},
url = {http://dx.doi.org/10.1080/09298219808570748},
volume = {27},
year = {1998}
}
@inproceedings{93,
address = {Lisbon, Portugal},
author = {Zhang, Qijun and Miranda, Eduardo R. },
booktitle = {Proceedings of ECAL 2007 workshop on music and artificial life (MusicAL 2007)},
title = {{Evolving Expressive Music Performance through Interaction of Artificial Agent Performers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.1412},
year = {2007}
}
@inproceedings{28,
address = {Bologna, Spain},
annote = {28},
author = {Hashida, M and Nagata, N and Katayose, H},
booktitle = {Proceedings of 9th International Conference on Music Perception and Cognition},
editor = {Baroni, M and Addessi, R and Caterina, R and Costa, M},
file = {:home/a108210/文件/Mendeley Desktop/2006 - Pop-E a performance rendering system for the ensemble music that considered group expression - Hashida, Nagata, Katayose.pdf:pdf},
pages = {526--534},
publisher = {ICMPC},
title = {{Pop-E: a performance rendering system for the ensemble music that considered group expression}},
url = {http://www.marcocosta.it/icmpc2006/pdfs/467.pdf},
year = {2006}
}
@inproceedings{46,
address = {Berlin, Germany},
annote = {46},
author = {Widmer, Gerhard},
booktitle = {Proceedings of the 2000 International Computer Music Conference},
editor = {Zannos, I},
file = {::},
pages = {344--347},
publisher = {International Computer Music Association, San Francisco},
title = {{Large-scale Induction of Expressive Performance Rules: First Quantitative Results}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.1409},
year = {2000}
}
@article{48,
annote = {48},
author = {Widmer, G},
journal = {Artificial Intelligence},
pages = {129--148},
title = {{Discovering simple rules in complex data: A meta-learning algorithm and some surprising musical discoveries}},
url = {http://www.sciencedirect.com/science/article/pii/S000437020300016X},
volume = {146},
year = {2003}
}
@article{47,
author = {Widmer, G and Tobudic, A},
journal = {Journal of New Music Research},
pages = {259--268},
title = {{Machine discoveries: A few simple, robust local expression principles}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.31.1.37.8103},
volume = {32},
year = {2002}
}
@article{49,
abstract = {The article describes basic research in the area of machine learning and musical expression. A first step towards automatic induction of multi-level models of expressive performance (currently only tempo and dynamics) from real performances by skilled pianists is presented. The goal is to learn to apply sensible tempo and dynamics ?shapes? at various levels of the hierarchical musical phrase structure. We propose a general method for decomposing given expression curves into elementary shapes at different levels, and for separating phrase-level expression patterns from local, note-level ones. We then present a hybrid learning system that learns to predict, via two different learning algorithms, both note-level and phrase-level expressive patterns, and combines these predictions into complex composite expression curves for new pieces. Experimental results indicate that the approach is generally viable; however, we also discuss a number of severe limitations that still need to be overcome in order to arrive at truly musical machine-generated performances.
The article describes basic research in the area of machine learning and musical expression. A first step towards automatic induction of multi-level models of expressive performance (currently only tempo and dynamics) from real performances by skilled pianists is presented. The goal is to learn to apply sensible tempo and dynamics ?shapes? at various levels of the hierarchical musical phrase structure. We propose a general method for decomposing given expression curves into elementary shapes at different levels, and for separating phrase-level expression patterns from local, note-level ones. We then present a hybrid learning system that learns to predict, via two different learning algorithms, both note-level and phrase-level expressive patterns, and combines these predictions into complex composite expression curves for new pieces. Experimental results indicate that the approach is generally viable; however, we also discuss a number of severe limitations that still need to be overcome in order to arrive at truly musical machine-generated performances.},
author = {Widmer, Gerhard and Tobudic, Asmir},
doi = {10.1076/jnmr.32.3.259.16860},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = sep,
number = {3},
pages = {259--268},
publisher = {Routledge},
title = {{Playing Mozart by Analogy: Learning Multi-level Timing and Dynamics Strategies}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.3.259.16860},
volume = {32},
year = {2003}
}
@inproceedings{51,
address = {Acapulco, Mexico},
author = {Tobudic, A and Widmer, G},
booktitle = {Proceedings of 2003 International Joint conference on Artifical Intelligence (Working Notes of IJCAI-03 Rencon Workshop)},
editor = {Hirata, Keiji},
file = {::},
title = {{Learning to play Mozart: Recent improvements}},
url = {http://renconmusic.org/ijcai2003/IJCAI\_Rencon2003\_FINALLAST.pdf\#page=43},
year = {2003}
}
@inproceedings{50,
author = {Tobudic, A and Widmer, G},
booktitle = {Proceedings of the 13th International Conference on Inductive Logic Programming},
editor = {Horvath, T and Yamamoto, A},
file = {::},
pages = {365--382},
publisher = {Springer Verlag, Berlin},
title = {{Relational IBL in music with a new structural similarity measure}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-39917-9\_24},
year = {2003}
}
@inproceedings{61,
address = {Lisbon, Portugal},
annote = {61 },
author = {Dahlstedt, P},
booktitle = {Proceedings of ECAL 2007 workshop on music and artificial life (Music AL 2007)},
file = {:home/a108210/文件/Mendeley Desktop/2007 - Autonomous evolution of complete piano pieces and performances - Dahlstedt.pdf:pdf},
title = {{Autonomous evolution of complete piano pieces and performances}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.9818\&rep=rep1\&type=pdf},
year = {2007}
}
@book{56,
annote = {56},
author = {Carlson, L and Nordmark, A and Wikilander, R},
publisher = {Propellerhead Software},
title = {{Reason version 2.5 -- Getting Started}},
year = {2003}
}
@misc{music21,
address = {Cambridge, Massachusetts, USA},
author = {Cuthbert, Michael and Ariza, Christopher},
title = {music21 [computer software]},
url = {http://web.mit.edu/music21/},
year = {2013}
}
@inproceedings{crestmuse,
annote = {crestmuse},
author = {Hashida, M and Matsui, T and Katayose, H},
booktitle = {International Conference of Music Information Retrival (ISMIR)},
pages = {489--494},
title = {{A New Music Database Describing Deviation Information of Performance Expressions}},
year = {2008}
}
@article{magaloff,
annote = {magaloff},
author = {Flossmann, Sebastian and Goebl, Werner and Grachten, Maarten and Niedermayer, B and Widmer, Gerhard},
file = {:home/a108210/文件/Mendeley Desktop/2010 - The Magaloff project An interim report - Flossmann et al.pdf:pdf},
journal = {Journal of New Music Research},
number = {4},
pages = {363--377},
title = {{The Magaloff project: An interim report}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2010.523469},
volume = {39},
year = {2010}
}
@inproceedings{Goebl2009,
annote = {magaloff-series

      },
author = {Goebl, W and Flossmann, S and Widmer, G},
booktitle = {Proceedings of the Sixth Sound and Music Computing Conference},
file = {::},
pages = {291----296},
title = {{Computational investigations into between-hand synchronization in piano playing: Magaloff's complete Chopin}},
url = {http://smcnetwork.org/files/proceedings/2009/272.pdf},
year = {2009}
}
@inproceedings{Grachten2011,
annote = {magaloff-series

      },
author = {Grachten, M and Widmer, G},
booktitle = {Proceedings of the 8th Sound and Music Computing Conference (SMC 2011)},
file = {::},
title = {{Explaining musical expression as a mixture of basis functions}},
url = {http://www.cp.jku.at/research/papers/Grachten\_Widmer\_SMC\_2011.pdf},
year = {2011}
}
@inproceedings{Flossmann2009,
annote = {magaloff-series

      },
author = {Flossmann, S and Goebl, W and Widmer, G},
booktitle = {Proceedings of the International Symposium on Performance Science},
title = {{Maintaining skill across the life span: Magaloff's entire Chopin at age 77}},
url = {http://www.tinyproxys.com/www.cp.jku.at/research/papers/Flossmann\_etal\_ISPS\_2009.pdf},
year = {2009}
}
@article{Grachten2012,
annote = {magaloff-series

      },
author = {Grachten, M and Widmer, G},
file = {::},
journal = {Journal of New Music Research},
title = {{Linear basis models for prediction and analysis of musical expression}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2012.731071},
year = {2012}
}
@incollection{Flossmann2013,
annote = {magaloff-series

      },
author = {Flossmann, S and Grachten, M and Widmer, G},
booktitle = {Guide to Computing for Expressive Music Performance},
editor = {Kirke, Alexis and Miranda, Eduardo R.},
pages = {75--98},
publisher = {Springer London},
title = {{Expressive performance rendering with probabilistic models}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-4123-5\_3},
year = {2013}
}
@inproceedings{Flossman2011,
address = {Utrecht},
annote = {magaloff-series

      },
author = {Flossman, S and Widmer, G},
booktitle = {International Symposium on Performance Science},
file = {::},
publisher = {AEC},
title = {{Toward a model of performance errors: A qualitative review of Magaloff's Chopin'}},
url = {http://www.legacyweb.rcm.ac.uk/cache/fl0026668.pdf},
year = {2011}
}
@inproceedings{Flossmann2010a,
address = {Seattle, Washington, USA},
annote = {magaloff-series

      },
author = {Flossmann, S and Goebl, W and Widmer, G},
booktitle = {Proceedings of the 11th ICMPC},
file = {::},
title = {{The Magaloff corpus: An empirical error study}},
url = {http://www.cp.jku.at/research/papers/Flossmann\_etal\_ICMPC\_2010.pdf},
year = {2010}
}
@book{GTTM,
abstract = {This work, which has become a classic in music theory since its publication in 1983, models music understanding from the perspective of cognitive science. The point of departure is a search for a grammar of music with the aid of generative linguistics. The theory, which is illustrated with numerous examples from Western classical music, relates the aural surface of a piece to the musical structure unconsciously inferred by the experienced listener. From the viewpoint of traditional music theory, it offers many innovations in notation as well as in the substance of rhythmic and reductional theory.},
annote = {GTTM},
author = {Lerdahl, Fred and Jackendoff, Ray S.},
isbn = {026262107X},
title = {{A Generative Theory of Tonal Music}},
url = {http://www.google.com.tw/books?hl=zh-TW\&lr=\&id=6HGiEW33lucC\&pgis=1},
year = {1983}
}
@book{brent1973,
abstract = {This outstanding text for graduate students and researchers proposes improvements to existing algorithms, extends their related mathematical theories, and offers details on new algorithms for approximating local and global minima. None of the algorithms requires an evaluation of derivatives; all depend entirely on sequential function evaluation, a highly practical scenario in the frequent event of difficult-to-evaluate derivatives.Topics include the use of successive interpolation for finding simple zeros of a function and its derivatives; an algorithm with guaranteed convergence for finding a minimum of a function of one variation; global minimization given an upper bound on the second derivative; and a new algorithm for minimizing a function of several variables without calculating derivatives. Many numerical examples augment the text, along with a complete analysis of rate of convergence for most algorithms and error bounds that allow for the effect of rounding errors.},
annote = {brent1973},
author = {Brent, Richard P.},
isbn = {0486143686},
title = {{Algorithms for Minimization Without Derivatives}},
url = {http://www.google.com.tw/books?hl=zh-TW\&lr=\&id=AITCAgAAQBAJ\&pgis=1},
year = {2013}
}
@article{yqx,
abstract = {The article is about AI research in the context of a complex artistic behavior: expressive music performance. A computer program is presented that learns to play piano with 'expression' and that even won an international computer piano performance contest. A superficial analysis of an expressive performance generated by the system seems to suggest creative musical abilities. After a critical discussion of the processes underlying this behavior, we abandon the question of whether the system is really creative, and turn to the true motivation that drives this research: to use AI methods to investigate and better understand music performance as a human creative behavior. A number of recent and current results from our research are briefly presented that indicate that machines can give us interesting insights into such a complex creative behavior, even if they may not be creative themselves.},
annote = {yqx},
author = {Widmer, Gerhard and Flossmann, Sebastian and Grachten, Maarten},
doi = {10.1609/aimag.v30i3.2249},
file = {::},
issn = {0738-4602},
journal = {AI Magazine},
language = {en},
month = jul,
number = {3},
pages = {35},
title = {{YQX Plays Chopin}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2249},
volume = {30},
year = {2009}
}
@misc{pmimacs,
author = {Kirke, A and Miranda, ER},
title = {{Using a biophysically-constrained multi-agent system to combine expressive performance with algorithmic composition}},
url = {http://scholar.google.com.tw/scholar?q=using+a+biophysically+constrained+multiagent+system+to+combine+expressive+performance+with+algorithmic+composition\&btnG=\&hl=zh-TW\&as\_sdt=0\%2C5\#0},
year = {2008}
}
