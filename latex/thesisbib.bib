@inproceedings{Good2001,
author = {Good, Michael},
booktitle = {XML Conference hosted by IDEAlliance},
doi = {10.1.1.118.5431},
file = {:home/a108210/Dropbox/mendeley sync/2001\_MusicXML An Internet-Friendly Format for Sheet Music\_Good.pdf:pdf},
title = {{MusicXML: An Internet-Friendly Format for Sheet Music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.118.5431},
year = {2001}
}
@misc{Joachims2008,
annote = {cornell lib },
author = {Joachims, Thorsten},
title = {{SVM\^{}hmm: Sequence Tagging with Structural Support Vector Machines}},
url = {http://www.cs.cornell.edu/people/tj/svm\_light/svm\_hmm.html},
year = {2008}
}
@article{Joachims2009,
annote = {SVM-HMM original paper},
author = {Joachims, Thorsten and Finley, Thomas and Yu, CNJ},
doi = {10.1007/s10994-009-5108-8},
file = {:home/a108210/Dropbox/mendeley sync/2009\_Cutting-plane training of structural SVMs\_Joachims, Finley, Yu.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {structural svms,structured output predic-,support vector machines,tion,training algorithms},
month = may,
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5108-8 http://link.springer.com/article/10.1007/s10994-009-5108-8},
volume = {77},
year = {2009}
}
@book{Selfridge-Field1997,
abstract = {"This volume, likely to become a standard reference work, describes an extraordinary number of approaches to the representation of musical information for purposes of computer processing. It is a considerable achievement, for it sorts and orders work in a confusing and sometimes embattled field, analyzing each encoding method in logical sequence and in light of the specific purposes for which it was designed." -- Raymond Erickson, Dean of Arts and Humanities, Queens College, CUNY; author of "DARMS: A Reference Manual" The establishment of the Musical Instrument Digital Interface (MIDI) in the late 1980s allowed hobbyists and musicians to experiment with sound control in ways that previously had been possible only in research studios. MIDI is now the most prevalent representation of music, but what it represents is based on hardware control protocols for sound synthesis. Programs that support sound input for graphics output necessarily span a gamut of representational categories. What is most likely to be lost is any sense of the musical work. Thus, for those involved in pedagogy, analysis, simulation, notation, and music theory, the nature of the representation matters a great deal. An understanding of the data requirements of different applications is fundamental to the creation of interchange codes. The contributors to "Beyond MIDI" present a broad range of schemes, illustrating a wide variety of approaches to music representation. Generally, each chapter describes the history and intended purposes of the code, a description of the representation of the primary attributes of music (pitch, duration, articulation, ornamentation, dynamics, andtimbre), a description of the file organization, some mention of existing data in the format, resources for further information, and at least one encoded example. The book also shows how intended applications influence the kinds of musical information that are encoded. Contributors: David Bainbridge, Ulf Berggren, Roger D. Boyle, Donald Byrd, David Cooper, Edmund Correia, Jr., David Cottle, Tim Crawford, J. Stephen Dydo, Brent A. Field, Roger Firman, John Gibson, Cindy Grande, Lippold Haken, Thomas Hall, David Halperin, Philip Hazel, Walter B. Hewlett, John Howard, David Huron, Werner Icking, David Jaffe, Bettye Krolick, Max V. Mathews, Toshiaki Matsushima, Steven R. Newcomb, Kia-Chuan Ng, Kjell E. Nordli, Sile O'Modhrain, Perry Roland, Helmut Schaffrath, Bill Schottstaedt, Eleanor Selfrdige-Field, Peer Sitter, Donald Sloan, Leland Smith, Andranick Tanguiane, Lynn M. Trowbridge, Frans Wiering.},
author = {Selfridge-Field, Eleanor},
isbn = {0262193949},
publisher = {MIT Press},
title = {{Beyond MIDI: The Handbook of Musical Codes}},
url = {http://books.google.com.tw/books/about/Beyond\_MIDI.html?id=Xm3J9DG9EFcC\&pgis=1},
year = {1997}
}
@misc{KernScores,
title = {{KernScores}},
url = {http://kern.ccarh.org/},
howpublished= {http://kern.ccarh.org/}
}
@misc{LilyPond,
title = {{LilyPond}},
url = {http://www.lilypond.org},
howpublished = {http://www.lilypond.org}
}
@inproceedings{Lyu2012,
author = {Lyu, Shing Hermes and Jeng, Shyh-kang},
booktitle = {workshop on Computer Music and Audio Technology},
title = {{COMPUTER EXPRESSIVE MUSIC PERFORMANCE BY PHRASE-WISE MODELING}},
year = {2012}
}
@misc{zenph,
booktitle = {Zenph Music},
title = {{Rachmianinoff - Plays Rachmaninoff}},
url = {https://www.zenph.com/rachmaninoff-plays-rachmaninoff},
howpublished= {https://www.zenph.com/rachmaninoff-plays-rachmaninoff},
urldate = {2014/2/11},
year = {2009}
}
@incollection{THEBOOK,
author = {Kirke, Alexis and Miranda, Eduardo R.},
booktitle = {Guide to Computing for Expressive Music Performance},
editor = {Kirke, Alexis and Miranda, Eduardo R.},
isbn = {978-1-4471-4123-5},
pages = {1--47},
publisher = {Springer},
title = {{An Overview of Computer Systems for Expressive Music Performance}},
year = {2013}
}
@inproceedings{RenCon,
address = {Hamatsu, Japan},
annote = {RenCon
},
author = {Hiraga, R and Bresin, R and Hirata, K and KH, RenCon},
booktitle = {Proceedings of 2004 new interfaces for musical expression conference},
editor = {Nagashima, Y and Lyons, M},
pages = {120-123},
publisher = {ACM Press},
title = {{Turing test for musical expression proceedings of international conference on new interfaces for musical expression}},
year = {2004}
}

@article{18,
abstract = {Presented is a model of rubato, implemented in Lisp, in which expression is viewed as the mapping of musical structure into the variables of expression. The basic idea is that the performer uses ?phrase final lengthening? as a device to reflect some internal representation of the phrase structure. The representation is based on Lardahl and Jackendoff's time-span reduction. The basic heuristic in the model is recursive involving look-ahead and planning at a number of levels. The planned phrasings are superposed beat by beat and the output from the program is a list of durations which could easily be adapted to be sent to a synthesiser given a suitable system.
Presented is a model of rubato, implemented in Lisp, in which expression is viewed as the mapping of musical structure into the variables of expression. The basic idea is that the performer uses ?phrase final lengthening? as a device to reflect some internal representation of the phrase structure. The representation is based on Lardahl and Jackendoff's time-span reduction. The basic heuristic in the model is recursive involving look-ahead and planning at a number of levels. The planned phrasings are superposed beat by beat and the output from the program is a list of durations which could easily be adapted to be sent to a synthesiser given a suitable system.},
author = {Todd, Neil},
doi = {10.1080/07494468900640061},
issn = {0749-4467},
journal = {Contemporary Music Review},
month = jan,
number = {1},
pages = {69--88},
publisher = {Routledge},
title = {{A computational model of rubato}},
url = {http://dx.doi.org/10.1080/07494468900640061},
volume = {3},
year = {1989}
}
@article{31,
abstract = {Abstract Emotions are a key part of creative endeavours, and a core problem for computational models of creativity. In this paper we discuss an affective computing architecture for the dynamic modification of music with a view to predictablyaffectinginducedmusicalemotions. Extending previous work on the modification of perceived emotions in music, our system architecture aims to provide reliable control of both perceived and induced musical emotions: its emotionality. A rule-based system is used to modify a subset of musical features at two processing levels, namely score and performance. The interactive model leverages sensed listener affect by adapting the emotionality of the music modifications in real-time to assist the listener in reaching a desired emotional state.
Abstract Emotions are a key part of creative endeavours, and a core problem for computational models of creativity. In this paper we discuss an affective computing architecture for the dynamic modification of music with a view to predictablyaffectinginducedmusicalemotions. Extending previous work on the modification of perceived emotions in music, our system architecture aims to provide reliable control of both perceived and induced musical emotions: its emotionality. A rule-based system is used to modify a subset of musical features at two processing levels, namely score and performance. The interactive model leverages sensed listener affect by adapting the emotionality of the music modifications in real-time to assist the listener in reaching a desired emotional state.},
annote = {31},
author = {Livingstone, Steven R. and M\"{u}hlberger, Ralf and Brown, Andrew R. and Loch, Andrew},
doi = {10.1080/14626260701253606},
issn = {1462-6268},
journal = {Digital Creativity},
month = mar,
number = {1},
pages = {43--53},
publisher = {Routledge},
title = {{Controlling musical emotionality: an affective computational architecture for influencing musical emotions}},
url = {http://dx.doi.org/10.1080/14626260701253606},
volume = {18},
year = {2007}
}
@book{29,
abstract = {The only things truly universal in music are those that are based on biological and/or perceptual facts. Tuning Timbre Spectrum Scale focuses on perceptions of consonance and dissonance, which are defined in the Harvard Dictionary of Music: "Consonance is used to describe the agreeable effect produced by certain intervals as against the disagreeable effect produced others. Consonance and dissonance are the very foundation of harmonic music... consonance represents the element of smoothness and repose, while dissonance represents the no less important elements of roughness and irregularity.” Tuning Timbre Spectrum Scale begins by asking (and answering) the question: How can we build a device to measure consonance and dissonance? The remainder of the book describes the impact of such a "dissonance meter” on music theory, on synthesizer design, on the construction of musical scales and tunings, on the design of musical instruments, and introduces related compositional techniques and new methods of musicological analyses. A new chapter contains a detailed explanation of how the software works. It incorporates several important simplifications over the full presentation in the current Chapter 7 in order to allow it to function in real time. Another new chapter describes the various ways that the software can be used. New sections throughout the book bring it up to date with the current state of the subject. Tuning Timbre Spectrum Scale offers a unique analysis of the relationship between the structure of sound and the structure of scale and will be useful to musicians and composers who use inharmonic tones and sounds. This includes a large percentage of people composing and performing with modern musical synthesizers. It will be of use to arrangers, musicologists, and others interested in musical analysis. Tuning Timbre Spectrum Scale provides a unique approach to working with environmental sounds, and there are clear applications for the use of inharmonic sounds in film scoring. The book will also be of interest to engineers and others interested in the design of audio devices such as musical synthesizers, special effects devices, and keyboards.},
annote = {29},
author = {Sethares, William A.},
isbn = {1852337974},
publisher = {Springer},
title = {{Tuning, Timbre, Spectrum, Scale}},
url = {http://books.google.com.tw/books/about/Tuning\_Timbre\_Spectrum\_Scale.html?id=KChoKKhjOb0C\&pgis=1},
year = {2005}
}
@misc{sibelius,
title = {{Sibelius}},
url = {http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share},
howpublished = {http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share}
}
@article{17,
author = {Friberg, Anders and Bresin, Roberto and Sundberg, Johan},
doi = {10.2478/v10053-008-0052-x},
file = {:home/a108210/文件/Mendeley Desktop/2006 - Overview of the KTH rule system for musical performance - Friberg, Bresin, Sundberg.pdf:pdf},
issn = {1895-1171},
journal = {Advances in Cognitive Psychology},
keywords = {expression,music performance modeling,rule system},
month = jan,
number = {2},
pages = {145--161},
title = {{Overview of the KTH rule system for musical performance}},
url = {http://versita.metapress.com/openurl.asp?genre=article\&id=doi:10.2478/v10053-008-0052-x},
volume = {2},
year = {2006}
}

@article{19,
abstract = {A computational model of musical dynamics is proposed that complements an earlier model of expressive timing. The model, implemented in the artificial intelligence language LISP, is based on the observation that a musical phrase is often indicated by a crescendo/decrescendo shape. The functional form of this shape is derived by making two main assumptions. First, that musical dynamics and tempo are coupled, that is, ‘‘the faster the louder, the slower the softer.’’ This tempo/dynamics coupling, it is suggested, may be a characteristic of some classical and romantic styles perhaps exemplified by performances of Chopin. Second, that the tempo change is governed by analogy to physical movement. The allusion of musical expression to physical motion is further extended by the introduction of the concepts of energy and mass. The utility of the model, in addition to giving an insight into the nature of musical expression, is that it provides a basis for a method of performance style analysis.},
author = {{McAngus Todd}, Neil P.},
doi = {10.1121/1.402843},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
month = jun,
number = {6},
pages = {3540},
publisher = {Acoustical Society of America},
title = {{The dynamics of dynamics: A model of musical expression}},
url = {http://scitation.aip.org/content/asa/journal/jasa/91/6/10.1121/1.402843},
volume = {91},
year = {1992}
}
@article{20,
abstract = {The notion that there is an intimate relationship between musical motion and physical movement is an old one and can be traced back to antiquity. Recently this idea has again received some attention, particularly in relation to musical expression. To use a modern metaphor, one can consider expressive performance to be analogous to the problems of kinematics and trajectory planning in robotics. The trajectories referred to, however, are not those of the performers limbs in physical space, but those of an abstract movement relative to a metrical grid associated with a musical score. Recent studies have attempted to substantiate this idea by comparing a model of motion with timing measurements of the final ritardandi from actual performances. This study extends these earlier analyses to include the accelerandi as well as the ritardandi from complete performances. One conclusion is that the variation of tempo in music can be reasonably compared with velocity in the equations of elementary mechanics. Further, it is suggested that the origin of metrical space, upon which the motion concept rests, lies in the way the auditory system processes rhythm.},
annote = {20
},
author = {Todd, Neil P. McAngus},
doi = {10.1121/1.412067},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
month = mar,
number = {3},
pages = {1940},
publisher = {Acoustical Society of America},
title = {{The kinematics of musical expression}},
url = {http://scitation.aip.org/content/asa/journal/jasa/97/3/10.1121/1.412067},
volume = {97},
year = {1995}
}
@article{21,
author = {Clynes, M},
journal = {Journal For The Integrated Study Of Artificial  \ldots},
title = {{Generative principles of musical thought: Integration of microstructure with structure}},
url = {http://ivizlab.sfu.ca/arya/Papers/Others/Generative Principles of Musical Thought.pdf},
year = {1986}
}
@article{22,
author = {Clynes, M},
journal = {Cognition},
title = {{Microstructural musical linguistics: composers' pulses are liked most by the best musicians}},
url = {http://www.sciencedirect.com/science/article/pii/001002779400650A},
year = {1995}
}
