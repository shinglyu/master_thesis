<!DOCTYPE html><HTML>
<HEAD>
<TITLE></TITLE>
</HEAD>
<BODY>
<A name=1></a><IMG src="thesis-1_1.jpg"/><br/>
國立臺灣大學電機資訊學院電機工程學系<br/>
碩士論文&#160;(初稿)<br/>
Department&#160;of&#160;Electrical&#160;Engineering<br/>
College&#160;of&#160;Electrical&#160;Engineering&#160;and&#160;Computer&#160;Science<br/>
National&#160;Taiwan&#160;University<br/>
Master&#160;Thesis&#160;(DRAFT)<br/>
利用結構性支撐向量機的具音樂表現能力之半自動電腦演奏<br/>
系統<br/>
A&#160;Semi-automatic&#160;Computer&#160;Expressive&#160;Music&#160;Performance<br/>
System&#160;Using&#160;Structural&#160;Support&#160;Vector&#160;Machine<br/>
呂　行<br/>
Shing&#160;Hermes&#160;Lyu<br/>
指導教授：鄭士康博士<br/>
Advisor:&#160;Shyh-Kang&#160;Jeng,&#160;Ph.D.<br/>
中華民國&#160;103&#160;年&#160;6&#160;月<br/>
June,&#160;2014<br/>
<hr>
<A name=2></a><IMG src="thesis-2_1.jpg"/><br/>
電<br/>
國<br/>
機<br/>
立<br/>
工<br/>
臺<br/>
程<br/>
灣<br/>
學<br/>
大<br/>
系<br/>
學<br/>
(<br/>
碩<br/>
初稿<br/>
士<br/>
)<br/>
論<br/>文<br/>
自動電腦演奏系統<br/>
利<br/>用<br/>結<br/>構<br/>性<br/>支<br/>撐<br/>向<br/>量<br/>機<br/>的<br/>具<br/>音<br/>樂<br/>表<br/>現<br/>能<br/>力<br/>之<br/>半<br/>
呂　行<br/>
103<br/>
6<br/>
<hr>
<A name=3></a><IMG src="thesis-3_1.jpg"/><br/>
國立臺灣大學（碩）博士學位論文<br/>
口試委員會審定書<br/>
論文中文題目<br/>
論文英文題目<br/>
本論文係呂行君（R01921032）在國立臺灣大學電機工程學研究<br/>
所完成之碩士學位論文，於民國&#160;103&#160;年○○月○○日承下列考試委員<br/>審查通過及口試及格，特此證明<br/>
口試委員：<br/>
　　　　　　　　　　　　　　　　（簽名）<br/>
（指導教授）<br/>
　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;<br/>
　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;<br/>
　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;<br/>
　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;　　<br/>
&#160;<br/>
　　　　　　<br/>
&#160;&#160;&#160;<br/>
系主任、所長&#160;&#160;　　　　　　　　　　　（簽名）<br/>
（是否須簽章依各院系所規定）<br/>
<hr>
<A name=4></a><IMG src="thesis-4_1.jpg"/><br/>
ii<br/>
<hr>
<A name=5></a><IMG src="thesis-5_1.jpg"/><br/>
中文&#160;要<br/>
電腦合成的音樂一向被認為是僵硬、機械化而且沒有音樂表現能<br/>
力。因此能夠產生具有表現能力的電腦自動演奏系統將會對音樂產業、<br/>個人化娛樂以及表驗藝術領域有重大的影響。在這篇論文中，我們藉<br/>由隱藏式馬可夫模型結構的結構性支撐向量機&#160;(SVM-HMM)&#160;來設計一<br/>個可以產生具有表現能力音樂的電腦自動演奏系統。我們邀請六位研<br/>究生錄製了克萊門蒂（Muzio&#160;Clementi）的小奏鳴曲集&#160;Op.36。我們手<br/>動將這些錄音分割成樂句，並且利用程式從中抽取出音樂特徵。這些<br/>音樂特徵藉由&#160;SVM-HMM&#160;訓練成數學模型後，可以利用這個數學模型<br/>來演奏訓練過程中沒有見過的樂譜（需要手動標注樂句）。此系統目前<br/>只能支援單音旋律。問卷調查的結果顯示，對於業餘或專業的音樂家<br/>來說，本系統產生的音樂尚不能達到真人的演奏水準，但是沒有音樂<br/>背景的受試者給予本系統產生的音樂的分數已經與真人演奏不相上下。<br/>
關鍵字：電腦自動演奏、結構性支撐向量機、支撐向量機<br/>
iii<br/>
<hr>
<A name=6></a><IMG src="thesis-6_1.jpg"/><br/>
<b>Abstract</b><br/>
Computer&#160;generated&#160;music&#160;is&#160;known&#160;to&#160;be&#160;robotic&#160;and&#160;inexpressive.&#160;A<br/>
computer&#160;system&#160;that&#160;can&#160;generate&#160;expressive&#160;performance&#160;can&#160;potentially<br/>
have&#160;significant&#160;impact&#160;on&#160;music&#160;production&#160;industry,&#160;personalized&#160;entertain-<br/>
ment&#160;or&#160;even&#160;art.&#160;In&#160;this&#160;paper,&#160;we&#160;have&#160;designed&#160;and&#160;implemented&#160;a&#160;system<br/>
that&#160;can&#160;generate&#160;expressive&#160;performance&#160;using&#160;structural&#160;support&#160;vector&#160;ma-<br/>
chine&#160;with&#160;hidden&#160;Markov&#160;model&#160;output&#160;(SVM-HMM).&#160;We&#160;recorded&#160;six&#160;sets<br/>
of&#160;Muzio&#160;Clementi's&#160;Sonatina&#160;Op.36&#160;performed&#160;by&#160;six&#160;graduate&#160;students.&#160;The<br/>
recordings&#160;and&#160;scores&#160;are&#160;manually&#160;split&#160;into&#160;phrases&#160;and&#160;had&#160;their&#160;musical<br/>
features&#160;automatically&#160;extracted.&#160;Using&#160;the&#160;SVM-HMM&#160;algorithm,&#160;a&#160;math-<br/>
ematical&#160;model&#160;of&#160;expressive&#160;performance&#160;knowledge&#160;is&#160;learned&#160;from&#160;these<br/>
features.&#160;The&#160;trained&#160;model&#160;can&#160;generate&#160;expressive&#160;performances&#160;for&#160;pre-<br/>
viously&#160;unseen&#160;scores&#160;(with&#160;user-assigned&#160;phrasing).&#160;The&#160;system&#160;currently<br/>
supports&#160;monophonic&#160;music&#160;only.&#160;Subjective&#160;test&#160;shows&#160;that&#160;for&#160;amateur&#160;and<br/>
professional&#160;musician,&#160;the&#160;generated&#160;performance&#160;still&#160;need&#160;improvements&#160;to<br/>
be&#160;comparable&#160;to&#160;human&#160;recording,&#160;but&#160;the&#160;generated&#160;performance&#160;received<br/>
nearly&#160;the&#160;same&#160;rating&#160;as&#160;human&#160;recordings&#160;from&#160;participants&#160;without&#160;music<br/>
background.<br/>
Key&#160;words:&#160;Computer&#160;Expressive&#160;Performance,&#160;Performance&#160;Rendering,<br/>
Structural&#160;SVMs,&#160;Support&#160;Vector&#160;Machines.<br/>
iv<br/>
<hr>
<A name=7></a><IMG src="thesis-7_1.jpg"/><br/>
<b>Table&#160;of&#160;Contents</b><br/>
<a href="thesiss.html#1">試</a><br/>
<a href="thesiss.html#1">會</a><br/>
<b>i</b><br/>
<b>ii</b><br/>
<a href="thesiss.html#5">中文&#160;要</a><br/>
<b>iii</b><br/>
<a href="thesiss.html#6"><b>Abstract</b></a><br/>
<b>iv</b><br/>
<a href="thesiss.html#7"><b>Table&#160;of&#160;Contents</b></a><br/>
<b>v</b><br/>
<a href="thesiss.html#10"><b>List&#160;of&#160;Figures</b></a><br/>
<b>viii</b><br/>
<a href="thesiss.html#12"><b>List&#160;of&#160;Tables</b></a><br/>
<b>x</b><br/>
<a href="thesiss.html#13"><b>1</b></a><br/>
<a href="thesiss.html#13"><b>Introduction</b></a><br/>
<b>1</b><br/>
<a href="thesiss.html#13">1.1</a><br/>
<a href="thesiss.html#13">Motivation&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
1<br/>
<a href="thesiss.html#14">1.2</a><br/>
<a href="thesiss.html#14">Goal&#160;and&#160;Contribution</a><br/>
.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
2<br/>
<a href="thesiss.html#14">1.3</a><br/>
<a href="thesiss.html#14">Chapter&#160;Organization&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
2<br/>
<a href="thesiss.html#15"><b>2</b></a><br/>
<a href="thesiss.html#15"><b>Previous&#160;Works</b></a><br/>
<b>3</b><br/>
<a href="thesiss.html#15">2.1</a><br/>
<a href="thesiss.html#15">Various&#160;Goals&#160;and&#160;Evaluation&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
3<br/>
<a href="thesiss.html#17">2.2</a><br/>
<a href="thesiss.html#17">Researches&#160;Classified&#160;by&#160;Methods&#160;Used&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
5<br/>
<a href="thesiss.html#19">2.3</a><br/>
<a href="thesiss.html#19">Additional&#160;Specialties&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
7<br/>
<a href="thesiss.html#21"><b>3</b></a><br/>
<a href="thesiss.html#21"><b>Proposed&#160;Method</b></a><br/>
<b>9</b><br/>
<a href="thesiss.html#21">3.1</a><br/>
<a href="thesiss.html#21">Overview&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
9<br/>
v<br/>
<hr>
<A name=8></a><IMG src="thesis-8_1.jpg"/><br/>
<a href="thesiss.html#22">3.2</a><br/>
<a href="thesiss.html#22">A&#160;Brief&#160;Introduction&#160;to&#160;SVM-HMM&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
10<br/>
<a href="thesiss.html#27">3.3</a><br/>
<a href="thesiss.html#27">Learning&#160;Performance&#160;Knowledge&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
15<br/>
<a href="thesiss.html#28">3.3.1</a><br/>
<a href="thesiss.html#28">Training&#160;Sample&#160;Loader&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
16<br/>
<a href="thesiss.html#28">3.3.2</a><br/>
<a href="thesiss.html#28">Features&#160;Extraction&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
16<br/>
<a href="thesiss.html#29">3.3.3</a><br/>
<a href="thesiss.html#29">SVM-HMM&#160;Learning&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
17<br/>
<a href="thesiss.html#31">3.4</a><br/>
<a href="thesiss.html#31">Performing&#160;Expressively&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
19<br/>
<a href="thesiss.html#32">3.4.1</a><br/>
<a href="thesiss.html#32">SVM-HMM&#160;Generation&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
20<br/>
<a href="thesiss.html#32">3.4.2</a><br/>
<a href="thesiss.html#32">MIDI&#160;Generation&#160;and&#160;Synthesis&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
20<br/>
<a href="thesiss.html#33">3.5</a><br/>
<a href="thesiss.html#33">Features&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
21<br/>
<a href="thesiss.html#33">3.5.1</a><br/>
<a href="thesiss.html#33">Score&#160;Features&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
21<br/>
<a href="thesiss.html#35">3.5.2</a><br/>
<a href="thesiss.html#35">Performance&#160;Features&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
23<br/>
<a href="thesiss.html#36">3.5.3</a><br/>
<a href="thesiss.html#36">Normalizing&#160;Onset&#160;Deviation</a><br/>
.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
24<br/>
<a href="thesiss.html#38"><b>4</b></a><br/>
<a href="thesiss.html#38"><b>Corpus&#160;Preparation</b></a><br/>
<b>26</b><br/>
<a href="thesiss.html#38">4.1</a><br/>
<a href="thesiss.html#38">Existing&#160;Corpora&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
26<br/>
<a href="thesiss.html#39">4.2</a><br/>
<a href="thesiss.html#39">Corpus&#160;Specification&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
27<br/>
<a href="thesiss.html#42">4.3</a><br/>
<a href="thesiss.html#42">Implementation&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
30<br/>
<a href="thesiss.html#42">4.3.1</a><br/>
<a href="thesiss.html#42">Score&#160;Preparation&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
30<br/>
<a href="thesiss.html#42">4.3.2</a><br/>
<a href="thesiss.html#42">MIDI&#160;Recording&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
30<br/>
<a href="thesiss.html#43">4.3.3</a><br/>
<a href="thesiss.html#43">MIDI&#160;Cleaning&#160;and&#160;Phrase&#160;Splitting&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
31<br/>
<a href="thesiss.html#43">4.4</a><br/>
<a href="thesiss.html#43">Results&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
31<br/>
<a href="thesiss.html#48"><b>5</b></a><br/>
<a href="thesiss.html#48"><b>Experiments</b></a><br/>
<b>36</b><br/>
<a href="thesiss.html#48">5.1</a><br/>
<a href="thesiss.html#48">Onset&#160;Deviation&#160;Normalization&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
36<br/>
<a href="thesiss.html#52">5.2</a><br/>
<a href="thesiss.html#52">Parameter&#160;Selection&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
40<br/>
<a href="thesiss.html#52">5.2.1</a><br/>
<a href="thesiss.html#52">SVM-HMM-related&#160;Parameters&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
40<br/>
<a href="thesiss.html#54">5.2.2</a><br/>
<a href="thesiss.html#54">Quantization&#160;Parameter&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
42<br/>
<a href="thesiss.html#56">5.3</a><br/>
<a href="thesiss.html#56">Human-like&#160;Performance&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
44<br/>
<a href="thesiss.html#62"><b>6</b></a><br/>
<a href="thesiss.html#62"><b>Conclusions</b></a><br/>
<b>50</b><br/>
vi<br/>
<hr>
<A name=9></a><IMG src="thesis-9_1.jpg"/><br/>
<a href="thesiss.html#62"><b>Bibliography</b></a><br/>
<b>50</b><br/>
<a href="thesiss.html#72"><b>A&#160;Software&#160;Tools&#160;Used&#160;in&#160;This&#160;Research</b></a><br/>
<b>60</b><br/>
vii<br/>
<hr>
<A name=10></a><IMG src="thesis-10_1.jpg"/><br/>
<b>List&#160;of&#160;Figures</b><br/>
<a href="thesiss.html#22">3.1</a><br/>
<a href="thesiss.html#22">High-level&#160;system&#160;architecture&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
10<br/>
<a href="thesiss.html#27">3.2</a><br/>
<a href="thesiss.html#27">Learning&#160;phase&#160;flow&#160;chart</a><br/>
.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
15<br/>
<a href="thesiss.html#31">3.3</a><br/>
<a href="thesiss.html#31">Performing&#160;phase&#160;flow&#160;chart&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
19<br/>
<a href="thesiss.html#34">3.4</a><br/>
<a href="thesiss.html#34">Interval&#160;from/to&#160;neighbor&#160;notes&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
22<br/>
<a href="thesiss.html#34">3.5</a><br/>
<a href="thesiss.html#34">Relative&#160;duration&#160;with&#160;the&#160;previous/next&#160;note&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
22<br/>
<a href="thesiss.html#35">3.6</a><br/>
<a href="thesiss.html#35">Metric&#160;position&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
23<br/>
<a href="thesiss.html#36">3.7</a><br/>
<a href="thesiss.html#36">Systematic&#160;bias&#160;in&#160;onset&#160;deviation&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
24<br/>
<a href="thesiss.html#45">4.1</a><br/>
<a href="thesiss.html#45">Movement&#160;length&#160;(notes)&#160;distribution</a><br/>
.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
33<br/>
<a href="thesiss.html#47">4.2</a><br/>
<a href="thesiss.html#47">Movement&#160;length&#160;(phrases)&#160;distribution&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
35<br/>
<a href="thesiss.html#47">4.3</a><br/>
<a href="thesiss.html#47">Phrase&#160;length&#160;(notes)&#160;distribution&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
35<br/>
<a href="thesiss.html#49">5.1</a><br/>
<a href="thesiss.html#49">Onset&#160;deviations&#160;by&#160;aligning&#160;last&#160;note&#160;onset&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
37<br/>
<a href="thesiss.html#50">5.2</a><br/>
<a href="thesiss.html#50">Onset&#160;deviations&#160;by&#160;aligning&#160;last&#160;notes&#160;note-off&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
38<br/>
<a href="thesiss.html#51">5.3</a><br/>
<a href="thesiss.html#51">Onset&#160;deviations&#160;using&#160;automated&#160;normalization&#160;method&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
39<br/>
<a href="thesiss.html#53">5.4</a><br/>
<a href="thesiss.html#53">Median&#160;distance&#160;between&#160;generated&#160;performances&#160;and&#160;recordings&#160;for&#160;dif-</a><br/>
<a href="thesiss.html#53">ferent&#160;<i>ε</i>'s&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
41<br/>
<a href="thesiss.html#54">5.5</a><br/>
<a href="thesiss.html#54">Execution&#160;time&#160;for&#160;different&#160;<i>ε</i>'s&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
42<br/>
<a href="thesiss.html#55">5.6</a><br/>
<a href="thesiss.html#55">Median&#160;distance&#160;between&#160;generated&#160;performances&#160;and&#160;recordings&#160;for&#160;dif-</a><br/>
<a href="thesiss.html#55">ferent&#160;C's&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
43<br/>
<a href="thesiss.html#55">5.7</a><br/>
<a href="thesiss.html#55">Execution&#160;time&#160;for&#160;different&#160;C's&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
43<br/>
<a href="thesiss.html#57">5.8</a><br/>
<a href="thesiss.html#57">Execution&#160;time&#160;for&#160;differnt&#160;number&#160;of&#160;quantization&#160;levels&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
45<br/>
viii<br/>
<hr>
<A name=11></a><IMG src="thesis-11_1.jpg"/><br/>
<a href="thesiss.html#58">5.9</a><br/>
<a href="thesiss.html#58">Distribution&#160;of&#160;onset&#160;deviation&#160;values&#160;from&#160;full&#160;corpus&#160;versus&#160;single&#160;per-</a><br/>
<a href="thesiss.html#58">former's&#160;corpus&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
46<br/>
<a href="thesiss.html#59">5.10&#160;Distribution&#160;of&#160;duration&#160;ratio&#160;values&#160;from&#160;full&#160;corpus&#160;versus&#160;single&#160;per-</a><br/>
<a href="thesiss.html#59">former's&#160;Corpus&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
47<br/>
<a href="thesiss.html#59">5.11&#160;Distribution&#160;of&#160;MIDI&#160;velocity&#160;values&#160;from&#160;full&#160;corpus&#160;versus&#160;single&#160;per-</a><br/>
<a href="thesiss.html#59">former's&#160;corpus&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
47<br/>
ix<br/>
<hr>
<A name=12></a><IMG src="thesis-12_1.jpg"/><br/>
<b>List&#160;of&#160;Tables</b><br/>
<a href="thesiss.html#40">4.1</a><br/>
<a href="thesiss.html#40">Clementi's&#160;Sonatinas&#160;Op.36</a><br/>
.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
28<br/>
<a href="thesiss.html#44">4.2</a><br/>
<a href="thesiss.html#44">Number&#160;of&#160;mistakes&#160;in&#160;the&#160;corpus,&#160;blank&#160;cell&#160;means&#160;the&#160;performer&#160;didn't</a><br/>
<a href="thesiss.html#44">record&#160;the&#160;piece&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
32<br/>
<a href="thesiss.html#46">4.3</a><br/>
<a href="thesiss.html#46">Total&#160;recorded&#160;phrases&#160;and&#160;notes&#160;count&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
34<br/>
<a href="thesiss.html#46">4.4</a><br/>
<a href="thesiss.html#46">Phrases&#160;and&#160;notes&#160;count&#160;for&#160;Clementi's&#160;Sonatina&#160;Op.36&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
34<br/>
<a href="thesiss.html#60">5.1</a><br/>
<a href="thesiss.html#60">Average&#160;rating&#160;for&#160;generated&#160;performance&#160;and&#160;human&#160;recording</a><br/>
.&#160;.&#160;.&#160;.&#160;.<br/>
48<br/>
<a href="thesiss.html#61">5.2</a><br/>
<a href="thesiss.html#61">Average&#160;rating&#160;for&#160;generated&#160;performance&#160;and&#160;human&#160;recording&#160;under&#160;dif-</a><br/>
<a href="thesiss.html#61">ferent&#160;part&#160;of&#160;the&#160;corpus&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
49<br/>
<a href="thesiss.html#61">5.3</a><br/>
<a href="thesiss.html#61">Number&#160;of&#160;participants&#160;who&#160;gives&#160;higher&#160;rating&#160;to&#160;generated&#160;performance,</a><br/>
<a href="thesiss.html#61">human&#160;recordings&#160;or&#160;equal&#160;rating&#160;</a>.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.&#160;.<br/>
49<br/>
<a href="thesiss.html#61">5.4</a><br/>
<a href="thesiss.html#61">Number&#160;of&#160;participants&#160;who&#160;gives&#160;higher&#160;rating&#160;to&#160;generated&#160;performance,</a><br/>
<a href="thesiss.html#61">human&#160;recordings&#160;or&#160;equal&#160;rating&#160;under&#160;different&#160;part&#160;of&#160;the&#160;corpus&#160;</a>.&#160;.&#160;.&#160;.<br/>
49<br/>
x<br/>
<hr>
<A name=13></a><IMG src="thesis-13_1.jpg"/><br/>
<b>Chapter&#160;1</b><br/>
<b>Introduction</b><br/>
<b>1.1</b><br/>
<b>Motivation</b><br/>
From&#160;the&#160;mechanical&#160;music&#160;performing&#160;automata&#160;from&#160;middle&#160;ages,&#160;to&#160;the&#160;latest&#160;Japanese<br/>
virtual&#160;signer&#160;Hatune&#160;Miku,&#160;there&#160;had&#160;been&#160;many&#160;attempts&#160;to&#160;create&#160;automated&#160;systems&#160;that<br/>
perform&#160;music.&#160;However,&#160;many&#160;of&#160;these&#160;systems&#160;can&#160;only&#160;generate&#160;predefined&#160;expression.<br/>
State-of-the-art&#160;text-to-speech&#160;system&#160;can&#160;already&#160;generate&#160;fluid&#160;and&#160;natural&#160;speech,&#160;but<br/>
computer&#160;performance&#160;still&#160;can't&#160;perform&#160;very&#160;expressively.&#160;Therefore,&#160;many&#160;researcher<br/>
have&#160;devoted&#160;their&#160;effort&#160;to&#160;develop&#160;systems&#160;that&#160;can&#160;automatically&#160;or&#160;semi-automatically<br/>
perform&#160;music&#160;expressively.&#160;There&#160;is&#160;even&#160;a&#160;biannual&#160;contest&#160;for&#160;such&#160;systems&#160;called<br/>
Music&#160;Performance&#160;Rendering&#160;Contest&#160;(RenCon)&#160;[1].&#160;The&#160;RenCon&#160;roadmap&#160;suggest&#160;that<br/>
by&#160;2050,&#160;they&#160;wish&#160;that&#160;a&#160;computer&#160;performer&#160;can&#160;win&#160;the&#160;Chopin&#160;International&#160;Piano<br/>
Contest.<br/>
There&#160;are&#160;many&#160;potential&#160;applications&#160;for&#160;a&#160;computer&#160;expressive&#160;performance&#160;system,<br/>
many&#160;commercial&#160;music&#160;typesetting&#160;software&#160;like&#160;Finale&#160;[2]&#160;and&#160;Sibelius&#160;[3]&#160;already&#160;have<br/>
expressive&#160;playback&#160;features&#160;built-in.&#160;For&#160;entertainment&#160;industry,&#160;such&#160;system&#160;can&#160;provide<br/>
personalized&#160;music&#160;listening&#160;experience.&#160;For&#160;music&#160;production&#160;industry,&#160;this&#160;technology<br/>
can&#160;save&#160;a&#160;lot&#160;of&#160;cost&#160;on&#160;hiring&#160;musicians&#160;and&#160;license&#160;fees.&#160;Such&#160;system&#160;also&#160;opens&#160;up<br/>
new&#160;opportunity&#160;in&#160;art,&#160;such&#160;as&#160;human-machine&#160;co-performance&#160;or&#160;interactive&#160;multimedia<br/>
installation.&#160;In&#160;academia,&#160;researchers&#160;can&#160;use&#160;this&#160;technology&#160;to&#160;study&#160;the&#160;performance<br/>
style&#160;of&#160;musicians,&#160;or&#160;restore&#160;historical&#160;recording&#160;archive.<br/>
1<br/>
<hr>
<A name=14></a><IMG src="thesis-14_1.jpg"/><br/>
<b>1.2</b><br/>
<b>Goal&#160;and&#160;Contribution</b><br/>
The&#160;ultimate&#160;goal&#160;of&#160;this&#160;paper&#160;is&#160;to&#160;be&#160;able&#160;to&#160;play&#160;any&#160;music&#160;in&#160;any&#160;expressive&#160;style<br/>
specified.&#160;But&#160;due&#160;to&#160;technical&#160;and&#160;time&#160;constrains,&#160;we&#160;narrow&#160;down&#160;our&#160;goal&#160;to&#160;building<br/>
a&#160;computer&#160;expressive&#160;performance&#160;system&#160;that&#160;performs&#160;monophonic&#160;music&#160;phrases&#160;by<br/>
off-line&#160;supervised&#160;learning.&#160;The&#160;phrasing&#160;need&#160;to&#160;be&#160;annotated&#160;by&#160;human,&#160;so&#160;it's&#160;a&#160;semi-<br/>
automatic&#160;system.<br/>
The&#160;major&#160;contribution&#160;of&#160;this&#160;paper&#160;is&#160;that&#160;we&#160;apply&#160;structural&#160;support&#160;vector&#160;machine<br/>
on&#160;expressive&#160;performance&#160;problem.&#160;There&#160;exist&#160;no&#160;previous&#160;work&#160;that&#160;uses&#160;the&#160;discrim-<br/>
inative&#160;learning&#160;power&#160;of&#160;structural&#160;support&#160;vector&#160;machine&#160;with&#160;hidden&#160;Markov&#160;model<br/>
output&#160;(SVM-HMM)&#160;on&#160;computer&#160;expressive&#160;performance&#160;question.&#160;We&#160;also&#160;developed<br/>
methods&#160;and&#160;tools&#160;to&#160;generate&#160;a&#160;expressive&#160;performance&#160;corpus.<br/>
<b>1.3</b><br/>
<b>Chapter&#160;Organization</b><br/>
In&#160;Chapter&#160;<a href="thesiss.html#15">2</a>,&#160;we&#160;will&#160;give&#160;an&#160;overview&#160;of&#160;previous&#160;works&#160;and&#160;their&#160;varying&#160;goals,&#160;these<br/>
works&#160;will&#160;be&#160;grouped&#160;by&#160;the&#160;way&#160;they&#160;learn&#160;performance&#160;knowledge,&#160;and&#160;we&#160;will&#160;discuss<br/>
some&#160;additional&#160;specialities&#160;such&#160;as&#160;special&#160;instrument&#160;model&#160;or&#160;special&#160;user&#160;interaction<br/>
pattern.&#160;In&#160;Chapter&#160;<a href="thesiss.html#21">3,&#160;</a>we&#160;will&#160;first&#160;give&#160;a&#160;brief&#160;introduction&#160;to&#160;the&#160;mathematical&#160;back-<br/>
ground&#160;of&#160;SVM-HMM,&#160;and&#160;then&#160;give&#160;a&#160;top-down&#160;explanation&#160;to&#160;the&#160;proposed&#160;method.&#160;In<br/>
Chapter&#160;<a href="thesiss.html#38">4</a>,&#160;we&#160;will&#160;explain&#160;how&#160;the&#160;corpus&#160;used&#160;for&#160;training&#160;is&#160;designed&#160;and&#160;implemented.<br/>
In&#160;Chapter&#160;<a href="thesiss.html#48">5</a>,&#160;we&#160;will&#160;discuss&#160;several&#160;experiments&#160;that&#160;demostrates&#160;design&#160;trade-offs&#160;and<br/>
the&#160;subjective&#160;test&#160;results.&#160;Finally,&#160;we&#160;have&#160;included&#160;an&#160;appendix&#160;that&#160;presents&#160;some&#160;soft-<br/>
ware&#160;tools&#160;used&#160;in&#160;this&#160;research,&#160;which&#160;may&#160;be&#160;helpful&#160;for&#160;other&#160;researchers&#160;in&#160;the&#160;computer<br/>
music&#160;field.<br/>
2<br/>
<hr>
<A name=15></a><IMG src="thesis-15_1.jpg"/><br/>
<b>Chapter&#160;2</b><br/>
<b>Previous&#160;Works</b><br/>
<b>2.1</b><br/>
<b>Various&#160;Goals&#160;and&#160;Evaluation</b><br/>
The&#160;general&#160;goal&#160;of&#160;a&#160;computer&#160;expressive&#160;performance&#160;system&#160;is&#160;to&#160;generate&#160;expres-<br/>
sive&#160;music,&#160;as&#160;opposed&#160;to&#160;the&#160;robotic&#160;and&#160;dull&#160;expression&#160;of&#160;rendered&#160;MIDI.&#160;But&#160;the&#160;defi-<br/>
nition&#160;of&#160;“expressive”&#160;is&#160;very&#160;vague&#160;and&#160;ambiguous,&#160;so&#160;each&#160;research&#160;will&#160;need&#160;to&#160;define&#160;a<br/>
more&#160;precise&#160;and&#160;measurable&#160;goal.&#160;The&#160;following&#160;are&#160;the&#160;most&#160;popular&#160;goals&#160;a&#160;computer<br/>
expressive&#160;performance&#160;system&#160;wants&#160;to&#160;achieve:<br/>
1.&#160;Perform&#160;music&#160;notations&#160;in&#160;a&#160;non-robotic&#160;way&#160;(no&#160;specific&#160;style).<br/>
2.&#160;Reproduce&#160;a&#160;human&#160;performance&#160;or&#160;a&#160;certain&#160;musician's&#160;style.<br/>
3.&#160;Accompany&#160;a&#160;human&#160;performance.<br/>
4.&#160;Validate&#160;a&#160;musicological&#160;theory&#160;of&#160;expressive&#160;performance.<br/>
5.&#160;Directly&#160;render&#160;computer-composed&#160;music&#160;works.<br/>
Some&#160;systems&#160;try&#160;to&#160;perform&#160;music&#160;notations&#160;in&#160;a&#160;non-robotic&#160;way&#160;in&#160;a&#160;general&#160;sense,<br/>
without&#160;a&#160;certain&#160;style&#160;in&#160;mind.&#160;These&#160;systems&#160;has&#160;been&#160;employed&#160;in&#160;music&#160;typesetting<br/>
softwares,&#160;like&#160;Finale&#160;[2]&#160;and&#160;Sibelius&#160;[3],&#160;to&#160;play&#160;the&#160;notation&#160;expressively.&#160;Most&#160;systems<br/>
will&#160;implicitly&#160;include&#160;this&#160;goal.<br/>
Systems&#160;that&#160;are&#160;designed&#160;to&#160;reproduce&#160;certain&#160;human&#160;performance&#160;or&#160;style&#160;are&#160;usually<br/>
designed&#160;and&#160;trained&#160;using&#160;a&#160;particular&#160;performer's&#160;recordings.&#160;One&#160;commercial&#160;example<br/>
3<br/>
<hr>
<A name=16></a><IMG src="thesis-16_1.jpg"/><br/>
is&#160;the&#160;Zenph&#160;re-performance&#160;CD&#160;[4].&#160;This&#160;CD&#160;contains&#160;music&#160;performed&#160;by&#160;an&#160;expressive<br/>
performance&#160;model&#160;of&#160;Rachimaninov's&#160;style,&#160;but&#160;Rachimaninov&#160;had&#160;never&#160;recorded&#160;these<br/>
pieces&#160;in&#160;his&#160;lifetime.<br/>
Accompaniment&#160;systems&#160;try&#160;to&#160;render&#160;expressive&#160;music&#160;that&#160;act&#160;as&#160;an&#160;accompaniment<br/>
for&#160;a&#160;human&#160;performance.&#160;The&#160;challenge&#160;is&#160;that&#160;the&#160;system&#160;must&#160;be&#160;able&#160;to&#160;track&#160;the<br/>
progress&#160;of&#160;a&#160;human&#160;performance&#160;and&#160;adaptively&#160;render&#160;the&#160;accompaniment&#160;in&#160;real-time.<br/>
One&#160;commercial&#160;example&#160;is&#160;Cadenza&#160;[5],&#160;using&#160;the&#160;technology&#160;created&#160;by&#160;Christopher<br/>
Raphel.&#160;It&#160;can&#160;track&#160;the&#160;soloist's&#160;performance&#160;and&#160;play&#160;the&#160;accompaniment&#160;orchestral&#160;part<br/>
accordingly.<br/>
Another&#160;goal&#160;is&#160;to&#160;validate&#160;musicological&#160;theories.&#160;Musicologist&#160;may&#160;propose&#160;theories<br/>
on&#160;expressive&#160;music&#160;performance,&#160;by&#160;building&#160;a&#160;generative&#160;model,&#160;they&#160;can&#160;validate&#160;their<br/>
theories.&#160;These&#160;systems&#160;may&#160;focus&#160;more&#160;on&#160;the&#160;specific&#160;phenomenon&#160;that&#160;the&#160;theory&#160;tries<br/>
to&#160;explain&#160;instead&#160;of&#160;generating&#160;music&#160;that&#160;is&#160;pleasant&#160;to&#160;human.<br/>
Finally,&#160;some&#160;systems&#160;combines&#160;computer&#160;composition&#160;with&#160;expressive&#160;performance.<br/>
These&#160;systems&#160;have&#160;a&#160;big&#160;advantage&#160;because&#160;the&#160;intention&#160;of&#160;the&#160;composer&#160;can&#160;be&#160;shared<br/>
with&#160;the&#160;performer.&#160;Other&#160;systems&#160;that&#160;performs&#160;past&#160;compositions&#160;can&#160;only&#160;guess&#160;the<br/>
composer's&#160;intention&#160;by&#160;analyzing&#160;the&#160;score&#160;notation.&#160;These&#160;systems&#160;usually&#160;has&#160;their<br/>
own&#160;data&#160;structure&#160;to&#160;represent&#160;music,&#160;which&#160;can&#160;contain&#160;more&#160;information&#160;than&#160;tradi-<br/>
tional&#160;music&#160;notation,&#160;but&#160;the&#160;performance&#160;system&#160;is&#160;not&#160;backward&#160;compatible&#160;with&#160;past<br/>
compositions.<br/>
Because&#160;of&#160;the&#160;high&#160;diversity&#160;in&#160;the&#160;goals&#160;they&#160;want&#160;to&#160;achieve,&#160;it&#160;is&#160;very&#160;hard&#160;to&#160;make<br/>
fair&#160;comparison&#160;between&#160;systems.&#160;But&#160;we&#160;can&#160;still&#160;evaluate&#160;the&#160;capability&#160;of&#160;these&#160;systems<br/>
by&#160;the&#160;following&#160;three&#160;key&#160;indicators&#160;proposed&#160;by&#160;[6]:<br/>
1.&#160;Expressive&#160;expression&#160;capability<br/>
2.&#160;Polyphonic&#160;capability<br/>
3.&#160;Performance&#160;creativity<br/>
Expressive&#160;expression&#160;capability&#160;can&#160;range&#160;from&#160;very&#160;high&#160;level&#160;structural&#160;expression<br/>
(e.g.&#160;tempo&#160;contrast&#160;between&#160;sections)&#160;to&#160;note&#160;level&#160;expression&#160;(e.g.&#160;onset,&#160;loudness,<br/>
4<br/>
<hr>
<A name=17></a><IMG src="thesis-17_1.jpg"/><br/>
duration)&#160;or&#160;even&#160;sub-note&#160;expression&#160;(e.g.&#160;loudness&#160;envelop,&#160;timbre).&#160;Most&#160;systems&#160;can<br/>
generate&#160;note-level&#160;expression,&#160;but&#160;higher&#160;or&#160;lower&#160;level&#160;expressions&#160;are&#160;much&#160;rare.<br/>
Polyphonic&#160;capability&#160;indicates&#160;if&#160;the&#160;system&#160;can&#160;perform&#160;polyphonic&#160;input.&#160;Poly-<br/>
phonic&#160;systems&#160;are&#160;more&#160;challenging&#160;than&#160;monophonic&#160;ones&#160;because&#160;they&#160;requires&#160;syn-<br/>
chronization&#160;between&#160;voices.<br/>
Performance&#160;creativity&#160;measures&#160;the&#160;ability&#160;of&#160;the&#160;system&#160;to&#160;create&#160;novel&#160;expression.<br/>
The&#160;desired&#160;level&#160;of&#160;creativity&#160;varies&#160;from&#160;goal&#160;to&#160;goal.&#160;A&#160;system&#160;aiming&#160;to&#160;recreate<br/>
human&#160;performance&#160;may&#160;want&#160;to&#160;produce&#160;deterministic&#160;expressions&#160;based&#160;on&#160;the&#160;learning<br/>
material,&#160;while&#160;a&#160;system&#160;that&#160;is&#160;combined&#160;with&#160;a&#160;composition&#160;system&#160;may&#160;want&#160;to&#160;create<br/>
highly&#160;novel&#160;performance.<br/>
Each&#160;system&#160;will&#160;design&#160;different&#160;experiment&#160;and&#160;metrics&#160;to&#160;verify&#160;their&#160;goals.&#160;Thus,<br/>
the&#160;self-reported&#160;results&#160;are&#160;can&#160;hardly&#160;be&#160;compared.&#160;The&#160;only&#160;public&#160;contest&#160;that&#160;eval-<br/>
uates&#160;expressive&#160;performance&#160;systems&#160;is&#160;called&#160;RenCon&#160;(Performance&#160;Rendering&#160;Con-<br/>
test)&#160;[1].&#160;Scores&#160;(MIDI)&#160;will&#160;be&#160;given&#160;to&#160;participants&#160;one&#160;hour&#160;before&#160;the&#160;competition<br/>
starts.&#160;The&#160;participants&#160;must&#160;generate&#160;the&#160;expressive&#160;version&#160;of&#160;the&#160;MIDIs&#160;in&#160;the&#160;given<br/>
time,&#160;the&#160;MIDIs&#160;will&#160;be&#160;played&#160;live&#160;on&#160;a&#160;Yamaha&#160;Disklavier&#160;piano.&#160;The&#160;audience&#160;and&#160;a<br/>
jury&#160;cosists&#160;of&#160;professional&#160;musicians&#160;will&#160;give&#160;ratings&#160;for&#160;each&#160;performance.&#160;The&#160;perfor-<br/>
mances&#160;are&#160;played&#160;in&#160;random&#160;order,&#160;so&#160;the&#160;audience&#160;and&#160;jury&#160;won't&#160;know&#160;which&#160;participant<br/>
is&#160;behind&#160;each&#160;performance.<br/>
The&#160;RenCon&#160;is&#160;divided&#160;into&#160;fully&#160;automatic&#160;and&#160;semi-automatic&#160;categories.&#160;But&#160;the<br/>
degree&#160;of&#160;human&#160;intervention&#160;in&#160;the&#160;semi-automatic&#160;category&#160;varies&#160;widely&#160;between&#160;sys-<br/>
tems.&#160;So&#160;it's&#160;not&#160;very&#160;fair&#160;to&#160;compare&#160;them.<br/>
<b>2.2</b><br/>
<b>Researches&#160;Classified&#160;by&#160;Methods&#160;Used</b><br/>
Despite&#160;the&#160;difference&#160;between&#160;goals&#160;of&#160;different&#160;expressive&#160;performance&#160;systems,&#160;all<br/>
expressive&#160;performance&#160;systems&#160;must&#160;have&#160;some&#160;strategy&#160;to&#160;learn&#160;and&#160;apply&#160;performance<br/>
knowledge.&#160;There&#160;are&#160;generally&#160;two&#160;approach:&#160;rule-based&#160;or&#160;machine-learning-based.<br/>
Using&#160;rules&#160;to&#160;generate&#160;expressive&#160;music&#160;is&#160;probably&#160;the&#160;earliest&#160;approach.&#160;Direc-<br/>
tor&#160;Musices&#160;[7]&#160;is&#160;one&#160;of&#160;the&#160;early&#160;example.&#160;Pop-E&#160;[8]&#160;is&#160;also&#160;a&#160;rule-based&#160;system&#160;which<br/>
5<br/>
<hr>
<A name=18></a><IMG src="thesis-18_1.jpg"/><br/>
can&#160;generate&#160;polyphonic&#160;music,&#160;using&#160;its&#160;voice&#160;synchronization&#160;algorithm.&#160;Computational<br/>
Music&#160;Emotion&#160;Rule&#160;System&#160;[9]&#160;tried&#160;to&#160;develop&#160;rules&#160;that&#160;express&#160;human&#160;emotions.&#160;Other<br/>
systems&#160;like&#160;Hierarchical&#160;Parabola&#160;System&#160;[7,&#160;10--12],&#160;Composer&#160;Pulse&#160;System&#160;[13,&#160;14],<br/>
Bach&#160;Fugue&#160;System&#160;[15],&#160;Trumpet&#160;Synthesis&#160;System&#160;[16,&#160;17]&#160;and&#160;Rubato&#160;[18,&#160;19]&#160;are&#160;also<br/>
some&#160;examples.&#160;Most&#160;of&#160;the&#160;rule-based&#160;systems&#160;focus&#160;on&#160;expressive&#160;attributes&#160;like&#160;note&#160;on-<br/>
set,&#160;note&#160;duration&#160;and&#160;loudness,&#160;but&#160;Hermode&#160;Tuning&#160;System&#160;[20]&#160;put&#160;special&#160;emphasis&#160;on<br/>
intonation.&#160;Rule-based&#160;systems&#160;are&#160;generally&#160;more&#160;computationally&#160;efficient&#160;because&#160;the<br/>
mathematical&#160;model&#160;is&#160;much&#160;simple&#160;than&#160;those&#160;learned&#160;by&#160;machine&#160;learning&#160;algorithms.<br/>
And&#160;rules&#160;are&#160;generally&#160;more&#160;understandable&#160;to&#160;human&#160;than&#160;complex&#160;model&#160;parameters.<br/>
But&#160;some&#160;of&#160;the&#160;nuance,&#160;such&#160;as&#160;some&#160;subconscious&#160;deviation,&#160;may&#160;be&#160;hard&#160;to&#160;describe&#160;by<br/>
rules,&#160;so&#160;there&#160;is&#160;a&#160;emperical&#160;limit&#160;on&#160;how&#160;complex&#160;the&#160;rule-based&#160;system&#160;can&#160;be.&#160;Lack&#160;of<br/>
creativity&#160;is&#160;also&#160;a&#160;problem&#160;for&#160;rule-based&#160;approach.<br/>
Another&#160;approach&#160;is&#160;to&#160;acquire&#160;performance&#160;knowledge&#160;by&#160;machine&#160;learning.&#160;Many<br/>
machine&#160;learning&#160;methods&#160;have&#160;already&#160;been&#160;applied&#160;to&#160;this&#160;problem.&#160;For&#160;example,&#160;Music<br/>
Interpretation&#160;System&#160;[21--23]&#160;and&#160;CaRo&#160;[24--26]&#160;both&#160;use&#160;linear&#160;regression&#160;to&#160;learn&#160;per-<br/>
formance&#160;knowledge.&#160;But&#160;it&#160;is&#160;very&#160;unlikely&#160;that&#160;the&#160;expressive&#160;performance&#160;problem&#160;is&#160;a<br/>
linear&#160;system,&#160;so&#160;Music&#160;Interpretation&#160;System&#160;try&#160;to&#160;introduce&#160;non-linearity&#160;by&#160;using&#160;logic<br/>
AND&#160;operations&#160;on&#160;linear&#160;regression&#160;results.&#160;But&#160;generally&#160;speaking,&#160;linear&#160;regression&#160;is<br/>
too&#160;simple&#160;to&#160;capture&#160;the&#160;core&#160;of&#160;expressive&#160;performance.<br/>
More&#160;complicated&#160;machine-learning&#160;algorithms&#160;have&#160;also&#160;been&#160;applied:&#160;ANN&#160;Piano<br/>
[27]&#160;and&#160;Emotional&#160;flute&#160;[28]&#160;uses&#160;artificial&#160;neural&#160;network.&#160;ESP&#160;Piano&#160;[29]&#160;and&#160;Music<br/>
Plus&#160;One&#160;[30--32]&#160;uses&#160;statistical&#160;graphical&#160;models&#160;such&#160;as&#160;hidden&#160;Markov&#160;model&#160;(HMM)<br/>
and&#160;Bayesian&#160;belief&#160;network,&#160;but&#160;they&#160;did&#160;no&#160;use&#160;structural&#160;support&#160;vector&#160;machine&#160;to&#160;train<br/>
the&#160;HMM.KCCA&#160;Piano&#160;System&#160;[33]&#160;uses&#160;kernel&#160;regression.&#160;Drumming&#160;System&#160;[34]&#160;tried<br/>
different&#160;mapping&#160;models&#160;that&#160;generates&#160;drum&#160;patterns.<br/>
Evolutionary&#160;computation&#160;such&#160;as&#160;genetic&#160;programming&#160;is&#160;used&#160;in&#160;Genetic&#160;Program-<br/>
ming&#160;Jazz&#160;Sax&#160;[35],&#160;Sequential&#160;Covering&#160;Algorithm&#160;Genetic&#160;Algorith&#160;[36],&#160;Generative<br/>
Performance&#160;Genetic&#160;Algorithm&#160;[37]&#160;and&#160;Multi-Agent&#160;System&#160;with&#160;Imitation&#160;[38,&#160;39].<br/>
Evolutionary&#160;computation&#160;takes&#160;long&#160;training&#160;time,&#160;and&#160;the&#160;results&#160;are&#160;less&#160;predictable.<br/>
6<br/>
<hr>
<A name=19></a><IMG src="thesis-19_1.jpg"/><br/>
But&#160;being&#160;unpredictable&#160;also&#160;means&#160;that&#160;these&#160;systems&#160;can&#160;create&#160;interesting&#160;performances<br/>
in&#160;an&#160;unconventional&#160;way.<br/>
Another&#160;possible&#160;approach&#160;is&#160;to&#160;use&#160;case-based&#160;reasoning.&#160;SaxE&#160;[40--42]&#160;use&#160;fuzzy<br/>
rules&#160;based&#160;on&#160;emotions&#160;to&#160;generate&#160;Jazz&#160;saxophone&#160;performance.&#160;Kagurame&#160;[43,&#160;44]<br/>
focus&#160;on&#160;style&#160;(Baroque,&#160;Romantic,&#160;Classic&#160;etc.)&#160;instead&#160;of&#160;emotion.&#160;Ha-Hi-Hun&#160;[45]&#160;has<br/>
a&#160;more&#160;ambitions&#160;goal&#160;in&#160;mind:&#160;to&#160;accept&#160;natural&#160;language&#160;instructions&#160;like&#160;“Perform&#160;piece<br/>
X&#160;in&#160;the&#160;style&#160;of&#160;Y.”&#160;Another&#160;series&#160;of&#160;researches&#160;done&#160;by&#160;Widmer&#160;at&#160;el.,&#160;called&#160;PLCG&#160;[46--<br/>
48],&#160;uses&#160;data&#160;mining&#160;technique&#160;to&#160;find&#160;rules&#160;for&#160;expressive&#160;performance.&#160;It's&#160;successor<br/>
--&#160;Phrase-decomposition/PLCG&#160;[49]&#160;added&#160;hierarchical&#160;phrase&#160;structures&#160;support&#160;to&#160;the<br/>
original&#160;PLCG&#160;system.&#160;And&#160;the&#160;latest&#160;research&#160;in&#160;the&#160;series&#160;called&#160;DISTALL&#160;[50,&#160;51]<br/>
added&#160;hierarchical&#160;rules&#160;to&#160;the&#160;original&#160;one.<br/>
Most&#160;of&#160;the&#160;performance&#160;systems&#160;discussed&#160;above&#160;takes&#160;musical&#160;notation&#160;(MusicXML,<br/>
MIDI,&#160;etc.)&#160;or&#160;inexpressive&#160;audio&#160;as&#160;input.&#160;They&#160;have&#160;to&#160;figures&#160;out&#160;the&#160;expressive&#160;inten-<br/>
tion&#160;of&#160;the&#160;composer&#160;by&#160;analyzing&#160;the&#160;score.&#160;But&#160;another&#160;type&#160;of&#160;computer&#160;expressive&#160;per-<br/>
formance&#160;has&#160;a&#160;big&#160;advantage&#160;over&#160;the&#160;previous&#160;described&#160;ones,&#160;by&#160;combining&#160;computer<br/>
composition&#160;and&#160;expressive&#160;performance,&#160;the&#160;performance&#160;module&#160;can&#160;receive&#160;the&#160;com-<br/>
position&#160;intention&#160;directly&#160;from&#160;the&#160;composition&#160;module.&#160;Ossia&#160;[52]&#160;and&#160;pMIMACS&#160;[53]<br/>
are&#160;two&#160;examples&#160;of&#160;this&#160;category.&#160;This&#160;approach&#160;provides&#160;great&#160;possibility&#160;for&#160;creativity,<br/>
but&#160;they&#160;can&#160;only&#160;play&#160;their&#160;own&#160;composition,&#160;which&#160;limits&#160;it&#160;range&#160;of&#160;application.<br/>
<b>2.3</b><br/>
<b>Additional&#160;Specialties</b><br/>
Most&#160;expressive&#160;performance&#160;systems&#160;implicitly&#160;or&#160;explicitly&#160;generate&#160;piano&#160;perfor-<br/>
mance,&#160;because&#160;it's&#160;relatively&#160;easy&#160;to&#160;collect&#160;training&#160;samples&#160;for&#160;piano,&#160;and&#160;piano&#160;sound&#160;is<br/>
relatively&#160;easy&#160;to&#160;synthesize.&#160;Yet,&#160;some&#160;systems&#160;generate&#160;music&#160;in&#160;other&#160;instruments,&#160;such<br/>
as&#160;saxophone&#160;[40--42],&#160;trumpet&#160;[16,17],&#160;flute&#160;[28]&#160;and&#160;drums&#160;[54].&#160;These&#160;systems&#160;requires<br/>
extra&#160;effort&#160;in&#160;creating&#160;instrument&#160;models&#160;in&#160;training,&#160;generation&#160;and&#160;synthesizing.&#160;Y.-H<br/>
Kuo&#160;et&#160;al.&#160;[55]&#160;also&#160;propsed&#160;a&#160;way&#160;to&#160;re-synthsize&#160;individual&#160;notes&#160;into&#160;a&#160;performance&#160;with<br/>
smooth&#160;timbre&#160;variation,&#160;but&#160;the&#160;work&#160;focus&#160;more&#160;on&#160;sub-note&#160;level&#160;timbre&#160;systhsis.<br/>
If&#160;not&#160;specified,&#160;most&#160;systems&#160;handles&#160;traditional&#160;western&#160;tonal&#160;music.&#160;However,&#160;most<br/>
7<br/>
<hr>
<A name=20></a><IMG src="thesis-20_1.jpg"/><br/>
saxophone-based&#160;work&#160;[40--42]&#160;generates&#160;Jazz&#160;music,&#160;because&#160;saxophone&#160;is&#160;an&#160;iconic&#160;in-<br/>
strument&#160;in&#160;Jazz&#160;performance.&#160;And&#160;the&#160;Drumming&#160;System&#160;[54]&#160;generates&#160;Brazilian&#160;drum-<br/>
ming&#160;music.<br/>
Performing&#160;polyphonic&#160;music&#160;is&#160;much&#160;more&#160;challenging&#160;than&#160;monophonic&#160;music,&#160;be-<br/>
cause&#160;it&#160;requires&#160;synchronization&#160;between&#160;voices.&#160;Pop-E&#160;[8]&#160;use&#160;a&#160;synchronization&#160;mech-<br/>
anism&#160;to&#160;achieve&#160;polyphonic&#160;performance.&#160;Bach&#160;Fugue&#160;System&#160;[15]&#160;is&#160;created&#160;using&#160;the<br/>
polyphonic&#160;rules&#160;in&#160;music&#160;theory&#160;about&#160;fugue,&#160;so&#160;it's&#160;inherently&#160;able&#160;to&#160;play&#160;polyphonic<br/>
fugue.&#160;KCCA&#160;Piano&#160;System&#160;[33]&#160;can&#160;generate&#160;homophonic&#160;music&#160;--&#160;an&#160;upper&#160;melody&#160;with<br/>
an&#160;accompaniment&#160;--&#160;which&#160;is&#160;common&#160;in&#160;piano&#160;music.&#160;Music&#160;Plus&#160;One&#160;[30--32]&#160;is&#160;a&#160;lit-<br/>
tle&#160;bit&#160;different&#160;because&#160;it's&#160;a&#160;accompaniment&#160;system,&#160;it&#160;adapts&#160;non-expressive&#160;orchestral<br/>
accompaniment&#160;track&#160;to&#160;user's&#160;performance.<br/>
8<br/>
<hr>
<A name=21></a><IMG src="thesis-21_1.jpg"/><br/>
<b>Chapter&#160;3</b><br/>
<b>Proposed&#160;Method</b><br/>
<b>3.1</b><br/>
<b>Overview</b><br/>
The&#160;high-level&#160;architecture&#160;of&#160;the&#160;purposed&#160;system&#160;is&#160;shown&#160;in&#160;Fig.&#160;<a href="thesiss.html#22">3.1</a>.&#160;The&#160;system<br/>
has&#160;two&#160;phases,&#160;the&#160;upper&#160;half&#160;of&#160;the&#160;figure&#160;is&#160;the&#160;learning&#160;phase,&#160;the&#160;lower&#160;half&#160;is&#160;the&#160;per-<br/>
forming&#160;phase.&#160;In&#160;the&#160;learning&#160;phase,&#160;score&#160;and&#160;expressive&#160;human&#160;recording&#160;pairs,&#160;split<br/>
into&#160;phrases&#160;by&#160;human,&#160;are&#160;used&#160;as&#160;training&#160;examples&#160;for&#160;structural&#160;support&#160;vector&#160;machine<br/>
with&#160;hidden&#160;Markov&#160;model&#160;output&#160;(SVM-HMM)&#160;algorithm&#160;to&#160;learn&#160;performance&#160;knowl-<br/>
edge&#160;model.&#160;In&#160;the&#160;performing&#160;phase,&#160;a&#160;score&#160;will&#160;be&#160;given&#160;to&#160;the&#160;system&#160;for&#160;expressive<br/>
performance.&#160;The&#160;SVM-HMM&#160;generation&#160;module&#160;will&#160;use&#160;the&#160;performance&#160;knowledge<br/>
learned&#160;in&#160;the&#160;previous&#160;phase&#160;to&#160;produce&#160;expressive&#160;performance.&#160;The&#160;SVM-HMM&#160;output<br/>
then&#160;go&#160;through&#160;a&#160;MIDI&#160;generator&#160;and&#160;MIDI&#160;synthesizer&#160;to&#160;produce&#160;audible&#160;performance.<br/>
All&#160;the&#160;scores&#160;and&#160;recordings&#160;are&#160;monophonic&#160;and&#160;contains&#160;only&#160;one&#160;musical&#160;phrase.<br/>
The&#160;phrasing&#160;is&#160;done&#160;by&#160;human,&#160;thus&#160;the&#160;system&#160;is&#160;semi-automatic.&#160;The&#160;learning&#160;algo-<br/>
rithm,&#160;namely&#160;SVM-HMM,&#160;can&#160;only&#160;perform&#160;off-line&#160;learning,&#160;so&#160;the&#160;learning&#160;phase&#160;can<br/>
only&#160;work&#160;in&#160;a&#160;non-realtime&#160;scenario.&#160;The&#160;generating&#160;phase&#160;can&#160;work&#160;much&#160;faster,&#160;ex-<br/>
pressive&#160;music&#160;can&#160;be&#160;generated&#160;almost&#160;instantaneously.<br/>
There&#160;are&#160;many&#160;ways&#160;the&#160;user&#160;can&#160;control&#160;the&#160;performance&#160;style&#160;of&#160;the&#160;final&#160;output:<br/>
first,&#160;the&#160;user&#160;can&#160;choose&#160;the&#160;training&#160;corpus.&#160;Theoratically,&#160;a&#160;model&#160;of&#160;a&#160;particular&#160;style<br/>
can&#160;be&#160;learned&#160;from&#160;a&#160;set&#160;of&#160;samples&#160;with&#160;that&#160;particular&#160;style.&#160;Second,&#160;the&#160;user&#160;can&#160;control<br/>
the&#160;structural&#160;expression&#160;by&#160;assigning&#160;the&#160;phrasing.<br/>
9<br/>
<hr>
<A name=22></a><IMG src="thesis-22_1.jpg"/><br/>
Figure&#160;3.1:&#160;High-level&#160;system&#160;architecture<br/>
In&#160;the&#160;following&#160;sections,&#160;we&#160;will&#160;give&#160;an&#160;overview&#160;of&#160;the&#160;theroratical&#160;background<br/>
behind&#160;SVM-HMM,&#160;and&#160;then&#160;walk&#160;through&#160;the&#160;detail&#160;steps&#160;in&#160;the&#160;learning&#160;and&#160;performing<br/>
phases,&#160;and&#160;some&#160;implementation&#160;detail.&#160;The&#160;features&#160;used&#160;will&#160;be&#160;presented&#160;in&#160;the&#160;end&#160;of<br/>
this&#160;chapter.<br/>
<b>3.2</b><br/>
<b>A&#160;Brief&#160;Introduction&#160;to&#160;SVM-HMM</b><br/>
In&#160;this&#160;thesis,&#160;we&#160;use&#160;structural&#160;support&#160;vector&#160;machine&#160;to&#160;learn&#160;performance&#160;knowl-<br/>
edge&#160;from&#160;expressive&#160;performance&#160;samples.&#160;Unlike&#160;traditional&#160;SVM&#160;algorithm,&#160;which<br/>
can&#160;only&#160;produce&#160;univariate&#160;prediction,&#160;structural&#160;SVM&#160;can&#160;produce&#160;structural&#160;predic-<br/>
tions&#160;like&#160;tree,&#160;sequence&#160;and&#160;hidden&#160;Markov&#160;model.&#160;Structural&#160;SVM&#160;with&#160;hidden&#160;Markov<br/>
model&#160;output&#160;(SVM-HMM)&#160;has&#160;been&#160;successfully&#160;applied&#160;to&#160;part-of-speech&#160;tagging&#160;prob-<br/>
lem&#160;[56].&#160;The&#160;part-of-speech&#160;tagging&#160;problem&#160;has&#160;some&#160;similarity&#160;with&#160;expressive&#160;per-<br/>
formance&#160;problem.&#160;In&#160;part-of-speech&#160;tagging,&#160;one&#160;tries&#160;to&#160;identify&#160;the&#160;role&#160;by&#160;which&#160;the<br/>
word&#160;plays&#160;in&#160;the&#160;sentence,&#160;while&#160;in&#160;expressive&#160;performance,&#160;one&#160;tries&#160;to&#160;determine&#160;how<br/>
a&#160;note&#160;should&#160;be&#160;played,&#160;usually&#160;based&#160;on&#160;it's&#160;role&#160;in&#160;the&#160;musical&#160;phrase.&#160;For&#160;example,<br/>
an&#160;authentic&#160;cadence&#160;at&#160;the&#160;end&#160;of&#160;a&#160;phrase&#160;is&#160;usually&#160;played&#160;louder&#160;and&#160;stronger&#160;than<br/>
a&#160;embellishment&#160;note&#160;in&#160;the&#160;middle&#160;of&#160;a&#160;phrase.&#160;Thus,&#160;we&#160;believe&#160;SVM-HMM&#160;will&#160;be<br/>
a&#160;good&#160;candidate&#160;for&#160;expressive&#160;performance.&#160;The&#160;following&#160;introduction&#160;and&#160;formulas<br/>
10<br/>
<hr>
<A name=23></a><IMG src="thesis-23_1.jpg"/><br/>
relies&#160;heavily&#160;on&#160;[56--58].<br/>
Traditional&#160;SVM&#160;prediction&#160;problem&#160;can&#160;be&#160;described&#160;as&#160;finding&#160;a&#160;function<br/>
<i>h&#160;</i>:&#160;X&#160;→&#160;Y<br/>
with&#160;lowest&#160;prediction&#160;error.&#160;X&#160;is&#160;the&#160;input&#160;features&#160;space,&#160;and&#160;Y&#160;is&#160;the&#160;prediction&#160;space.<br/>
In&#160;traditional&#160;SVM,&#160;elements&#160;in&#160;Y&#160;are&#160;labels&#160;(classification)&#160;or&#160;real&#160;values&#160;(regression).<br/>
But&#160;structural&#160;SVM&#160;extends&#160;the&#160;framework&#160;to&#160;generate&#160;structural&#160;output,&#160;such&#160;as&#160;tree,<br/>
sequence,&#160;or&#160;hidden&#160;Markov&#160;model.&#160;To&#160;extend&#160;SVM&#160;to&#160;support&#160;structured&#160;output,&#160;the<br/>
problem&#160;is&#160;modified&#160;as&#160;finding&#160;a&#160;discriminant&#160;function&#160;<i>f&#160;</i>:&#160;X&#160;×&#160;Y&#160;→&#160;R,&#160;in&#160;which&#160;the<br/>
input/output&#160;pairs&#160;are&#160;mapped&#160;to&#160;a&#160;real&#160;number&#160;score.&#160;To&#160;predict&#160;an&#160;output&#160;<i>y&#160;</i>for&#160;an&#160;input<br/>
<i>x</i>,&#160;one&#160;try&#160;to&#160;maximize&#160;<i>f&#160;</i>over&#160;all&#160;<i>y&#160;</i>∈&#160;Y.<br/>
<i>h</i><b>w</b>(<i>x</i>)&#160;=&#160;arg&#160;max&#160;<i>f</i><b>w</b>(<i>x,&#160;y</i>)<br/>
<i>y</i>∈Y<br/>
Let&#160;<i>f</i><b>w&#160;</b>be&#160;a&#160;linear&#160;function&#160;of&#160;the&#160;form:<br/>
<i>f</i><b>w&#160;</b>=&#160;<b>w</b><i>T&#160;</i>Ψ(<i>x,&#160;y</i>)<br/>
,&#160;where&#160;<b>w&#160;</b>is&#160;the&#160;parameter&#160;vector,&#160;and&#160;Ψ(<i>x,&#160;y</i>)&#160;is&#160;the&#160;kernel&#160;function&#160;relating&#160;input&#160;<i>x&#160;</i>to<br/>
output&#160;<i>y</i>.&#160;Ψ&#160;can&#160;be&#160;defined&#160;to&#160;accommodate&#160;various&#160;kind&#160;of&#160;structures.<br/>
For&#160;each&#160;structure&#160;we&#160;want&#160;to&#160;predict,&#160;a&#160;loss&#160;function&#160;that&#160;measures&#160;the&#160;accuracy&#160;of<br/>
of&#160;a&#160;prediction&#160;is&#160;required.&#160;A&#160;loss&#160;function&#160;∆&#160;:&#160;Y&#160;×&#160;Y&#160;→&#160;<i>R&#160;</i>need&#160;to&#160;satisfy&#160;the&#160;following<br/>
property:<br/>
∆(<i>y,&#160;y</i>′)&#160;≥&#160;<i>f&#160;or&#160;y&#160;</i≯=&#160;<i>y</i>′<br/>
∆(<i>y,&#160;y</i>)&#160;=&#160;0<br/>
The&#160;loss&#160;function&#160;is&#160;assumed&#160;to&#160;be&#160;bounded.&#160;Let's&#160;assume&#160;the&#160;input-output&#160;pair&#160;(<i>x,&#160;y</i>)<br/>
is&#160;drawn&#160;from&#160;a&#160;join&#160;distribution&#160;P(x,y),&#160;the&#160;prediction&#160;problem&#160;is&#160;to&#160;minimize&#160;the&#160;total<br/>
loss:<br/>
11<br/>
<hr>
<A name=24></a><IMG src="thesis-24_1.jpg"/><br/>
∫<br/>
<i>R</i>∆&#160;=<br/>
∆(<i>y,&#160;f&#160;</i>(<i>x</i>))<i>dP&#160;</i>(<i>x,&#160;y</i>)<br/>
<i>p</i><br/>
X×Y<br/>
Since&#160;we&#160;can't&#160;directly&#160;find&#160;the&#160;distribution&#160;<i>P&#160;</i>,&#160;we&#160;need&#160;to&#160;replace&#160;this&#160;total&#160;loss&#160;with<br/>
a&#160;empirical&#160;loss,&#160;which&#160;can&#160;be&#160;calculated&#160;from&#160;the&#160;observed&#160;training&#160;set&#160;of&#160;(<i>xi,&#160;yi</i>)&#160;pairs.<br/>
1&#160;<i>n</i><br/>
∑<br/>
<i>R</i>∆(<i>f&#160;</i>)&#160;=<br/>
∆(<i>y</i><br/>
<i>s</i><br/>
<i>i,&#160;f&#160;</i>(<i>xi</i>))<br/>
<i>n&#160;i</i>=1<br/>
Now&#160;we&#160;are&#160;ready&#160;to&#160;extend&#160;SVM&#160;to&#160;structural&#160;output,&#160;starting&#160;with&#160;a&#160;linear&#160;separable<br/>
case,&#160;and&#160;we&#160;will&#160;then&#160;extend&#160;it&#160;to&#160;soft-margin&#160;formulation.<br/>
A&#160;linear&#160;separable&#160;case&#160;can&#160;be&#160;expressed&#160;by&#160;a&#160;set&#160;of&#160;linear&#160;constrains<br/>
∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>∀&#160;ˆ<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<i>yi</i>)]&#160;≤&#160;0<br/>
However,&#160;in&#160;the&#160;SVM&#160;context,&#160;we&#160;want&#160;the&#160;solution&#160;to&#160;have&#160;the&#160;largest&#160;margin&#160;possible.<br/>
So&#160;the&#160;above&#160;linear&#160;constrains&#160;will&#160;become&#160;this&#160;optimization&#160;problem:<br/>
max<br/>
<i>γ</i><br/>
<i>γ,</i><b>w</b>:∥<b>w</b>∥=1<br/>
<i>s.t&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>∀&#160;ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≤&#160;<i>γ</i><br/>
,&#160;which&#160;is&#160;equivalent&#160;to&#160;the&#160;convex&#160;quadratic&#160;programming&#160;problem:<br/>
1<br/>
min<br/>
<b>w</b><i>T&#160;</i><b>w</b><br/>
<b>w</b><i>,ξi</i>≥0&#160;2<br/>
<i>s.t.&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≥&#160;1<br/>
To&#160;extend&#160;the&#160;linear-separable&#160;case&#160;to&#160;non-separable&#160;case,&#160;slack&#160;variables&#160;<i>εi&#160;</i>can&#160;be<br/>
introduced&#160;to&#160;penalize&#160;prediction&#160;errors,&#160;results&#160;in&#160;a&#160;soft-margin&#160;formalization:<br/>
1<br/>
<i>C&#160;n</i><br/>
∑<br/>
min<br/>
<b>w</b><i>T&#160;</i><b>w&#160;</b>+<br/>
<i>ξi</i><br/>
<b>w</b><i>,ξi</i>≥0&#160;2<br/>
<i>n&#160;i</i>=1<br/>
<i>s.t.&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≥&#160;1&#160;−&#160;<i>ξi</i><br/>
<i>C&#160;</i>is&#160;the&#160;weighting&#160;parameter&#160;controlling&#160;the&#160;trade-off&#160;between&#160;low&#160;training&#160;error&#160;and<br/>
12<br/>
<hr>
<A name=25></a><IMG src="thesis-25_1.jpg"/><br/>
large&#160;margin.&#160;The&#160;optimal&#160;<i>C&#160;</i>varies&#160;between&#160;different&#160;problems,&#160;so&#160;experiment&#160;should&#160;be<br/>
conducted&#160;to&#160;find&#160;the&#160;optimal&#160;<i>C&#160;</i>for&#160;our&#160;problem.<br/>
Intuitively,&#160;a&#160;constrain&#160;violation&#160;with&#160;larger&#160;loss&#160;should&#160;be&#160;penalize&#160;more&#160;than&#160;the&#160;one<br/>
with&#160;smaller&#160;loss.&#160;So&#160;I.&#160;Tsochantaridis&#160;et&#160;al.&#160;[57]&#160;proposed&#160;two&#160;possible&#160;way&#160;to&#160;take&#160;the<br/>
loss&#160;function&#160;into&#160;account.&#160;The&#160;first&#160;way&#160;is&#160;to&#160;re-scale&#160;the&#160;slack&#160;variable&#160;by&#160;the&#160;inverse&#160;of<br/>
the&#160;loss,&#160;so&#160;a&#160;high&#160;loss&#160;leads&#160;to&#160;smaller&#160;re-scaled&#160;slack&#160;variable:<br/>
1<br/>
<i>C&#160;n</i><br/>
∑<br/>
min<br/>
<b>w</b><i>T&#160;</i><b>w&#160;</b>+<br/>
<i>ξi</i><br/>
<b>w</b><i>,ξi</i>≥0&#160;2<br/>
<i>n&#160;i</i>=1<br/>
<i>s.t.&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≥&#160;1&#160;−<br/>
<i>ξi</i><br/>
∆(<i>yi,&#160;</i>ˆ<br/>
<i>yi</i>)<br/>
The&#160;second&#160;way&#160;is&#160;to&#160;re-scale&#160;the&#160;margin,&#160;which&#160;yields<br/>
1<br/>
<i>C&#160;n</i><br/>
∑<br/>
min<br/>
<b>w</b><i>T&#160;</i><b>w&#160;</b>+<br/>
<i>ξi</i><br/>
<b>w</b><i>,ξi</i>≥0&#160;2<br/>
<i>n&#160;i</i>=1<br/>
<i>s.t.&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≥&#160;∆(<i>yi,&#160;</i>ˆ<br/>
<i>yi</i>)&#160;−&#160;<i>ξi</i><br/>
But&#160;the&#160;above&#160;quadratic&#160;programming&#160;problem&#160;has&#160;a&#160;very&#160;large&#160;number&#160;(<i>O</i>(<i>n</i>|Y|))<br/>
of&#160;constrains&#160;,&#160;which&#160;will&#160;take&#160;considerable&#160;time&#160;to&#160;solve.&#160;I.&#160;Tsochantaridis&#160;et&#160;al.&#160;[57]<br/>
proposed&#160;a&#160;greedy&#160;algorithm&#160;to&#160;speed&#160;up&#160;the&#160;process&#160;by&#160;selecting&#160;only&#160;part&#160;of&#160;the&#160;constrains<br/>
that&#160;contributes&#160;the&#160;most&#160;to&#160;finding&#160;the&#160;solution.&#160;Initially,&#160;the&#160;solver&#160;starts&#160;with&#160;an&#160;empty<br/>
working&#160;set&#160;containing&#160;no&#160;constrains.&#160;Than&#160;the&#160;solver&#160;iteratively&#160;scans&#160;the&#160;training&#160;set<br/>
to&#160;find&#160;the&#160;most&#160;violated&#160;constrains&#160;under&#160;the&#160;current&#160;solution.&#160;If&#160;a&#160;constrain&#160;is&#160;violated<br/>
more&#160;times&#160;than&#160;a&#160;desired&#160;threshold,&#160;the&#160;constrain&#160;is&#160;added&#160;to&#160;the&#160;working&#160;set&#160;of&#160;constrains.<br/>
Then&#160;the&#160;solver&#160;re-calculate&#160;the&#160;solution&#160;under&#160;the&#160;new&#160;working&#160;set.&#160;The&#160;algorithm&#160;will<br/>
terminate&#160;once&#160;no&#160;more&#160;constrain&#160;can&#160;be&#160;added&#160;under&#160;the&#160;desired&#160;precision.<br/>
In&#160;a&#160;later&#160;work&#160;by&#160;Joachims&#160;et&#160;al.&#160;[56],&#160;they&#160;created&#160;a&#160;new&#160;formulation&#160;and&#160;algorithm<br/>
to&#160;further&#160;speed&#160;up&#160;the&#160;algorithm.&#160;Instead&#160;of&#160;using&#160;one&#160;slack&#160;variables&#160;for&#160;each&#160;training<br/>
sample,&#160;which&#160;results&#160;in&#160;a&#160;total&#160;of&#160;<i>n&#160;</i>slack&#160;variables,&#160;they&#160;use&#160;a&#160;single&#160;slack&#160;variable&#160;for<br/>
all&#160;<i>n&#160;</i>training&#160;samples.&#160;The&#160;following&#160;formula&#160;is&#160;the&#160;1-slack&#160;version&#160;of&#160;slack-rescaling<br/>
13<br/>
<hr>
<A name=26></a><IMG src="thesis-26_1.jpg"/><br/>
structural&#160;SVM:<br/>
1<br/>
min<br/>
<b>w</b><i>T&#160;</i><b>w&#160;</b>+&#160;<i>Cξ</i><br/>
<b>w</b><i>,ξi</i>≥0&#160;2<br/>
<i>n</i><br/>
∑<br/>
<i>s.t.&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≥&#160;1<br/>
1&#160;−<br/>
<i>ξ</i><br/>
<i>n</i><br/>
∆(<i>y</i><br/>
<i>i</i>=1<br/>
<i>i,&#160;</i>ˆ<br/>
<i>yi</i>)<br/>
And&#160;margin-rescaling&#160;structural&#160;SVM:<br/>
1<br/>
min<br/>
<b>w</b><i>T&#160;</i><b>w&#160;</b>+&#160;<i>Cξ</i><br/>
<b>w</b><i>,ξi</i>≥0&#160;2<br/>
<i>n</i><br/>
∑<br/>
<i>s.t.&#160;</i>∀<i>i&#160;</i>∈&#160;{1<i>,&#160;</i>·&#160;·&#160;·&#160;<i>,&#160;n</i>}<i>,&#160;</i>ˆ<br/>
<i>yi&#160;</i>∈&#160;Y&#160;:&#160;<b>w</b><i>T&#160;</i>[Ψ(<i>xi,&#160;yi</i>)&#160;−&#160;Ψ(<i>xi,&#160;</i>ˆ<br/>
<i>yi</i>)]&#160;≥&#160;1<br/>
∆(<i>yi,&#160;</i>ˆ<br/>
<i>yi</i>)&#160;−&#160;<i>ξ</i><br/>
<i>n&#160;i</i>=1<br/>
Detailed&#160;proof&#160;on&#160;how&#160;the&#160;new&#160;formulation&#160;is&#160;equally&#160;general&#160;as&#160;the&#160;old&#160;one&#160;is&#160;given&#160;in<br/>
the&#160;paper&#160;[56].<br/>
With&#160;the&#160;framework&#160;described&#160;above,&#160;the&#160;only&#160;problem&#160;left&#160;is&#160;how&#160;to&#160;define&#160;the&#160;general<br/>
loss&#160;function&#160;for&#160;hidden&#160;Markov&#160;model&#160;(HMM)?&#160;In&#160;[58],&#160;Y.&#160;Altun&#160;et&#160;al.&#160;proposed&#160;two<br/>
types&#160;of&#160;features&#160;for&#160;a&#160;equal-length&#160;observation/label&#160;sequence&#160;pair&#160;(<i>x,&#160;y</i>).&#160;The&#160;first&#160;is&#160;the<br/>
interaction&#160;of&#160;a&#160;observation&#160;with&#160;a&#160;label,&#160;the&#160;other&#160;is&#160;the&#160;interaction&#160;between&#160;neighboring<br/>
labels.<br/>
To&#160;illustrate&#160;the&#160;method,&#160;we&#160;use&#160;a&#160;example&#160;from&#160;music:&#160;for&#160;some&#160;observed&#160;features<br/>
Φ<i>r</i>(<i>xs</i>)&#160;of&#160;a&#160;note&#160;<i>x&#160;</i>located&#160;in&#160;<i>s</i>th&#160;position&#160;of&#160;the&#160;phrase,&#160;and&#160;assume&#160;[[<i>yt&#160;</i>=&#160;<i>τ&#160;</i>]]&#160;denotes&#160;the<br/>
<i>t</i>th&#160;note&#160;is&#160;played&#160;at&#160;a&#160;velocity&#160;of&#160;<i>τ&#160;</i>,&#160;the&#160;interaction&#160;of&#160;the&#160;two&#160;predicate&#160;can&#160;be&#160;written&#160;as<br/>
[[<br/>
]]<br/>
<i>ϕst&#160;</i>(<b>x</b><i>,&#160;</i><b>y</b>)&#160;=<br/>
<i>yt&#160;</i>=&#160;<i>τ</i><br/>
Ψ<br/>
<i>rσ</i><br/>
<i>r&#160;</i>(<i>xs</i>)<i>,&#160;</i>1&#160;≤&#160;<i>γ&#160;</i>≤&#160;<i>d,&#160;τ&#160;</i>∈&#160;Σ<br/>
And&#160;for&#160;interaction&#160;between&#160;labels,&#160;the&#160;feature&#160;can&#160;be&#160;written&#160;as<br/>
[[<br/>
]]<br/>
ˆ<br/>
<i>ϕst&#160;</i>(<b>x</b><i>,&#160;</i><b>y</b>)&#160;=<br/>
<i>ys&#160;</i>=&#160;<i>σ&#160;</i>∧&#160;<i>yt&#160;</i>=&#160;<i>τ</i><br/>
<i>,&#160;σ,&#160;τ&#160;</i>∈&#160;Σ<br/>
<i>rσ</i><br/>
By&#160;selecting&#160;a&#160;order&#160;of&#160;dependency&#160;for&#160;the&#160;HMM&#160;model,&#160;we&#160;can&#160;further&#160;restrict&#160;<i>s</i>'s<br/>
and&#160;<i>t</i>'s.&#160;For&#160;example,&#160;for&#160;a&#160;first-order&#160;HMM,&#160;<i>s&#160;</i>=&#160;<i>t&#160;</i>for&#160;the&#160;first&#160;feature,&#160;and&#160;<i>s&#160;</i>=&#160;<i>t&#160;</i>−&#160;1<br/>
for&#160;the&#160;second&#160;feature.&#160;The&#160;two&#160;features&#160;on&#160;the&#160;same&#160;time&#160;<i>t&#160;</i>is&#160;then&#160;stacked&#160;into&#160;a&#160;vector<br/>
14<br/>
<hr>
<A name=27></a><IMG src="thesis-27_1.jpg"/><br/>
Figure&#160;3.2:&#160;Learning&#160;phase&#160;flow&#160;chart<br/>
Ψ(<i>x,&#160;y</i>;&#160;<i>t</i>).&#160;The&#160;feature&#160;map&#160;for&#160;the&#160;whole&#160;sequence&#160;is&#160;simply&#160;the&#160;sum&#160;of&#160;all&#160;the&#160;feature<br/>
vectors<br/>
<i>T</i><br/>
∑<br/>
Φ(<b>x</b><i>,&#160;</i><b>y</b>)&#160;=<br/>
Φ(<b>x</b><i>,&#160;</i><b>y</b>;&#160;<i>t</i>)<br/>
<i>t</i>=1<br/>
Finally,&#160;the&#160;distance&#160;between&#160;two&#160;feature&#160;maps&#160;depends&#160;on&#160;the&#160;number&#160;of&#160;common<br/>
label&#160;segments&#160;and&#160;the&#160;inner&#160;product&#160;between&#160;the&#160;input&#160;features&#160;sequence&#160;with&#160;common<br/>
labels.<br/>
∑&#160;[[<br/>
]]<br/>
∑&#160;[[<br/>
]]<br/>
⟨Φ(<b>x</b><i>,&#160;</i><b>y</b>)<i>,&#160;</i>Φ(<b>ˆx</b><i>,&#160;</i><b>ˆy</b>)⟩&#160;=<br/>
<i>ys</i>−1&#160;=&#160;ˆ<br/>
<i>yt</i>−1&#160;∧&#160;<i>ys&#160;</i>=&#160;ˆ<br/>
<i>yt</i><br/>
+<br/>
<i>ys&#160;</i>=&#160;ˆ<br/>
<i>yt</i><br/>
<i>k</i>(<i>xs,&#160;</i>ˆ<br/>
<i>xt</i>)<br/>
<i>s,t</i><br/>
<i>s,t</i><br/>
A&#160;Viterbi-like&#160;decoding&#160;algorithm&#160;is&#160;used&#160;to&#160;speed&#160;up&#160;the&#160;computation&#160;of&#160;<i>F&#160;</i>for&#160;HMM..<br/>
<b>3.3</b><br/>
<b>Learning&#160;Performance&#160;Knowledge</b><br/>
In&#160;this&#160;section,&#160;we&#160;will&#160;introduce&#160;the&#160;componants&#160;that&#160;consist&#160;the&#160;learning&#160;phase.&#160;The<br/>
main&#160;goal&#160;in&#160;the&#160;learning&#160;phase&#160;is&#160;to&#160;extract&#160;performance&#160;knowledge&#160;from&#160;training&#160;sam-<br/>
ples.&#160;Fig.&#160;<a href="thesiss.html#27">3.2&#160;</a>shows&#160;the&#160;internal&#160;structure&#160;of&#160;the&#160;learning&#160;phase.<br/>
Training&#160;samples&#160;are&#160;matched&#160;score&#160;and&#160;expressive&#160;performance&#160;pairs&#160;(their&#160;format<br/>
and&#160;preparation&#160;process&#160;is&#160;discussed&#160;in&#160;Chapter&#160;<a href="thesiss.html#38">4).&#160;</a>The&#160;raw&#160;data&#160;from&#160;the&#160;samples&#160;are<br/>
too&#160;complex&#160;to&#160;process,&#160;so&#160;we&#160;need&#160;to&#160;extract&#160;important&#160;features&#160;from&#160;them.&#160;Two&#160;types<br/>
of&#160;features&#160;will&#160;be&#160;extracted&#160;from&#160;the&#160;samples:&#160;first,&#160;the&#160;musicological&#160;cues&#160;from&#160;the<br/>
scores&#160;are&#160;called&#160;score&#160;features;&#160;second,&#160;the&#160;measurable&#160;expression&#160;from&#160;the&#160;expressive<br/>
performances&#160;are&#160;called&#160;the&#160;performance&#160;features.&#160;We&#160;want&#160;the&#160;system&#160;to&#160;learn&#160;how&#160;score<br/>
15<br/>
<hr>
<A name=28></a><IMG src="thesis-28_1.jpg"/><br/>
features&#160;are&#160;“translated”&#160;into&#160;performeance&#160;features.&#160;This&#160;process&#160;can&#160;be&#160;analogize&#160;to<br/>
a&#160;human&#160;performer&#160;reading&#160;the&#160;explicit&#160;and&#160;implicit&#160;cues&#160;from&#160;the&#160;score,&#160;and&#160;perform<br/>
the&#160;music&#160;with&#160;certain&#160;expressive&#160;expression.&#160;The&#160;definition&#160;of&#160;the&#160;features&#160;used&#160;will&#160;be<br/>
presented&#160;in&#160;Section&#160;<a href="thesiss.html#33">3.5.</a><br/>
<b>3.3.1</b><br/>
<b>Training&#160;Sample&#160;Loader</b><br/>
The&#160;training&#160;samples&#160;are&#160;loaded&#160;by&#160;the&#160;sample&#160;loader&#160;module.&#160;Since&#160;a&#160;training&#160;sam-<br/>
ple&#160;consists&#160;of&#160;a&#160;score&#160;(musicXML&#160;format)&#160;and&#160;an&#160;expressive&#160;recording&#160;(MIDI&#160;format),<br/>
the&#160;sample&#160;loader&#160;finds&#160;the&#160;two&#160;files,&#160;and&#160;load&#160;them&#160;into&#160;an&#160;intermediate&#160;representation<br/>
(music21.Stream&#160;object&#160;provided&#160;by&#160;the&#160;music21&#160;library&#160;[59]&#160;from&#160;MIT).&#160;The&#160;mu-<br/>
sic21&#160;library&#160;will&#160;convert&#160;the&#160;musicXML&#160;and&#160;MIDI&#160;format&#160;into&#160;a&#160;Python&#160;Object&#160;hierarchy<br/>
that&#160;is&#160;easy&#160;to&#160;access&#160;and&#160;manipulate&#160;by&#160;Python&#160;code.<br/>
One&#160;caveat&#160;here&#160;is&#160;the&#160;music21&#160;library&#160;will&#160;quantize&#160;the&#160;time&#160;in&#160;MIDI,&#160;which&#160;will<br/>
destroy&#160;the&#160;subtle&#160;onset&#160;and&#160;duration&#160;expressions.&#160;And&#160;the&#160;music21&#160;library&#160;don't&#160;handle<br/>
the&#160;“ticks&#160;per&#160;quarter&#160;note”&#160;information&#160;in&#160;the&#160;MIDI&#160;header&#160;[60],&#160;which&#160;is&#160;essential&#160;for&#160;the<br/>
MIDI&#160;parser&#160;to&#160;interprete&#160;the&#160;correct&#160;time&#160;scale.&#160;So&#160;we&#160;must&#160;explicitly&#160;disable&#160;quantization<br/>
and&#160;specify&#160;the&#160;“ticks&#160;per&#160;quarter&#160;note”&#160;value&#160;during&#160;MIDI&#160;loading.<br/>
<b>3.3.2</b><br/>
<b>Features&#160;Extraction</b><br/>
In&#160;order&#160;to&#160;keep&#160;the&#160;system&#160;architecture&#160;simple,&#160;feature&#160;extractors&#160;are&#160;designed&#160;to&#160;be<br/>
independent&#160;to&#160;other&#160;feature&#160;extractors,&#160;so&#160;features&#160;can&#160;included&#160;or&#160;removed&#160;without&#160;af-<br/>
fecting&#160;the&#160;rest&#160;of&#160;the&#160;system.&#160;Furthermore,&#160;this&#160;enables&#160;parallel&#160;feature&#160;extraction.&#160;But<br/>
sometimes&#160;a&#160;feature&#160;inevitably&#160;depends&#160;on&#160;other&#160;features,&#160;for&#160;example,&#160;the&#160;“relative&#160;du-<br/>
ration&#160;with&#160;the&#160;previous&#160;note”&#160;is&#160;calculated&#160;based&#160;on&#160;the&#160;“duration”&#160;feature.&#160;Since&#160;we<br/>
want&#160;to&#160;avoid&#160;complex&#160;dependency&#160;management,&#160;the&#160;“relative&#160;duration&#160;with&#160;the&#160;previ-<br/>
ous&#160;note”&#160;feature&#160;extractor&#160;has&#160;to&#160;invoke&#160;the&#160;“duration”&#160;extractor,&#160;instead&#160;of&#160;waiting&#160;for<br/>
the&#160;“duration”&#160;extractor&#160;to&#160;finish&#160;first.&#160;Therefore,&#160;the&#160;“duration”&#160;feature&#160;extracted&#160;will<br/>
be&#160;computed&#160;twice.&#160;To&#160;avoid&#160;redundant&#160;computation&#160;of&#160;the&#160;feature&#160;extractors,&#160;we&#160;imple-<br/>
mented&#160;a&#160;caching&#160;mechanism.&#160;Once&#160;the&#160;“duration”&#160;feature&#160;had&#160;been&#160;computed,&#160;no&#160;matter<br/>
16<br/>
<hr>
<A name=29></a><IMG src="thesis-29_1.jpg"/><br/>
it&#160;is&#160;calculated&#160;during&#160;“duration”&#160;extraction&#160;or&#160;or&#160;during&#160;“relative&#160;dutaion&#160;with&#160;the&#160;previ-<br/>
ous&#160;note”&#160;extraction&#160;process,&#160;it's&#160;value&#160;will&#160;be&#160;cached&#160;during&#160;this&#160;execution&#160;session.&#160;So<br/>
no&#160;matter&#160;how&#160;many&#160;feature&#160;extractors&#160;uses&#160;the&#160;“duration”&#160;feature,&#160;they&#160;can&#160;get&#160;the&#160;value<br/>
directly&#160;from&#160;cache.&#160;This&#160;method&#160;can&#160;speed&#160;up&#160;the&#160;execution&#160;without&#160;needing&#160;to&#160;handling<br/>
dependencies.<br/>
The&#160;extracted&#160;features&#160;are&#160;aggregated&#160;and&#160;stored&#160;into&#160;a&#160;JavaScript&#160;Object&#160;Notation<br/>
(JSON)&#160;file&#160;for&#160;the&#160;SVM-HMM&#160;module&#160;to&#160;load.&#160;By&#160;saving&#160;the&#160;features&#160;in&#160;a&#160;human-<br/>
readable&#160;intermediate&#160;file,&#160;we&#160;can&#160;debug&#160;potential&#160;problems&#160;easily.<br/>
<b>3.3.3</b><br/>
<b>SVM-HMM&#160;Learning</b><br/>
After&#160;all&#160;features&#160;are&#160;extracted,&#160;the&#160;next&#160;step&#160;is&#160;to&#160;learn&#160;performance&#160;knowledge&#160;from<br/>
the&#160;features.&#160;In&#160;the&#160;early&#160;stage&#160;of&#160;this&#160;research,&#160;we&#160;have&#160;successfully&#160;applied&#160;linear&#160;regres-<br/>
sio&#160;[61].&#160;However,&#160;assuming&#160;this&#160;problem&#160;to&#160;be&#160;linear&#160;is&#160;clearly&#160;an&#160;oversimplification,&#160;so<br/>
we&#160;switch&#160;to&#160;structural&#160;support&#160;vector&#160;machine&#160;with&#160;hidden&#160;Markov&#160;model&#160;output&#160;(SVM-<br/>
HMM)&#160;[56--58]&#160;as&#160;our&#160;supervised&#160;learning&#160;algorithm.<br/>
The&#160;SVM-HMM&#160;learning&#160;module&#160;loads&#160;the&#160;feature&#160;file&#160;from&#160;the&#160;previous&#160;stage,&#160;and<br/>
aggregate&#160;the&#160;features&#160;to&#160;fit&#160;the&#160;required&#160;input&#160;format&#160;of&#160;the&#160;SVM-HMM&#160;learner&#160;program.<br/>
However,&#160;most&#160;features&#160;from&#160;the&#160;previous&#160;stage&#160;are&#160;real&#160;values,&#160;but&#160;SVM-HMM&#160;only&#160;takes<br/>
discrete&#160;performance&#160;features<a href="">1</a>,&#160;so&#160;quantization&#160;is&#160;required.&#160;There&#160;are&#160;many&#160;possible&#160;way<br/>
to&#160;quantize&#160;the&#160;features,&#160;each&#160;will&#160;result&#160;in&#160;different&#160;output,&#160;here&#160;we&#160;will&#160;present&#160;a&#160;quantizer<br/>
design&#160;for&#160;demonstration&#160;purpose:&#160;for&#160;each&#160;performance&#160;feature,&#160;the&#160;mean&#160;and&#160;standard<br/>
deviation&#160;from&#160;all&#160;training&#160;samples&#160;are&#160;calculated&#160;first.&#160;The&#160;range&#160;between&#160;mean&#160;minus&#160;or<br/>
plus&#160;four&#160;standard&#160;deviations&#160;is&#160;divided&#160;into&#160;128&#160;uniform&#160;intervals.&#160;Values&#160;over&#160;than&#160;mean<br/>
plus&#160;four&#160;standard&#160;deviations&#160;are&#160;quantized&#160;into&#160;the&#160;128th&#160;bin,&#160;and&#160;values&#160;below&#160;mean<br/>
minus&#160;four&#160;standard&#160;deviations&#160;are&#160;quantized&#160;into&#160;the&#160;1st&#160;bin.&#160;The&#160;number&#160;of&#160;intervals<br/>
decides&#160;how&#160;fine-grain&#160;the&#160;quantization&#160;is,&#160;if&#160;the&#160;number&#160;is&#160;too&#160;low,&#160;subtle&#160;expressions&#160;will<br/>
be&#160;lost&#160;due&#160;to&#160;high&#160;quantization&#160;error.&#160;However,&#160;if&#160;the&#160;number&#160;is&#160;too&#160;large,&#160;there&#160;will&#160;be&#160;too<br/>
few&#160;samples&#160;for&#160;each&#160;interval,&#160;which&#160;is&#160;bad&#160;from&#160;a&#160;statistical&#160;learning&#160;perspective.&#160;Also<br/>
1SVM-HMM&#160;is&#160;initially&#160;designed&#160;for&#160;tasks&#160;like&#160;part-of-speech&#160;tagging,&#160;in&#160;which&#160;real&#160;value&#160;or&#160;binary<br/>
features&#160;are&#160;used&#160;to&#160;predict&#160;discrete&#160;part-of-speech&#160;tags.<br/>
17<br/>
<hr>
<A name=30></a><IMG src="thesis-30_1.jpg"/><br/>
the&#160;training&#160;process&#160;will&#160;take&#160;a&#160;lot&#160;of&#160;CPU&#160;and&#160;memory&#160;resources&#160;without&#160;significant&#160;gain<br/>
in&#160;prediction&#160;accuracy.&#160;The&#160;range&#160;of&#160;four&#160;standard&#160;deviation&#160;is&#160;chosen&#160;by&#160;trail&#160;and&#160;error,<br/>
a&#160;narrower&#160;range&#160;will&#160;make&#160;most&#160;of&#160;the&#160;extreme&#160;values&#160;be&#160;quantized&#160;into&#160;the&#160;largest&#160;of<br/>
smallest&#160;bin,&#160;so&#160;the&#160;performance&#160;will&#160;have&#160;a&#160;lot&#160;of&#160;saturated&#160;values.&#160;But&#160;a&#160;very&#160;large&#160;range<br/>
will&#160;make&#160;the&#160;interval&#160;between&#160;each&#160;quantization&#160;bin&#160;too&#160;large,&#160;rising&#160;the&#160;quantization<br/>
error.<br/>
The&#160;theoretical&#160;background&#160;of&#160;SVM-HMM&#160;is&#160;already&#160;mentioned&#160;in&#160;Section&#160;<a href="thesiss.html#22">3.2</a>.&#160;We<br/>
leverage&#160;Thorsten&#160;Joachims's&#160;implementation&#160;called&#160;<i>SV&#160;M&#160;hmm&#160;</i>[62].&#160;<i>SV&#160;M&#160;hmm&#160;</i>is&#160;an&#160;im-<br/>
plementation&#160;of&#160;structural&#160;SVMs&#160;for&#160;sequence&#160;tagging&#160;[58]&#160;using&#160;the&#160;training&#160;algorithm<br/>
described&#160;in&#160;[57]&#160;and&#160;[56].&#160;The&#160;<i>SV&#160;M&#160;hmm&#160;</i>package&#160;contains&#160;a&#160;SVM-HMM&#160;training&#160;pro-<br/>
gram&#160;called&#160;svm_hmm_learn&#160;and&#160;a&#160;prediction&#160;program&#160;called&#160;svm_hmm_classify.<br/>
For&#160;architectural&#160;simplicity,&#160;we&#160;train&#160;one&#160;model&#160;for&#160;each&#160;performance&#160;feature,&#160;each&#160;model<br/>
uses&#160;all&#160;the&#160;score&#160;features&#160;to&#160;predict&#160;a&#160;single&#160;performance&#160;feature.&#160;The&#160;svm_hmm_learn<br/>
read&#160;the&#160;features&#160;from&#160;a&#160;file&#160;in&#160;the&#160;following&#160;format:&#160;Each&#160;line&#160;represents&#160;features&#160;for&#160;a<br/>
note&#160;in&#160;time&#160;order,&#160;format&#160;as<br/>
PERF&#160;qid:EXNUM&#160;FEAT1:FEAT1_VAL&#160;FEAT2:FEAT2_VAL&#160;...&#160;#comment<br/>
PERF&#160;is&#160;a&#160;quantized&#160;performance&#160;feature.&#160;The&#160;EXNUM&#160;after&#160;qid:&#160;identifies&#160;the&#160;phrases,<br/>
all&#160;notes&#160;in&#160;a&#160;phrase&#160;will&#160;have&#160;the&#160;same&#160;qid:EXNUM&#160;identifier.&#160;Following&#160;the&#160;identi-<br/>
fier&#160;are&#160;quantized&#160;score&#160;features,&#160;denote&#160;as&#160;feature&#160;name&#160;:<br/>
feature&#160;value,<br/>
separated&#160;by&#160;spaces.&#160;And&#160;text&#160;following&#160;a&#160;#&#160;symbol&#160;is&#160;comment.<br/>
There&#160;are&#160;some&#160;key&#160;parameters&#160;needed&#160;to&#160;be&#160;adjusted&#160;for&#160;the&#160;training&#160;program.&#160;First<br/>
the&#160;<i>C&#160;</i>parameter&#160;in&#160;SVM,&#160;which&#160;controls&#160;the&#160;trade-off&#160;between&#160;lowering&#160;training&#160;error<br/>
and&#160;maximizing&#160;margin.&#160;Larger&#160;C&#160;will&#160;result&#160;in&#160;lower&#160;training&#160;error,&#160;but&#160;the&#160;margin&#160;may<br/>
be&#160;smaller.&#160;Second,&#160;the&#160;<i>ε&#160;</i>parameter&#160;controls&#160;the&#160;required&#160;precision&#160;for&#160;termination.&#160;The<br/>
smaller&#160;the&#160;<i>ε</i>,&#160;the&#160;higher&#160;precision,&#160;but&#160;it&#160;may&#160;require&#160;more&#160;time&#160;and&#160;computing&#160;resource.<br/>
Finally,&#160;for&#160;the&#160;HMM&#160;part&#160;of&#160;the&#160;model,&#160;the&#160;order&#160;of&#160;dependencies&#160;of&#160;transition&#160;states<br/>
and&#160;emission&#160;states&#160;needs&#160;to&#160;be&#160;specified.&#160;In&#160;our&#160;case,&#160;both&#160;are&#160;set&#160;to&#160;defaults:&#160;transition<br/>
dependency&#160;is&#160;set&#160;to&#160;one,&#160;which&#160;stands&#160;for&#160;first-order&#160;Markov&#160;property,&#160;and&#160;emission&#160;de-<br/>
pendency&#160;is&#160;set&#160;to&#160;zero.&#160;Since&#160;we&#160;train&#160;one&#160;models&#160;for&#160;each&#160;performance&#160;feature,&#160;each<br/>
18<br/>
<hr>
<A name=31></a><IMG src="thesis-31_1.jpg"/><br/>
Figure&#160;3.3:&#160;Performing&#160;phase&#160;flow&#160;chart<br/>
model&#160;can&#160;have&#160;their&#160;own&#160;set&#160;of&#160;parameters.&#160;The&#160;parameter&#160;selection&#160;experiments&#160;will&#160;be<br/>
presented&#160;in&#160;Chapter&#160;<a href="thesiss.html#48">5</a>.<br/>
Finally,&#160;the&#160;training&#160;program&#160;will&#160;output&#160;three&#160;model&#160;files&#160;(because&#160;we&#160;use&#160;three&#160;per-<br/>
formance&#160;features)&#160;which&#160;contains&#160;SVM-HMM&#160;model&#160;parameters,&#160;such&#160;as&#160;the&#160;support<br/>
vectors&#160;and&#160;other&#160;metadata.&#160;Since&#160;it&#160;takes&#160;considerable&#160;time&#160;(roughly&#160;a&#160;dozen&#160;minutes&#160;to<br/>
a&#160;few&#160;hours)&#160;to&#160;train&#160;a&#160;model,&#160;depending&#160;on&#160;the&#160;amount&#160;of&#160;training&#160;samples&#160;and&#160;the&#160;power<br/>
of&#160;the&#160;computer,&#160;the&#160;system&#160;can&#160;only&#160;support&#160;off-line&#160;learning.&#160;But&#160;the&#160;learning&#160;process<br/>
only&#160;need&#160;to&#160;be&#160;run&#160;once.&#160;The&#160;performance&#160;knowledge&#160;model&#160;can&#160;be&#160;reused&#160;over&#160;and&#160;over<br/>
again&#160;in&#160;the&#160;performing&#160;phase.<br/>
<b>3.4</b><br/>
<b>Performing&#160;Expressively</b><br/>
The&#160;performing&#160;phase&#160;uses&#160;the&#160;performance&#160;knowledge&#160;model&#160;learned&#160;in&#160;the&#160;previ-<br/>
ous&#160;phase&#160;to&#160;generate&#160;expressive&#160;performances.&#160;The&#160;input&#160;is&#160;a&#160;score&#160;file&#160;to&#160;be&#160;performed,<br/>
which&#160;should&#160;not&#160;be&#160;used&#160;as&#160;training&#160;sample&#160;to&#160;prevent&#160;overfitting.&#160;Score&#160;features&#160;will&#160;be<br/>
extracted&#160;from&#160;it&#160;using&#160;the&#160;same&#160;routine&#160;as&#160;in&#160;the&#160;learning&#160;phase.&#160;The&#160;SVM-HMM&#160;gener-<br/>
ation&#160;module&#160;will&#160;use&#160;the&#160;learned&#160;model&#160;and&#160;the&#160;score&#160;features&#160;to&#160;predict&#160;the&#160;performance<br/>
features.&#160;These&#160;features&#160;will&#160;than&#160;be&#160;de-quantized&#160;back&#160;to&#160;real&#160;values&#160;using&#160;the&#160;method<br/>
described&#160;previously.&#160;An&#160;MIDI&#160;generation&#160;module&#160;will&#160;apply&#160;those&#160;performance&#160;features<br/>
onto&#160;the&#160;score&#160;to&#160;produce&#160;a&#160;expressive&#160;MIDI&#160;file.&#160;The&#160;MIDI&#160;file&#160;itself&#160;is&#160;already&#160;a&#160;expres-<br/>
sive&#160;performance,&#160;in&#160;order&#160;to&#160;listen&#160;to&#160;the&#160;sound,&#160;an&#160;software&#160;synthesizer&#160;can&#160;be&#160;used&#160;to<br/>
19<br/>
<hr>
<A name=32></a><IMG src="thesis-32_1.jpg"/><br/>
render&#160;the&#160;MIDI&#160;file&#160;into&#160;WAV&#160;or&#160;MP3&#160;format.<br/>
<b>3.4.1</b><br/>
<b>SVM-HMM&#160;Generation</b><br/>
The&#160;feature&#160;extraction&#160;and&#160;aggregation&#160;process&#160;in&#160;the&#160;performing&#160;phase&#160;is&#160;similar&#160;to<br/>
the&#160;learning&#160;phase,&#160;but&#160;the&#160;PERF&#160;fields&#160;in&#160;the&#160;SVM-HMM&#160;input&#160;file&#160;are&#160;left&#160;blank&#160;for&#160;the<br/>
algorithm&#160;to&#160;predict.&#160;The&#160;svm_hmm_classify&#160;program&#160;will&#160;take&#160;these&#160;inputs&#160;with&#160;the<br/>
learned&#160;model&#160;file&#160;and&#160;predict&#160;the&#160;quantized&#160;labels&#160;of&#160;the&#160;performance&#160;features.&#160;These<br/>
performance&#160;features&#160;are&#160;de-quantized&#160;back&#160;to&#160;the&#160;middle&#160;point&#160;of&#160;each&#160;bin.<br/>
<b>3.4.2</b><br/>
<b>MIDI&#160;Generation&#160;and&#160;Synthesis</b><br/>
The&#160;predicted&#160;performance&#160;features&#160;are&#160;then&#160;applied&#160;onto&#160;the&#160;input&#160;score,&#160;i.e.&#160;the<br/>
onset&#160;timings&#160;will&#160;be&#160;shifted,&#160;the&#160;duration&#160;extended&#160;or&#160;shortened,&#160;and&#160;the&#160;loudness&#160;shifted<br/>
according&#160;to&#160;the&#160;predicted&#160;performance&#160;features.&#160;The&#160;resulting&#160;expressive&#160;performance<br/>
will&#160;be&#160;transfromed&#160;into&#160;MIDI&#160;files&#160;using&#160;music21&#160;library&#160;[59].<br/>
In&#160;order&#160;to&#160;actually&#160;hear&#160;the&#160;expressive&#160;performance,&#160;the&#160;MIDI&#160;file&#160;can&#160;be&#160;rendered&#160;by<br/>
a&#160;software&#160;MIDI&#160;synthesizer.&#160;For&#160;example,&#160;timidity++&#160;software&#160;synthesizer&#160;for&#160;Linux<br/>
can&#160;render&#160;the&#160;MIDI&#160;into&#160;a&#160;WAV&#160;(Waveform&#160;Audio&#160;Format)&#160;file,&#160;which&#160;can&#160;be&#160;compressed<br/>
into&#160;MP3&#160;(MPEG-2&#160;Audio&#160;Layer&#160;III)&#160;by&#160;lame&#160;audio&#160;encoder.&#160;Alternatively,&#160;one&#160;can&#160;use<br/>
hardware&#160;synthesizers,&#160;for&#160;example,&#160;RenCon&#160;[1]&#160;contest&#160;uses&#160;Yamaha&#160;Disklavier&#160;digital<br/>
piano&#160;to&#160;render&#160;contestants'&#160;submission.<br/>
Because&#160;sub-note&#160;level&#160;expression&#160;is&#160;not&#160;the&#160;primary&#160;goal&#160;of&#160;this&#160;research,&#160;we&#160;choose<br/>
standard&#160;MIDI&#160;grand&#160;piano&#160;sound&#160;to&#160;render&#160;the&#160;music.&#160;The&#160;system&#160;can&#160;be&#160;extended&#160;to&#160;used<br/>
more&#160;advanced&#160;physical&#160;model&#160;or&#160;instrument-specific&#160;audio&#160;synthesizer.&#160;Some&#160;sub-note<br/>
level&#160;features,&#160;such&#160;as&#160;special&#160;techniques&#160;for&#160;violins,&#160;can&#160;be&#160;added&#160;to&#160;the&#160;features&#160;list&#160;and<br/>
be&#160;learned&#160;by&#160;the&#160;SVM-HMM&#160;model.<br/>
20<br/>
<hr>
<A name=33></a><IMG src="thesis-33_1.jpg"/><br/>
<b>3.5</b><br/>
<b>Features</b><br/>
As&#160;mentioned&#160;in&#160;Section&#160;<a href="thesiss.html#27">3.3,&#160;</a>there&#160;are&#160;two&#160;types&#160;of&#160;features,&#160;score&#160;features&#160;and&#160;perfor-<br/>
mance&#160;features.&#160;We&#160;will&#160;present&#160;the&#160;features&#160;used&#160;in&#160;the&#160;system,&#160;and&#160;discuss&#160;the&#160;difficulties<br/>
encountered.<br/>
<b>3.5.1</b><br/>
<b>Score&#160;Features</b><br/>
Score&#160;features&#160;are&#160;musicological&#160;cues&#160;presented&#160;in&#160;the&#160;score.&#160;The&#160;purpose&#160;of&#160;score<br/>
features&#160;are&#160;to&#160;simulate&#160;the&#160;high&#160;level&#160;information&#160;a&#160;performer&#160;may&#160;perceive&#160;when&#160;he/she<br/>
reads&#160;the&#160;score.&#160;The&#160;basic&#160;time&#160;unit&#160;for&#160;these&#160;features&#160;are&#160;notes.&#160;Each&#160;note&#160;will&#160;have&#160;all<br/>
features&#160;presented&#160;below.&#160;Score&#160;features&#160;includes:<br/>
<b>Relative&#160;position&#160;in&#160;the&#160;phrase:&#160;</b>The&#160;relative&#160;position&#160;of&#160;a&#160;note&#160;in&#160;the&#160;phrase,&#160;its&#160;value<br/>
ranges&#160;from&#160;0%&#160;to&#160;100%.&#160;This&#160;feature&#160;is&#160;intended&#160;to&#160;capture&#160;the&#160;special&#160;expression<br/>
in&#160;the&#160;start&#160;or&#160;the&#160;end&#160;of&#160;a&#160;phrase,&#160;or&#160;time-variant&#160;expression&#160;like&#160;arch-type&#160;loudness<br/>
variation.<br/>
<b>Pitch:&#160;</b>The&#160;pitch&#160;of&#160;a&#160;note&#160;denoted&#160;by&#160;MIDI&#160;pitch&#160;number&#160;(resolution&#160;is&#160;down&#160;to&#160;semi-<br/>
tone).<br/>
<b>Interval&#160;from&#160;the&#160;previous&#160;note:&#160;</b>The&#160;interval&#160;between&#160;the&#160;current&#160;note&#160;and&#160;its&#160;previous<br/>
note&#160;(in&#160;semitone).&#160;This&#160;represents&#160;the&#160;direction&#160;of&#160;the&#160;melodic&#160;line.See&#160;Fig.&#160;<a href="thesiss.html#34">3.4&#160;</a>for<br/>
example.<br/>
∆<i>P&#160;</i>−&#160;=&#160;<i>Pi&#160;</i>−&#160;<i>Pi</i>−1<br/>
<b>Interval&#160;to&#160;the&#160;next&#160;note:&#160;</b>The&#160;interval&#160;between&#160;the&#160;current&#160;note&#160;and&#160;its&#160;previous&#160;note&#160;(in<br/>
semitone).&#160;See&#160;Fig.&#160;<a href="thesiss.html#34">3.4&#160;</a>for&#160;example.<br/>
∆<i>P&#160;</i>+&#160;=&#160;<i>Pi</i>+1&#160;−&#160;<i>Pi</i><br/>
<b>Note&#160;duration:&#160;</b>The&#160;duration&#160;of&#160;a&#160;note&#160;(quarter&#160;notes).<br/>
21<br/>
<hr>
<A name=34></a><IMG src="thesis-34_1.jpg"/><br/>
<IMG src="thesis-34_2.png"/><br/>
<IMG src="thesis-34_3.png"/><br/>
Figure&#160;3.4:&#160;Interval&#160;from/to&#160;neighbor&#160;notes<br/>
Figure&#160;3.5:&#160;Relative&#160;duration&#160;with&#160;the&#160;previous/next&#160;note<br/>
Grace&#160;notes&#160;have&#160;no&#160;duration&#160;in&#160;musicXML&#160;specification&#160;[63].&#160;The&#160;reason&#160;for&#160;this&#160;is<br/>
that&#160;grace&#160;notes&#160;are&#160;considered&#160;very&#160;short&#160;ornaments&#160;that&#160;does&#160;not&#160;occupy&#160;real&#160;beat<br/>
position.&#160;But&#160;zero&#160;duration&#160;is&#160;hard&#160;to&#160;handle&#160;in&#160;math&#160;formulation.&#160;So&#160;we&#160;assigned<br/>
a&#160;duration&#160;of&#160;a&#160;sixty-fourth&#160;note,&#160;because&#160;it's&#160;far&#160;shorter&#160;than&#160;all&#160;the&#160;notes&#160;in&#160;our<br/>
corpus.<br/>
<b>Relative&#160;Duration&#160;with&#160;the&#160;previous&#160;note:&#160;</b>The&#160;duration&#160;of&#160;a&#160;note&#160;divided&#160;by&#160;the&#160;dura-<br/>
tion&#160;of&#160;its&#160;previous&#160;note.&#160;See&#160;Fig.&#160;<a href="thesiss.html#34">3.5&#160;</a>for&#160;example.&#160;For&#160;a&#160;phrase&#160;of&#160;<i>n&#160;</i>notes&#160;with<br/>
duration&#160;<i>D</i>1<i>,&#160;D</i>2<i>,&#160;.&#160;.&#160;.&#160;,&#160;Dn</i>,<br/>
<i>D</i><br/>
<i>RD</i>−&#160;=<br/>
<i>i</i><br/>
<i>Di</i>−1<br/>
This&#160;feature&#160;is&#160;intended&#160;to&#160;locate&#160;local&#160;changes&#160;in&#160;tempo,&#160;such&#160;as&#160;a&#160;series&#160;of&#160;rapid<br/>
consecutive&#160;notes&#160;followed&#160;by&#160;a&#160;long&#160;note,&#160;which&#160;will&#160;cause&#160;a&#160;discontinuity&#160;in&#160;this<br/>
feature.<br/>
<b>Relative&#160;duration&#160;with&#160;the&#160;next&#160;note:&#160;</b>The&#160;duration&#160;of&#160;a&#160;note&#160;divided&#160;by&#160;duration&#160;of&#160;its<br/>
next&#160;note.&#160;See&#160;Fig.&#160;<a href="thesiss.html#34">3.5&#160;</a>for&#160;example.<br/>
<i>D</i><br/>
<i>RD</i>+&#160;=<br/>
<i>i</i><br/>
<i>Di</i>+1<br/>
<b>Metric&#160;position:&#160;</b>The&#160;position&#160;(beat)&#160;of&#160;a&#160;note&#160;in&#160;a&#160;measure.&#160;For&#160;example,&#160;under&#160;a&#160;time<br/>
signature&#160;of&#160;4,&#160;if&#160;a&#160;measure&#160;consists&#160;of&#160;five&#160;notes,&#160;they&#160;will&#160;have&#160;metric&#160;position&#160;of<br/>
4<br/>
22<br/>
<hr>
<A name=35></a><IMG src="thesis-35_1.jpg"/><br/>
<IMG src="thesis-35_2.png"/><br/>
Figure&#160;3.6:&#160;Metric&#160;position<br/>
1,&#160;2,&#160;2.5,&#160;3&#160;and&#160;4,&#160;respectively.<br/>
Metric&#160;position&#160;usually&#160;implies&#160;beat&#160;strength.&#160;In&#160;most&#160;tonal&#160;music,&#160;there&#160;exist&#160;a<br/>
hierarchy&#160;of&#160;beat&#160;strength.&#160;For&#160;example,&#160;for&#160;a&#160;time&#160;signature&#160;of&#160;4,&#160;the&#160;first&#160;note<br/>
4<br/>
is&#160;usually&#160;the&#160;strongest,&#160;the&#160;third&#160;note&#160;is&#160;the&#160;second&#160;strongest,&#160;and&#160;the&#160;second&#160;and<br/>
fourth&#160;notes&#160;are&#160;the&#160;least&#160;strong.<br/>
<b>3.5.2</b><br/>
<b>Performance&#160;Features</b><br/>
Performance&#160;features&#160;are&#160;the&#160;expressive&#160;expressions&#160;we&#160;would&#160;like&#160;to&#160;learn&#160;from&#160;a&#160;per-<br/>
formance.&#160;Performance&#160;features&#160;are&#160;extracted&#160;by&#160;calculating&#160;how&#160;the&#160;expression&#160;deviates<br/>
from&#160;the&#160;nominal&#160;notation&#160;in&#160;the&#160;score.&#160;Performance&#160;features&#160;includes:<br/>
<b>Onset&#160;time&#160;deviation:&#160;</b>A&#160;human&#160;performer&#160;usually&#160;adds&#160;conscious&#160;or&#160;unconsious&#160;rubato<br/>
ot&#160;their&#160;performance.&#160;The&#160;onset&#160;time&#160;deviation&#160;is&#160;the&#160;difference&#160;of&#160;onset&#160;timing<br/>
between&#160;the&#160;performance&#160;and&#160;the&#160;score.&#160;Namely,<br/>
∆<i>O&#160;</i>=&#160;<i>Operf&#160;</i>−&#160;<i>Oscore</i><br/>
<i>i</i><br/>
<i>i</i><br/>
Where&#160;<i>Operf&#160;</i>is&#160;the&#160;onset&#160;time&#160;of&#160;note&#160;<i>i&#160;</i>in&#160;the&#160;performance,&#160;<i>Oscore&#160;</i>is&#160;the&#160;onset&#160;time<br/>
<i>i</i><br/>
<i>i</i><br/>
of&#160;note&#160;<i>i&#160;</i>in&#160;the&#160;score.<br/>
However,&#160;the&#160;above&#160;formula&#160;assumes&#160;the&#160;performance&#160;is&#160;played&#160;at&#160;the&#160;exact&#160;same<br/>
tempo&#160;assigned&#160;by&#160;the&#160;score.&#160;However,&#160;performers&#160;can't&#160;always&#160;keep&#160;up&#160;with&#160;the<br/>
speed&#160;of&#160;the&#160;score&#160;because&#160;of&#160;limited&#160;piano&#160;skill,&#160;or&#160;they&#160;may&#160;speed&#160;up&#160;or&#160;slow&#160;down<br/>
certain&#160;sections&#160;as&#160;expression.&#160;Therefore,&#160;the&#160;performance&#160;should&#160;be&#160;linearly&#160;scaled<br/>
to&#160;avoid&#160;systematic&#160;bias,&#160;We&#160;will&#160;present&#160;a&#160;solution&#160;to&#160;this&#160;issue&#160;in&#160;Section&#160;<a href="thesiss.html#36">3.5.3</a>.<br/>
<b>Loudness:&#160;</b>The&#160;loudness&#160;of&#160;a&#160;note.&#160;Measured&#160;by&#160;MIDI&#160;velocity&#160;level&#160;0&#160;to&#160;127.<br/>
23<br/>
<hr>
<A name=36></a><IMG src="thesis-36_1.jpg"/><br/>
Figure&#160;3.7:&#160;Systematic&#160;bias&#160;in&#160;onset&#160;deviation<br/>
<b>Relative&#160;duration:&#160;</b>The&#160;performed&#160;duration&#160;of&#160;a&#160;note&#160;divided&#160;by&#160;the&#160;nominal&#160;duration&#160;on<br/>
the&#160;score.<br/>
<i>Dperf</i><br/>
<i>RD&#160;</i>=<br/>
<i>i</i><br/>
<i>Dscore</i><br/>
<i>i</i><br/>
<b>3.5.3</b><br/>
<b>Normalizing&#160;Onset&#160;Deviation</b><br/>
In&#160;the&#160;previous&#160;section,&#160;we&#160;mentioned&#160;that&#160;the&#160;onset&#160;deviation&#160;feature&#160;will&#160;have&#160;prob-<br/>
lems&#160;when&#160;the&#160;performer&#160;did&#160;not&#160;play&#160;at&#160;the&#160;exact&#160;tempo&#160;indicated&#160;by&#160;the&#160;score.&#160;As&#160;illus-<br/>
trated&#160;in&#160;Fig.&#160;<a href="thesiss.html#36">3.7</a>,&#160;if&#160;the&#160;performance&#160;is&#160;played&#160;slower&#160;than&#160;expected,&#160;the&#160;deviation&#160;will<br/>
grow&#160;larger&#160;and&#160;larger&#160;over&#160;time&#160;same,&#160;and&#160;vice&#160;versa&#160;is&#160;it's&#160;played&#160;faster.&#160;The&#160;systematic<br/>
bias&#160;caused&#160;by&#160;the&#160;difference&#160;in&#160;total&#160;duration&#160;will&#160;mix&#160;up&#160;with&#160;the&#160;local&#160;deviation,&#160;For&#160;a<br/>
long&#160;phrase,&#160;the&#160;onset&#160;deviation&#160;of&#160;the&#160;last&#160;notes&#160;can&#160;be&#160;as&#160;larger&#160;as&#160;a&#160;dozen&#160;quarter&#160;notes.<br/>
These&#160;kind&#160;of&#160;extremely&#160;large&#160;values&#160;will&#160;be&#160;learned&#160;by&#160;the&#160;model&#160;and&#160;cause&#160;erroneous<br/>
predictions.&#160;A&#160;note&#160;may&#160;be&#160;delayed&#160;for&#160;a&#160;few&#160;quarter&#160;notes,&#160;causing&#160;it&#160;the&#160;notes&#160;to&#160;be<br/>
played&#160;in&#160;the&#160;wrong&#160;order.<br/>
In&#160;other&#160;words,&#160;the&#160;onset&#160;deviation&#160;actually&#160;contains&#160;two&#160;type&#160;of&#160;deviation:&#160;a&#160;global/<br/>
systematic&#160;deviation&#160;cause&#160;by&#160;the&#160;difference&#160;between&#160;performed&#160;and&#160;nominal&#160;tempo,&#160;and<br/>
a&#160;local&#160;deviation&#160;cause&#160;by&#160;note-level&#160;expression.&#160;Since&#160;the&#160;intention&#160;of&#160;the&#160;onset&#160;deviation<br/>
feature&#160;is&#160;to&#160;capture&#160;the&#160;note-level&#160;expression,&#160;the&#160;performance&#160;must&#160;be&#160;linearly&#160;scaled&#160;to<br/>
cancel&#160;out&#160;the&#160;global&#160;deviation.<br/>
Initially,&#160;we&#160;tried&#160;two&#160;possible&#160;way&#160;of&#160;normalization:<br/>
1.&#160;Align&#160;the&#160;onset&#160;of&#160;the&#160;first&#160;notes,&#160;and&#160;align&#160;the&#160;onset&#160;of&#160;the&#160;last&#160;notes.<br/>
2.&#160;Align&#160;the&#160;onset&#160;of&#160;the&#160;first&#160;notes,&#160;and&#160;align&#160;the&#160;end&#160;(MIDI&#160;note-off&#160;event)&#160;of&#160;the&#160;last<br/>
24<br/>
<hr>
<A name=37></a><IMG src="thesis-37_1.jpg"/><br/>
notes.<br/>
However,&#160;neither&#160;of&#160;the&#160;method&#160;can&#160;robustly&#160;eliminate&#160;extreme&#160;values.&#160;Therefore,&#160;we<br/>
proposed&#160;an&#160;automated&#160;approach&#160;to&#160;find&#160;the&#160;best&#160;scaling&#160;ratio&#160;such&#160;that&#160;the&#160;normalized<br/>
onset&#160;deviations&#160;in&#160;the&#160;performances&#160;fits&#160;best&#160;with&#160;those&#160;in&#160;the&#160;score.&#160;The&#160;measure&#160;of<br/>
fitting&#160;is&#160;defined&#160;as&#160;the&#160;Euclidean&#160;distance&#160;between&#160;the&#160;normalized&#160;performance&#160;onset<br/>
sequences&#160;and&#160;the&#160;score&#160;onset&#160;sequences,&#160;represented&#160;as&#160;vectors.&#160;Brent's&#160;Method&#160;[64]&#160;is<br/>
used&#160;to&#160;find&#160;this&#160;optimal&#160;ratio.&#160;To&#160;speed&#160;up&#160;the&#160;optimization&#160;and&#160;prevent&#160;unreasonable<br/>
local&#160;minima&#160;value,&#160;a&#160;search&#160;range&#160;of&#160;[<i>initial&#160;guess&#160;</i>×&#160;0<i>.</i>5<i>,&#160;initial&#160;guess&#160;</i>×&#160;2]&#160;is&#160;imposed<br/>
on&#160;the&#160;optimizer.&#160;The&#160;<i>initial&#160;guess&#160;</i>is&#160;used&#160;as&#160;a&#160;rough&#160;estimate&#160;of&#160;the&#160;ratio,&#160;calculated&#160;by<br/>
aligning&#160;the&#160;first&#160;and&#160;last&#160;onsets.&#160;Than&#160;we&#160;assume&#160;the&#160;actual&#160;ratio&#160;will&#160;not&#160;be&#160;smaller&#160;than<br/>
half&#160;of&#160;<i>initial&#160;guess&#160;</i>and&#160;not&#160;larger&#160;than&#160;twice&#160;of&#160;<i>initial&#160;guess</i>.&#160;The&#160;two&#160;numbers&#160;0.5&#160;and<br/>
2&#160;are&#160;chosen&#160;by&#160;trail&#160;and&#160;error,&#160;and&#160;most&#160;of&#160;the&#160;empirical&#160;data&#160;supports&#160;this&#160;decision.&#160;We<br/>
will&#160;demonstrate&#160;the&#160;effectiveness&#160;of&#160;this&#160;solution&#160;in&#160;Section&#160;<a href="thesiss.html#48">5.1</a>.<br/>
25<br/>
<hr>
<A name=38></a><IMG src="thesis-38_1.jpg"/><br/>
<b>Chapter&#160;4</b><br/>
<b>Corpus&#160;Preparation</b><br/>
An&#160;expressive&#160;performance&#160;corpus&#160;is&#160;a&#160;set&#160;of&#160;performance&#160;samples.&#160;Since&#160;this&#160;re-<br/>
search&#160;is&#160;based&#160;on&#160;a&#160;supervised&#160;learning&#160;algorithm,&#160;a&#160;high-quality&#160;corpus&#160;is&#160;essential&#160;to<br/>
our&#160;success.&#160;Each&#160;sample&#160;consists&#160;of&#160;a&#160;score&#160;and&#160;its&#160;corresponding&#160;human&#160;recording.<br/>
Some&#160;metadata&#160;such&#160;as&#160;phrasing,&#160;structure&#160;analysis,&#160;or&#160;harmonic&#160;analysis.&#160;may&#160;also&#160;be<br/>
included.&#160;In&#160;this&#160;chapter,&#160;we&#160;will&#160;review&#160;some&#160;the&#160;existing&#160;corpora,&#160;specifications&#160;and<br/>
formats&#160;of&#160;our&#160;corpus,&#160;and&#160;how&#160;we&#160;actually&#160;construct&#160;it.<br/>
<b>4.1</b><br/>
<b>Existing&#160;Corpora</b><br/>
Unlike&#160;other&#160;research&#160;fields&#160;like&#160;speech&#160;processing&#160;or&#160;natural&#160;language&#160;processing,<br/>
there&#160;exist&#160;virtually&#160;no&#160;public&#160;accessible&#160;corpus&#160;for&#160;computer&#160;expressive&#160;performance&#160;re-<br/>
search.&#160;CrestMusePEDB&#160;[65]&#160;(PEDB&#160;stands&#160;for&#160;“Performance&#160;Expression&#160;Database”)&#160;is<br/>
a&#160;corpus&#160;created&#160;by&#160;Japan&#160;Science&#160;and&#160;Technology&#160;Agency's&#160;CREST&#160;program.&#160;However,<br/>
until&#160;the&#160;time&#160;of&#160;this&#160;writing,&#160;we&#160;can't&#160;establish&#160;any&#160;contact&#160;with&#160;the&#160;database&#160;administra-<br/>
tors&#160;to&#160;gain&#160;access&#160;to&#160;it.&#160;They&#160;claims&#160;to&#160;have&#160;a&#160;GUI&#160;tool&#160;for&#160;annotate&#160;the&#160;expressive&#160;per-<br/>
formance&#160;parameters&#160;from&#160;audio&#160;recordings.&#160;Their&#160;repertoire&#160;covers&#160;many&#160;piano&#160;works<br/>
from&#160;well-known&#160;classical&#160;composers&#160;like&#160;Bach,&#160;Mozart,&#160;and&#160;Chopin,&#160;and&#160;are&#160;recorded<br/>
by&#160;world&#160;famous&#160;pianists.&#160;From&#160;their&#160;website&#160;[65]&#160;they&#160;claim&#160;to&#160;contain&#160;the&#160;following<br/>
data:&#160;PEDB-SCR&#160;-&#160;score&#160;text&#160;information,&#160;PEDB-DEV&#160;-&#160;performance&#160;deviation&#160;data&#160;and<br/>
PEDB-IDX&#160;-&#160;audio&#160;performance&#160;credit.&#160;But&#160;the&#160;quality&#160;of&#160;the&#160;data&#160;is&#160;unknown.<br/>
26<br/>
<hr>
<A name=39></a><IMG src="thesis-39_1.jpg"/><br/>
Another&#160;example&#160;is&#160;the&#160;Magaloff&#160;Project&#160;[66],&#160;which&#160;is&#160;created&#160;by&#160;some&#160;universities<br/>
in&#160;Austria.&#160;They&#160;invited&#160;Russian&#160;pianist&#160;Nikita&#160;Magaloff&#160;to&#160;record&#160;all&#160;solo&#160;works&#160;for&#160;piano<br/>
by&#160;Frederic&#160;Chopin&#160;on&#160;a&#160;Bösendorfer&#160;SE&#160;computer-controlled&#160;grand&#160;piano.&#160;This&#160;corpus<br/>
became&#160;the&#160;material&#160;for&#160;many&#160;subsequent&#160;researches&#160;[67--73].&#160;Flossmann&#160;et&#160;al.,&#160;one&#160;of<br/>
the&#160;leading&#160;researchers&#160;of&#160;the&#160;project,&#160;also&#160;won&#160;the&#160;2008&#160;RenCon&#160;contest&#160;with&#160;a&#160;system<br/>
based&#160;on&#160;this&#160;corpus&#160;called&#160;YQX&#160;[74].&#160;However,&#160;the&#160;corpus&#160;is&#160;not&#160;opened&#160;up&#160;to&#160;the&#160;public.<br/>
Since&#160;both&#160;corpora&#160;are&#160;not&#160;available,&#160;we&#160;need&#160;to&#160;implement&#160;our&#160;own&#160;.&#160;We&#160;will&#160;start&#160;by<br/>
defining&#160;the&#160;specification.<br/>
<b>4.2</b><br/>
<b>Corpus&#160;Specification</b><br/>
The&#160;corpus&#160;we&#160;need&#160;must&#160;fulfill&#160;the&#160;following&#160;criterias:<br/>
1.&#160;All&#160;the&#160;samples&#160;are&#160;monophonic,&#160;containing&#160;only&#160;a&#160;single&#160;melody&#160;without&#160;chords.<br/>
2.&#160;No&#160;human&#160;error,&#160;such&#160;as&#160;insertion,&#160;deletion,&#160;or&#160;wrong&#160;pitch&#160;exist&#160;in&#160;the&#160;recording;<br/>
the&#160;score&#160;and&#160;recording&#160;are&#160;matched&#160;note-to-note.<br/>
3.&#160;Phrasing&#160;is&#160;annotated&#160;by&#160;human.<br/>
4.&#160;The&#160;scores,&#160;recordings&#160;and&#160;phrasing&#160;data&#160;are&#160;in&#160;machine-readable&#160;format.<br/>
Some&#160;potentially&#160;useful&#160;information&#160;are&#160;not&#160;included&#160;because&#160;they&#160;are&#160;less&#160;relevant&#160;to<br/>
our&#160;goal.&#160;Examples&#160;are:<br/>
1.&#160;Advanced&#160;structural&#160;analysis,&#160;such&#160;as&#160;GTTM&#160;(Generative&#160;Theory&#160;of&#160;Tonal&#160;Music)<br/>
[75]<br/>
2.&#160;Harmonic&#160;analysis<br/>
3.&#160;Piano&#160;paddle&#160;usage<br/>
4.&#160;Piano&#160;fingering<br/>
5.&#160;Other&#160;instrument&#160;specific&#160;techniques,&#160;such&#160;as&#160;violin&#160;pizzicato,&#160;tapping,&#160;or&#160;bow&#160;tech-<br/>
niques.<br/>
27<br/>
<hr>
<A name=40></a><IMG src="thesis-40_1.jpg"/><br/>
Table&#160;4.1:&#160;Clementi's&#160;Sonatinas&#160;Op.36<br/>
<b>Title</b><br/>
<b>Movement</b><br/>
<b>Time&#160;Signature</b><br/>
No.1&#160;Sonatina&#160;in&#160;C&#160;major<br/>
I.&#160;Allegro<br/>
4/4<br/>
II.&#160;Andante<br/>
3/4<br/>
III.&#160;Vivace<br/>
3/8<br/>
No.2&#160;Sonatina&#160;in&#160;G&#160;major<br/>
I.&#160;Allegretto<br/>
2/4<br/>
II.&#160;Allegretto<br/>
3/4<br/>
III.&#160;Allegro<br/>
3/8<br/>
No.3&#160;Sonatina&#160;in&#160;C&#160;major<br/>
I.&#160;Spiritoso<br/>
4/4<br/>
II.&#160;Un&#160;poco&#160;adagio<br/>
2/2<br/>
III.&#160;Allegro<br/>
2/4<br/>
No.4&#160;Sonatina&#160;in&#160;F&#160;major<br/>
I.&#160;Con&#160;spirito<br/>
3/4<br/>
II.&#160;Andante&#160;con&#160;espressione<br/>
2/4<br/>
III.&#160;Rondó:&#160;Allegro&#160;vivace<br/>
2/4<br/>
No.5&#160;Sonatina&#160;in&#160;G&#160;major<br/>
I.&#160;Presto<br/>
2/2<br/>
II.&#160;Allegretto&#160;moderato<br/>
3/8<br/>
III.&#160;Rondó:&#160;Allegro&#160;molto<br/>
2/4<br/>
No.6&#160;Sonatina&#160;in&#160;D&#160;major<br/>
I.&#160;Allegro&#160;con&#160;spirito<br/>
4/4<br/>
II.&#160;Allegretto<br/>
6/8<br/>
We&#160;choose&#160;Clementi's&#160;Sonatina&#160;Op.36&#160;for&#160;our&#160;corpus,&#160;because&#160;it&#160;is&#160;a&#160;must-learn&#160;reper-<br/>
toire&#160;for&#160;piano&#160;students,&#160;so&#160;it's&#160;easy&#160;to&#160;find&#160;performers&#160;with&#160;a&#160;wide&#160;range&#160;of&#160;skill&#160;level&#160;to<br/>
record&#160;the&#160;corpus.&#160;These&#160;sonatinas&#160;are&#160;in&#160;classical&#160;style,&#160;so&#160;the&#160;learned&#160;model&#160;can&#160;easily<br/>
be&#160;extended&#160;to&#160;other&#160;classical&#160;era&#160;works&#160;like&#160;Mozart&#160;and&#160;Haydn.&#160;There&#160;are&#160;six&#160;sonatinas<br/>
included&#160;in&#160;Op.36,&#160;the&#160;first&#160;five&#160;have&#160;three&#160;movements&#160;each,&#160;and&#160;the&#160;last&#160;one&#160;has&#160;two<br/>
movements.&#160;The&#160;titles&#160;and&#160;time&#160;signatures&#160;of&#160;all&#160;the&#160;pieces&#160;are&#160;listed&#160;in&#160;Table&#160;<a href="thesiss.html#40">4.1</a><br/>
MusicXML&#160;is&#160;used&#160;to&#160;represent&#160;Clementi's&#160;work&#160;in&#160;digital&#160;format.&#160;MusicXML&#160;is&#160;a<br/>
digital&#160;score&#160;notation&#160;using&#160;XML&#160;(eXtensible&#160;Markup&#160;Language),&#160;it&#160;can&#160;express&#160;most<br/>
traditional&#160;music&#160;notations&#160;and&#160;metadata.&#160;Most&#160;music&#160;notation&#160;software&#160;and&#160;software&#160;tool<br/>
supports&#160;musicXML&#160;format.&#160;Although&#160;MIDI&#160;is&#160;also&#160;a&#160;possible&#160;candidate&#160;for&#160;representing<br/>
score,&#160;it&#160;is&#160;designed&#160;to&#160;hold&#160;instrument&#160;control&#160;signal&#160;rather&#160;than&#160;notation,&#160;so&#160;some&#160;music<br/>
symbols&#160;may&#160;not&#160;be&#160;available&#160;in&#160;MIDI.&#160;Furthermore,&#160;MIDI&#160;represents&#160;music&#160;as&#160;a&#160;series&#160;of<br/>
note-on&#160;and&#160;note-off&#160;events,&#160;which&#160;requires&#160;additional&#160;effort&#160;to&#160;transform&#160;into&#160;traditional<br/>
notation.<br/>
But&#160;for&#160;representing&#160;performance,&#160;MIDI&#160;is&#160;the&#160;most&#160;suitable&#160;format.&#160;Using&#160;a&#160;key-<br/>
pressure-sensitive&#160;digital&#160;piano,&#160;pianist&#160;can&#160;record&#160;in&#160;a&#160;natural&#160;way.&#160;The&#160;recordings&#160;will<br/>
have&#160;high&#160;precision&#160;in&#160;time,&#160;pitch&#160;and&#160;loudness&#160;(key&#160;pressure),&#160;and&#160;polyphonic&#160;tracks&#160;can<br/>
28<br/>
<hr>
<A name=41></a><IMG src="thesis-41_1.jpg"/><br/>
easily&#160;be&#160;recorded&#160;separately.&#160;Although&#160;WAV&#160;(Waveform&#160;Audio&#160;Format)&#160;audio&#160;record-<br/>
ing&#160;has&#160;higher&#160;fidelity&#160;than&#160;MIDI,&#160;but&#160;they&#160;are&#160;harder&#160;to&#160;parse&#160;by&#160;computers.&#160;Without<br/>
robust&#160;onset&#160;detection,&#160;pitch&#160;detection,&#160;and&#160;source&#160;separation&#160;technology,&#160;the&#160;information<br/>
is&#160;extremely&#160;difficult&#160;to&#160;extract.&#160;Manually&#160;annotate&#160;each&#160;WAV&#160;recording&#160;takes&#160;unrealistic<br/>
effort,&#160;and&#160;the&#160;accuracy&#160;across&#160;different&#160;annotators&#160;may&#160;not&#160;be&#160;consistent.<br/>
There&#160;is&#160;a&#160;way&#160;to&#160;keep&#160;both&#160;the&#160;score&#160;and&#160;the&#160;recording&#160;in&#160;one&#160;single&#160;MIDI&#160;file.&#160;Instead<br/>
of&#160;recording&#160;the&#160;actual&#160;note-on&#160;and&#160;note-off&#160;timing,&#160;we&#160;keep&#160;the&#160;nominal&#160;note-on&#160;and&#160;note-<br/>
off&#160;the&#160;same&#160;as&#160;in&#160;the&#160;score.&#160;Then,&#160;MIDI&#160;tempo-change&#160;events&#160;are&#160;inserted&#160;before&#160;each<br/>
note&#160;to&#160;shift&#160;the&#160;performed&#160;timing&#160;of&#160;the&#160;recorded&#160;notes.&#160;Thus,&#160;the&#160;nominal&#160;time&#160;of&#160;each<br/>
note&#160;represents&#160;the&#160;score,&#160;and&#160;the&#160;rendered&#160;time&#160;represents&#160;the&#160;performance.&#160;But&#160;since<br/>
MIDI&#160;is&#160;so&#160;limited&#160;as&#160;a&#160;score&#160;format,&#160;and&#160;it&#160;requires&#160;complex&#160;calculations&#160;to&#160;recover&#160;the<br/>
performance,&#160;this&#160;method&#160;is&#160;not&#160;used&#160;in&#160;the&#160;research.<br/>
Finally,&#160;we&#160;store&#160;the&#160;phrasing,&#160;which&#160;is&#160;the&#160;only&#160;metadata&#160;we&#160;used,&#160;in&#160;a&#160;plaintext&#160;file,<br/>
each&#160;line&#160;in&#160;the&#160;phrasing&#160;file&#160;is&#160;the&#160;starting&#160;point&#160;of&#160;each&#160;phrase.&#160;The&#160;starting&#160;point&#160;is<br/>
defined&#160;as&#160;the&#160;onset&#160;timing&#160;(in&#160;quarter&#160;notes)&#160;counted&#160;from&#160;the&#160;beginning&#160;of&#160;the&#160;<a href="">piece1</a>.<br/>
The&#160;phrasing&#160;is&#160;decided&#160;by&#160;the&#160;us&#160;using&#160;the&#160;following&#160;principles:<br/>
1.&#160;Phrase&#160;may&#160;be&#160;separated&#160;by&#160;a&#160;salient&#160;pause.<br/>
2.&#160;Phrase&#160;may&#160;end&#160;with&#160;a&#160;cadence.<br/>
3.&#160;Phrase&#160;may&#160;be&#160;separated&#160;by&#160;dramatic&#160;change&#160;in&#160;tempo,&#160;key&#160;or&#160;loudness.<br/>
4.&#160;Repeated&#160;structures&#160;in&#160;tempo&#160;or&#160;pitch&#160;may&#160;be&#160;a&#160;repeated&#160;phrase.<br/>
Since&#160;phrasing&#160;controls&#160;the&#160;structural&#160;interpretation&#160;of&#160;a&#160;piece,&#160;we&#160;would&#160;like&#160;to&#160;leave<br/>
this&#160;freedom&#160;for&#160;expression&#160;to&#160;the&#160;user.&#160;However,&#160;if&#160;there&#160;exist&#160;any&#160;good&#160;automatic&#160;phras-<br/>
ing&#160;algorithm,&#160;it&#160;can&#160;be&#160;easily&#160;integrated&#160;into&#160;the&#160;current&#160;system&#160;to&#160;make&#160;it&#160;full-automatic.<br/>
1For&#160;a&#160;phrase&#160;that&#160;start&#160;at&#160;a&#160;point&#160;which&#160;is&#160;a&#160;circulating&#160;decimal,&#160;for&#160;example&#160;2&#160;1&#160;=&#160;2<i>.</i>333&#160;·&#160;·&#160;·&#160;,&#160;the&#160;starting<br/>
3<br/>
point&#160;can&#160;be&#160;alternatively&#160;defined&#160;as&#160;any&#160;finite&#160;decimal&#160;between&#160;the&#160;end&#160;of&#160;the&#160;last&#160;phrase&#160;and&#160;the&#160;start&#160;of<br/>the&#160;current&#160;phrase.&#160;For&#160;example,&#160;if&#160;the&#160;last&#160;phrase&#160;stops&#160;at&#160;beat&#160;1,&#160;the&#160;second&#160;phrase&#160;start&#160;at&#160;2&#160;1&#160;=&#160;2<i>.</i>333&#160;·&#160;·&#160;·<br/>
3<br/>
beat,&#160;the&#160;start&#160;point&#160;of&#160;the&#160;second&#160;phrase&#160;can&#160;be&#160;written&#160;as&#160;2.3&#160;or&#160;2.0,&#160;etc.<br/>
29<br/>
<hr>
<A name=42></a><IMG src="thesis-42_1.jpg"/><br/>
<b>4.3</b><br/>
<b>Implementation</b><br/>
<b>4.3.1</b><br/>
<b>Score&#160;Preparation</b><br/>
The&#160;digital&#160;scores&#160;are&#160;downloaded&#160;from&#160;KernScore&#160;website&#160;[76].&#160;The&#160;scores&#160;are&#160;trans-<br/>
formed&#160;into&#160;MusicXML&#160;from&#160;the&#160;original&#160;Hundrum&#160;file&#160;format&#160;(.krn)&#160;using&#160;the&#160;music21<br/>
toolkit&#160;[59].&#160;Because&#160;this&#160;research&#160;focus&#160;on&#160;monophonic&#160;melody&#160;only,&#160;the&#160;accompani-<br/>
ments&#160;are&#160;remove&#160;and&#160;the&#160;chords&#160;are&#160;reduced&#160;to&#160;their&#160;highest-pitched&#160;note,&#160;which&#160;is&#160;usu-<br/>
ally&#160;the&#160;most&#160;salient&#160;melody.&#160;The&#160;reduced&#160;scores&#160;are&#160;doubled-checked&#160;against&#160;a&#160;printed<br/>
version&#160;publish&#160;by&#160;Durand&#160;&amp;&#160;Cie.,&#160;Paris&#160;[77]&#160;to&#160;eliminate&#160;all&#160;errors.<br/>
<b>4.3.2</b><br/>
<b>MIDI&#160;Recording</b><br/>
We&#160;have&#160;implemented&#160;two&#160;methods&#160;for&#160;recording:&#160;First,&#160;using&#160;a&#160;Yamaha&#160;digital&#160;piano<br/>
to&#160;record&#160;MIDI.&#160;Second,&#160;by&#160;tapping&#160;on&#160;a&#160;touch-sensitive&#160;device&#160;to&#160;express&#160;tempo,&#160;duration<br/>
and&#160;loudness.&#160;Due&#160;to&#160;accuracy&#160;consideration,&#160;only&#160;the&#160;recordings&#160;from&#160;Yamaha&#160;digital<br/>
piano&#160;are&#160;used&#160;in&#160;the&#160;expreiments.<br/>
We&#160;used&#160;a&#160;Yamaha&#160;P80&#160;88-key&#160;graded&#160;hammer&#160;effect<a href="">2</a>digital&#160;piano&#160;for&#160;recording.<br/>
Through&#160;a&#160;MIDI-to-USB&#160;converter,&#160;the&#160;keyboard&#160;was&#160;connected&#160;to&#160;Rosegarden&#160;Digital<br/>
Audio&#160;Workstation&#160;(DAW)&#160;software&#160;on&#160;a&#160;Linux&#160;computer.&#160;The&#160;Rosegarden&#160;DAW&#160;also<br/>
generated&#160;the&#160;metronome&#160;sound&#160;to&#160;help&#160;the&#160;performer&#160;maintain&#160;a&#160;steady&#160;speed.&#160;Metronome<br/>
is&#160;mandatory&#160;because&#160;if&#160;the&#160;performer&#160;plays&#160;freely,&#160;the&#160;tempo&#160;information&#160;written&#160;in&#160;the<br/>
MIDI&#160;file&#160;will&#160;be&#160;invalid,&#160;which&#160;makes&#160;subsequent&#160;parsing&#160;and&#160;linear&#160;scaling&#160;very&#160;diffi-<br/>
cult.&#160;So&#160;the&#160;performers&#160;were&#160;asked&#160;to&#160;follow&#160;the&#160;speed&#160;of&#160;the&#160;metronome,&#160;but&#160;they&#160;can<br/>
adjust&#160;the&#160;metronome&#160;speed&#160;as&#160;they&#160;like,&#160;and&#160;apply&#160;any&#160;level&#160;of&#160;rubato&#160;as&#160;long&#160;as&#160;the<br/>
overall&#160;tempo&#160;is&#160;steady.<br/>
The&#160;second&#160;method,&#160;which&#160;is&#160;not&#160;used&#160;in&#160;the&#160;experiments,&#160;is&#160;to&#160;utilize&#160;touch-enabled<br/>
input&#160;device&#160;like&#160;smartphone&#160;touchscreen&#160;or&#160;laptop&#160;touchpad.&#160;We&#160;have&#160;implemented&#160;an<br/>
prototype&#160;using&#160;a&#160;Synaptics&#160;Touchpad&#160;on&#160;a&#160;Lenovo&#160;ThinkPad&#160;X200i&#160;laptop.&#160;When&#160;the&#160;user<br/>
taps&#160;the&#160;touchpad&#160;once,&#160;one&#160;note&#160;from&#160;the&#160;score&#160;will&#160;be&#160;played,&#160;the&#160;duration&#160;and&#160;loudness<br/>
2Graded&#160;Hammer&#160;Effect&#160;feature&#160;provides&#160;realistic&#160;key&#160;pressure&#160;response&#160;similar&#160;to&#160;a&#160;traditional&#160;acoustic<br/>
piano.<br/>
30<br/>
<hr>
<A name=43></a><IMG src="thesis-43_1.jpg"/><br/>
will&#160;be&#160;controlled&#160;by&#160;the&#160;duration&#160;and&#160;pressure&#160;of&#160;the&#160;tapping&#160;action.&#160;So&#160;the&#160;user&#160;can<br/>
“play”&#160;the&#160;touchpad&#160;like&#160;a&#160;musical&#160;instrument.&#160;This&#160;idea&#160;has&#160;already&#160;be&#160;used&#160;in&#160;musical<br/>
games&#160;and&#160;toys.&#160;This&#160;method&#160;is&#160;more&#160;user-friendly&#160;to&#160;general&#160;public&#160;because&#160;it&#160;requires<br/>
minimal&#160;instrument&#160;skill&#160;and&#160;utilize&#160;widely&#160;available&#160;hardware.&#160;But&#160;most&#160;touchpad&#160;esti-<br/>
mates&#160;pressure&#160;by&#160;finger&#160;contact&#160;area,&#160;so&#160;the&#160;accuracy&#160;in&#160;pressure&#160;is&#160;not&#160;very&#160;satisfying.<br/>
But&#160;it&#160;is&#160;indeed&#160;a&#160;low&#160;cost&#160;alternative&#160;to&#160;MIDI&#160;digital&#160;piano.<br/>
<b>4.3.3</b><br/>
<b>MIDI&#160;Cleaning&#160;and&#160;Phrase&#160;Splitting</b><br/>
After&#160;MIDIs&#160;are&#160;recorded,&#160;we&#160;use&#160;Python&#160;scripts&#160;to&#160;check&#160;if&#160;each&#160;recording&#160;is&#160;matched<br/>
note-to-note&#160;with&#160;its&#160;corresponding&#160;score.&#160;If&#160;not,&#160;the&#160;mistakes&#160;are&#160;manually&#160;corrected.<br/>
If&#160;there&#160;are&#160;a&#160;small&#160;segments&#160;that&#160;are&#160;totally&#160;messed&#160;up,&#160;they&#160;will&#160;be&#160;reconstruct&#160;using<br/>
repeated&#160;or&#160;similar&#160;segments&#160;from&#160;the&#160;same&#160;piece.&#160;The&#160;matched&#160;score&#160;and&#160;MIDI&#160;pairs&#160;are<br/>
then&#160;split&#160;into&#160;phrases&#160;according&#160;to&#160;the&#160;corresponding&#160;phrasing&#160;file.&#160;The&#160;split&#160;phrases&#160;are<br/>
checked&#160;once&#160;again&#160;for&#160;note-to-note&#160;match&#160;before&#160;they&#160;are&#160;put&#160;into&#160;experiment.<br/>
<b>4.4</b><br/>
<b>Results</b><br/>
Six&#160;graduate&#160;students&#160;(not&#160;majored&#160;in&#160;music)&#160;were&#160;invited&#160;to&#160;record&#160;the&#160;samples.&#160;The<br/>
number&#160;of&#160;mistakes&#160;they&#160;made&#160;are&#160;listed&#160;in&#160;Table&#160;<a href="thesiss.html#44">4.2.</a><a href="">3&#160;</a>These&#160;mistakes&#160;are&#160;identified<br/>
using&#160;the&#160;unix&#160;diff&#160;[78]&#160;tool.&#160;Five&#160;of&#160;them&#160;(A&#160;to&#160;E)&#160;finished&#160;Clementi's&#160;entire&#160;Op.36,<br/>
while&#160;performer&#160;F&#160;only&#160;recorded&#160;part&#160;of&#160;the&#160;work.&#160;The&#160;total&#160;number&#160;of&#160;recordings&#160;and&#160;the<br/>
corresponding&#160;phrases/notes&#160;counts&#160;are&#160;shown&#160;in&#160;Table&#160;<a href="thesiss.html#46">4.3.</a><br/>
The&#160;number&#160;of&#160;phrases&#160;(according&#160;to&#160;our&#160;phrasing&#160;annotation)&#160;and&#160;notes&#160;are&#160;shown&#160;in<br/>
Table&#160;<a href="thesiss.html#46">4.4.&#160;</a>Fig.&#160;<a href="thesiss.html#45">4.1&#160;</a>shows&#160;the&#160;length&#160;distribution&#160;of&#160;each&#160;movement,&#160;most&#160;movements&#160;have<br/>
around&#160;a&#160;few&#160;hundred&#160;notes,&#160;except&#160;the&#160;long&#160;No.6&#160;and&#160;some&#160;short&#160;second&#160;movements.&#160;Fig.<br/>
<a href="thesiss.html#47">4.2&#160;</a>shows&#160;the&#160;length&#160;distribution&#160;in&#160;numbers&#160;of&#160;phrases,&#160;most&#160;movements&#160;are&#160;around&#160;20<br/>
phrases.&#160;The&#160;length&#160;distribution&#160;of&#160;the&#160;phrases&#160;in&#160;all&#160;six&#160;pieces&#160;are&#160;shown&#160;in&#160;Fig.&#160;<a href="thesiss.html#47">4.3</a>,&#160;most<br/>
phrases&#160;are&#160;shorter&#160;than&#160;30&#160;notes.&#160;Some&#160;very&#160;long&#160;phrases&#160;are&#160;usually&#160;virtuoso&#160;segments<br/>
3The&#160;performers&#160;are&#160;allowed&#160;to&#160;re-record&#160;as&#160;many&#160;time&#160;as&#160;they&#160;want,&#160;so&#160;the&#160;actual&#160;number&#160;of&#160;mistakes<br/>
may&#160;be&#160;higher.<br/>
31<br/>
<hr>
<A name=44></a><IMG src="thesis-44_1.jpg"/><br/>
59<br/>
64<br/>
67<br/>
67<br/>
81<br/>
135<br/>
473<br/>
Subtotal<br/>
1<br/>
7<br/>
1<br/>
2<br/>
6-2<br/>
13<br/>
24<br/>
piece<br/>
4<br/>
6<br/>
the<br/>
6-1<br/>
10<br/>
13<br/>
18<br/>
51<br/>
3<br/>
3<br/>
7<br/>
9<br/>
20<br/>
record<br/>
5-3<br/>
35<br/>
77<br/>
2<br/>
3<br/>
1<br/>
2<br/>
3<br/>
1<br/>1<br/>
didn't<br/>
5-2<br/>
9<br/>
5-1<br/>
12<br/>
10<br/>
10<br/>
23<br/>
64<br/>
9<br/>
6<br/>
3<br/>
3<br/>
performer<br/>
4-3<br/>
22<br/>
43<br/>
the<br/>
5<br/>
3<br/>
2<br/>
6<br/>
6<br/>
4-2<br/>
15<br/>
37<br/>
means<br/>
4<br/>
2<br/>
3<br/>
4-1<br/>
10<br/>
21<br/>
40<br/>
cell<br/>
2<br/>
3<br/>
0<br/>
1<br/>
0<br/>
6<br/>
3-3<br/>
12<br/>
blank<br/>
2<br/>
0<br/>
0<br/>
1<br/>
0<br/>
2<br/>
5<br/>
3-2<br/>
4<br/>
6<br/>
2<br/>
4<br/>
4<br/>
7<br/>
corpus,<br/>
3-1<br/>
27<br/>
the<br/>
0<br/>
1<br/>
1<br/>
1<br/>
3<br/>
8<br/>
in<br/>
2-3<br/>
14<br/>
3<br/>
2<br/>
0<br/>
3<br/>
0<br/>
6<br/>
2-2<br/>
14<br/>
mistakes<br/>
4<br/>
2<br/>
1<br/>
2<br/>
4<br/>
1<br/>1<br/>
24<br/>
of<br/>
2-1<br/>
2<br/>
1<br/>
0<br/>
1<br/>
4<br/>
2<br/>
1-3<br/>
10<br/>
Number<br/>
5<br/>
1<br/>
1<br/>
1<br/>
3<br/>
3<br/>
1-2<br/>
14<br/>
4.2:<br/>
0<br/>
2<br/>
1<br/>
0<br/>
2<br/>
1<br/>
6<br/>
able<br/>
1-1<br/>
T<br/>
A<br/>
B<br/>
C<br/>
D<br/>
E<br/>
F<br/>
Performer<br/>
Subtotal<br/>
32<br/>
<hr>
<A name=45></a><IMG src="thesis-45_1.jpg"/><br/>
&#160;4<br/>
distribution<br/>
&#160;3.5<br/>
&#160;3<br/>
&#160;2.5<br/>
&#160;2<br/>
&#160;1.5<br/>
&#160;1<br/>
Number of Movements<br/>
&#160;0.5<br/>
&#160;0<br/>
&#160;0<br/>
&#160;100&#160;&#160;200&#160;&#160;300&#160;&#160;400&#160;&#160;500&#160;&#160;600&#160;&#160;700&#160;&#160;800&#160;&#160;900<br/>
Movement Length (notes)<br/>
Figure&#160;4.1:&#160;Movement&#160;length&#160;(notes)&#160;distribution<br/>
with&#160;very&#160;fast&#160;note&#160;sequences,&#160;so&#160;it's&#160;hard&#160;to&#160;further&#160;split&#160;them.<br/>
33<br/>
<hr>
<A name=46></a><IMG src="thesis-46_1.jpg"/><br/>
Table&#160;4.3:&#160;Total&#160;recorded&#160;phrases&#160;and&#160;notes&#160;count<br/>
<b>Title</b><br/>
<b>Recordings</b><br/>
<b>Total&#160;Phrases</b><br/>
<b>Total&#160;Notes</b><br/>
<b>Count</b><br/>
No.1&#160;Mov.&#160;I<br/>
6<br/>
72<br/>
1332<br/>
No.1&#160;Mov.&#160;II<br/>
6<br/>
60<br/>
882<br/>
No.1&#160;Mov.&#160;III<br/>
6<br/>
102<br/>
1566<br/>
No.2&#160;Mov.&#160;I<br/>
6<br/>
108<br/>
1920<br/>
No.2&#160;Mov.&#160;II<br/>
6<br/>
36<br/>
750<br/>
No.2&#160;Mov.&#160;III<br/>
6<br/>
168<br/>
2484<br/>
No.3&#160;Mov.&#160;I<br/>
6<br/>
156<br/>
3156<br/>
No.3&#160;Mov.&#160;II<br/>
6<br/>
42<br/>
444<br/>
No.3&#160;Mov.&#160;III<br/>
6<br/>
120<br/>
2628<br/>
No.4&#160;Mov.&#160;I<br/>
5<br/>
80<br/>
2325<br/>
No.4&#160;Mov.&#160;II<br/>
6<br/>
78<br/>
1332<br/>
No.4&#160;Mov.&#160;III<br/>
5<br/>
85<br/>
1920<br/>
No.5&#160;Mov.&#160;I<br/>
5<br/>
85<br/>
3360<br/>
No.5&#160;Mov.&#160;II<br/>
5<br/>
70<br/>
1580<br/>
No.5&#160;Mov.&#160;III<br/>
6<br/>
144<br/>
3384<br/>
No.6&#160;Mov.&#160;I<br/>
5<br/>
145<br/>
4180<br/>
No.6&#160;Mov.&#160;II<br/>
6<br/>
78<br/>
2754<br/>
Total<br/>
97<br/>
1629<br/>
35997<br/>
Table&#160;4.4:&#160;Phrases&#160;and&#160;notes&#160;count&#160;for&#160;Clementi's&#160;Sonatina&#160;Op.36<br/>
<b>Title</b><br/>
<b>Phrases&#160;Count</b><br/>
<b>Notes&#160;Count</b><br/>
No.1&#160;Mov.&#160;I<br/>
12<br/>
222<br/>
No.1&#160;Mov.&#160;II<br/>
10<br/>
147<br/>
No.1&#160;Mov.&#160;III<br/>
16<br/>
261<br/>
No.2&#160;Mov.&#160;I<br/>
18<br/>
320<br/>
No.2&#160;Mov.&#160;II<br/>
6<br/>
125<br/>
No.2&#160;Mov.&#160;III<br/>
28<br/>
414<br/>
No.3&#160;Mov.&#160;I<br/>
25<br/>
526<br/>
No.3&#160;Mov.&#160;II<br/>
6<br/>
74<br/>
No.3&#160;Mov.&#160;III<br/>
19<br/>
438<br/>
No.4&#160;Mov.&#160;I<br/>
25<br/>
465<br/>
No.4&#160;Mov.&#160;II<br/>
12<br/>
222<br/>
No.4&#160;Mov.&#160;III<br/>
16<br/>
384<br/>
No.5&#160;Mov.&#160;I<br/>
17<br/>
672<br/>
No.5&#160;Mov.&#160;II<br/>
13<br/>
316<br/>
No.5&#160;Mov.&#160;III<br/>
24<br/>
564<br/>
No.6&#160;Mov.&#160;I<br/>
28<br/>
836<br/>
No.6&#160;Mov.&#160;II<br/>
11<br/>
459<br/>
<b>Total</b><br/>
286<br/>
6445<br/>
34<br/>
<hr>
<A name=47></a><IMG src="thesis-47_1.jpg"/><br/>
&#160;5<br/>
distribution<br/>
&#160;4<br/>
&#160;3<br/>
&#160;2<br/>
Number of Movements<br/>
&#160;1<br/>
&#160;0<br/>
&#160;0<br/>
&#160;5<br/>
&#160;10<br/>
&#160;15<br/>
&#160;20<br/>
&#160;25<br/>
&#160;30<br/>
Movement Length (phrases)<br/>
Figure&#160;4.2:&#160;Movement&#160;length&#160;(phrases)&#160;distribution<br/>
&#160;70<br/>
distribution<br/>
&#160;60<br/>
&#160;50<br/>
&#160;40<br/>
&#160;30<br/>
&#160;20<br/>
Number of Phrases<br/>
&#160;10<br/>
&#160;0<br/>
&#160;0<br/>
&#160;20<br/>
&#160;40<br/>
&#160;60<br/>
&#160;80<br/>
&#160;100<br/>
&#160;120<br/>
Phrase Length (notes)<br/>
Figure&#160;4.3:&#160;Phrase&#160;length&#160;(notes)&#160;distribution<br/>
35<br/>
<hr>
<A name=48></a><IMG src="thesis-48_1.jpg"/><br/>
<b>Chapter&#160;5</b><br/>
<b>Experiments</b><br/>
In&#160;this&#160;chapter,&#160;we&#160;will&#160;show&#160;some&#160;experiment&#160;results&#160;to&#160;proof&#160;the&#160;effectiveness&#160;of<br/>
our&#160;method.&#160;Section&#160;<a href="thesiss.html#48">5.1&#160;</a>deals&#160;with&#160;the&#160;onset&#160;deviation&#160;problem&#160;highlighted&#160;in&#160;Section<br/>
<a href="thesiss.html#36">3.5.3</a>.&#160;Section&#160;<a href="thesiss.html#52">5.2&#160;</a>discusses&#160;how&#160;various&#160;parameters&#160;in&#160;our&#160;system&#160;are&#160;chosen.&#160;Section<br/>
<a href="thesiss.html#56">5.3&#160;</a>describes&#160;a&#160;subjective&#160;test&#160;to&#160;test&#160;if&#160;audience&#160;can&#160;or&#160;can't&#160;identify&#160;the&#160;difference&#160;between<br/>
generated&#160;and&#160;human&#160;performances.<br/>
<b>5.1</b><br/>
<b>Onset&#160;Deviation&#160;Normalization</b><br/>
As&#160;mentioned&#160;in&#160;Section&#160;<a href="thesiss.html#36">3.5.3,&#160;</a>a&#160;bad&#160;normalization&#160;method&#160;will&#160;usually&#160;result&#160;in&#160;un-<br/>
reasonable&#160;high&#160;onset&#160;deviation.&#160;To&#160;overcome&#160;this&#160;challenge,&#160;we&#160;proposed&#160;a&#160;automated<br/>
way&#160;to&#160;select&#160;the&#160;normalization&#160;.&#160;In&#160;this&#160;section,&#160;we&#160;will&#160;evaluate&#160;the&#160;effectiveness&#160;of&#160;the<br/>
method.<br/>
We&#160;extract&#160;the&#160;onset&#160;deviation&#160;feature&#160;from&#160;performer&#160;E's&#160;<a href="">recording1</a>,&#160;using&#160;the&#160;two<br/>
types&#160;of&#160;fixed&#160;normalization&#160;method&#160;and&#160;also&#160;the&#160;automatic&#160;normalization&#160;method&#160;men-<br/>
tioned&#160;in&#160;Section&#160;<a href="thesiss.html#36">3.5.3</a>.&#160;The&#160;onset&#160;deviations&#160;extracted&#160;by&#160;each&#160;method&#160;are&#160;shown&#160;in&#160;Fig.<br/>
<a href="thesiss.html#49">5.1,&#160;</a>Fig.&#160;<a href="thesiss.html#50">5.2&#160;</a>and&#160;Fig.&#160;<a href="thesiss.html#51">5.3.&#160;</a>Each&#160;dotted&#160;line&#160;from&#160;left&#160;to&#160;right&#160;represents&#160;a&#160;phrase&#160;in&#160;the<br/>
corpus.&#160;Each&#160;dot&#160;represents&#160;the&#160;onset&#160;deviation&#160;value&#160;of&#160;a&#160;note.&#160;The&#160;notes&#160;are&#160;spread&#160;uni-<br/>
formly&#160;on&#160;the&#160;horizontal&#160;axis,&#160;which&#160;only&#160;shows&#160;the&#160;order&#160;of&#160;appearance,&#160;instead&#160;of&#160;the<br/>
1The&#160;effect&#160;of&#160;this&#160;method&#160;is&#160;less&#160;obvious&#160;for&#160;performer&#160;with&#160;better&#160;piano&#160;skill,&#160;because&#160;they&#160;have&#160;better<br/>
control&#160;over&#160;tempo&#160;stability.<br/>
36<br/>
<hr>
<A name=49></a><IMG src="thesis-49_1.jpg"/><br/>
1.0<br/>
0.8<br/>
0.6<br/>
0.4<br/>
0.2<br/>
0.0<br/>
viation&#160;(quarter&#160;note)<br/>t&#160;De&#160;−0.2<br/>
Onse<br/>
−0.4<br/>
−0.6<br/>
−0.&#160;08<br/>
20<br/>
40<br/>
60<br/>
80<br/>
100<br/>
120<br/>
Note&#160;Number<br/>
Figure&#160;5.1:&#160;Onset&#160;deviations&#160;by&#160;aligning&#160;last&#160;note&#160;onset<br/>
real&#160;time&#160;scale.&#160;First,&#160;we&#160;can&#160;see&#160;in&#160;Fig.&#160;<a href="thesiss.html#50">5.2&#160;</a>that&#160;by&#160;aligning&#160;the&#160;note-off&#160;events&#160;of&#160;the<br/>
last&#160;notes&#160;results&#160;in&#160;very&#160;large&#160;deviations&#160;in&#160;some&#160;phrases.&#160;This&#160;is&#160;because&#160;extending&#160;the<br/>
last&#160;note&#160;in&#160;certain&#160;phrases&#160;to&#160;emphasize&#160;the&#160;ending&#160;is&#160;a&#160;common&#160;expression.&#160;This&#160;kind&#160;of<br/>
extension&#160;will&#160;cause&#160;the&#160;last&#160;notes&#160;onset&#160;in&#160;the&#160;performance&#160;to&#160;be&#160;far&#160;apart&#160;from&#160;the&#160;score.<br/>
Fig.&#160;<a href="thesiss.html#49">5.1&#160;</a>and&#160;Fig.&#160;<a href="thesiss.html#51">5.3&#160;</a>seemed&#160;to&#160;work&#160;better.Although&#160;they&#160;look&#160;similar,&#160;but&#160;the&#160;onset<br/>
deviation&#160;values&#160;in&#160;Fig.&#160;<a href="thesiss.html#49">5.1&#160;</a>is&#160;more&#160;dramatic&#160;than&#160;those&#160;in&#160;Fig.&#160;<a href="thesiss.html#51">5.3,&#160;</a>which&#160;proofs&#160;that<br/>
the&#160;automatic&#160;normalization&#160;method&#160;can&#160;generally&#160;reduce&#160;the&#160;onset&#160;deviations.&#160;Another<br/>
benefit&#160;of&#160;the&#160;automated&#160;normalization&#160;method&#160;over&#160;aligning&#160;last&#160;notes&#160;onset&#160;method&#160;is<br/>
that&#160;the&#160;last&#160;notes&#160;are&#160;not&#160;force&#160;aligned,&#160;which&#160;allows&#160;more&#160;space&#160;for&#160;free&#160;expression&#160;for<br/>
the&#160;last&#160;note.&#160;This&#160;effect&#160;can&#160;be&#160;seen&#160;in&#160;Fig.&#160;<a href="thesiss.html#49">5.1</a>,&#160;in&#160;which&#160;the&#160;right-most&#160;end&#160;of&#160;a&#160;line,<br/>
i.e.&#160;the&#160;last&#160;note,&#160;always&#160;goes&#160;back&#160;to&#160;zero,&#160;while&#160;in&#160;Fig.&#160;<a href="thesiss.html#51">5.3</a>,&#160;the&#160;end&#160;of&#160;a&#160;line&#160;can&#160;end&#160;in<br/>
different&#160;values<br/>
37<br/>
<hr>
<A name=50></a><IMG src="thesis-50_1.jpg"/><br/>
3.5<br/>
3.0<br/>
2.5<br/>
2.0<br/>
1.5<br/>
1.0<br/>
viation&#160;(quarter&#160;note)<br/>t&#160;De&#160;0.5<br/>
Onse<br/>
0.0<br/>
−0.5<br/>
−1.&#160;00<br/>
20<br/>
40<br/>
60<br/>
80<br/>
100<br/>
120<br/>
Note&#160;Number<br/>
Figure&#160;5.2:&#160;Onset&#160;deviations&#160;by&#160;aligning&#160;last&#160;notes&#160;note-off<br/>
38<br/>
<hr>
<A name=51></a><IMG src="thesis-51_1.jpg"/><br/>
1.0<br/>
0.8<br/>
0.6<br/>
0.4<br/>
0.2<br/>
viation&#160;(quarter&#160;note)<br/>t&#160;De&#160;0.0<br/>Onse<br/>
−0.2<br/>
−0.4<br/>
−0.&#160;06<br/>
20<br/>
40<br/>
60<br/>
80<br/>
100<br/>
120<br/>
Note&#160;Number<br/>
Figure&#160;5.3:&#160;Onset&#160;deviations&#160;using&#160;automated&#160;normalization&#160;method<br/>
39<br/>
<hr>
<A name=52></a><IMG src="thesis-52_1.jpg"/><br/>
<b>5.2</b><br/>
<b>Parameter&#160;Selection</b><br/>
<b>5.2.1</b><br/>
<b>SVM-HMM-related&#160;Parameters</b><br/>
There&#160;are&#160;many&#160;parameters&#160;which&#160;need&#160;adjustment&#160;in&#160;SVM-HMM.&#160;Two&#160;most&#160;impor-<br/>
tant&#160;parameters,&#160;the&#160;termination&#160;accuracy&#160;<i>ε&#160;</i>and&#160;the&#160;misclassification&#160;penalty&#160;factor&#160;C&#160;in<br/>
SVM,&#160;are&#160;systematically&#160;tested&#160;in&#160;this&#160;experiment&#160;to&#160;find&#160;the&#160;optimal&#160;value.&#160;Since&#160;SVM-<br/>
HMM&#160;is&#160;an&#160;iterative&#160;algorithm,&#160;the&#160;<i>ε&#160;</i>parameter&#160;defines&#160;the&#160;required&#160;accuracy&#160;for&#160;the&#160;algo-<br/>
rithm&#160;to&#160;terminate.&#160;A&#160;smaller&#160;<i>ε&#160;</i>will&#160;result&#160;in&#160;higher&#160;accuracy,&#160;but&#160;may&#160;take&#160;more&#160;iterations<br/>
to&#160;compute.&#160;The&#160;C&#160;parameter&#160;determines&#160;how&#160;much&#160;weight&#160;should&#160;be&#160;assigned&#160;to&#160;penalise<br/>
non-separable&#160;samples.&#160;A&#160;larger&#160;C&#160;will&#160;sacrifice&#160;larger&#160;margin&#160;for&#160;lower&#160;misclassification<br/>
error,&#160;but&#160;it&#160;will&#160;make&#160;the&#160;execution&#160;time&#160;longer.<br/>
We&#160;split&#160;performer&#160;A's&#160;recordings&#160;into&#160;two&#160;sets:&#160;the&#160;training&#160;set&#160;includes&#160;pieces&#160;No.2<br/>
to&#160;No.6,&#160;and&#160;the&#160;testing&#160;set&#160;includes&#160;piece&#160;No.1.&#160;We&#160;train&#160;a&#160;model&#160;with&#160;the&#160;training&#160;set,<br/>
and&#160;use&#160;the&#160;learned&#160;model&#160;to&#160;generate&#160;the&#160;testing&#160;set.&#160;The&#160;generated&#160;expressive&#160;perfor-<br/>
mance&#160;is&#160;compared&#160;to&#160;the&#160;corresponding&#160;human&#160;recordings&#160;to&#160;calculate&#160;the&#160;accuracy&#160;of<br/>
the&#160;prediction.<br/>
Ideally,&#160;the&#160;generated&#160;performance&#160;will&#160;be&#160;very&#160;similar&#160;in&#160;expression&#160;to&#160;the&#160;recording.<br/>
In&#160;order&#160;to&#160;choose&#160;the&#160;best&#160;<i>ε</i>,&#160;we&#160;calculate&#160;the&#160;median&#160;of&#160;similarities&#160;between&#160;the&#160;generated<br/>
and&#160;recorded&#160;performances&#160;for&#160;each&#160;<i>ε&#160;</i>choice.&#160;Note&#160;that&#160;each&#160;performance&#160;feature&#160;has&#160;its<br/>
own&#160;model,&#160;so&#160;we&#160;will&#160;be&#160;looking&#160;at&#160;one&#160;performance&#160;feature&#160;and&#160;its&#160;<i>ε&#160;</i>parameter&#160;at&#160;a&#160;time.<br/>
First,&#160;the&#160;generated&#160;performance&#160;feature&#160;sequence&#160;and&#160;the&#160;recorded&#160;one&#160;are&#160;normalized&#160;to<br/>
a&#160;range&#160;from&#160;0&#160;to&#160;1.&#160;This&#160;is&#160;because&#160;the&#160;generated&#160;performance&#160;may&#160;have&#160;the&#160;same&#160;up-<br/>
and-downs&#160;as&#160;the&#160;score,&#160;but&#160;the&#160;value&#160;range&#160;may&#160;be&#160;different,&#160;so&#160;we&#160;use&#160;normalization&#160;to<br/>
ease&#160;our&#160;these&#160;difference.&#160;The&#160;Euclidean&#160;distance&#160;between&#160;the&#160;two&#160;normalized&#160;sequences<br/>
is&#160;calculated&#160;and&#160;divided&#160;by&#160;the&#160;length&#160;(number&#160;of&#160;notes)&#160;of&#160;the&#160;phrase,&#160;since&#160;the&#160;phrase<br/>
can&#160;have&#160;arbitrary&#160;length.&#160;Similar&#160;procedure&#160;is&#160;applied&#160;to&#160;find&#160;the&#160;best&#160;C.<br/>
First&#160;we&#160;fixed&#160;C&#160;at&#160;0<i>.</i>1&#160;and&#160;tried&#160;different&#160;<i>ε</i>'s:&#160;100,&#160;10,&#160;1,&#160;0<i>.</i>75,&#160;0<i>.</i>5&#160;and&#160;0<i>.</i>1.&#160;Then,<br/>
we&#160;fix&#160;<i>ε&#160;</i>at&#160;the&#160;optimal&#160;value&#160;determined&#160;in&#160;the&#160;previous&#160;step&#160;and&#160;test&#160;different&#160;C's:&#160;10−3,<br/>
10−2,&#160;10−1,&#160;0<i>.</i>5,&#160;1,&#160;and&#160;5.&#160;For&#160;each&#160;<i>ε&#160;</i>and&#160;C&#160;combination,&#160;we&#160;calculate&#160;the&#160;distance&#160;be-<br/>
40<br/>
<hr>
<A name=53></a><IMG src="thesis-53_1.jpg"/><br/>
&#160;0.12<br/>
Relative Duration<br/>
&#160;0.115<br/>
Onset Deviation<br/>
MIDI Velocity<br/>
&#160;0.11<br/>
&#160;0.105<br/>
&#160;0.1<br/>
&#160;0.095<br/>
Prediction Error<br/>
&#160;0.09<br/>
&#160;0.085<br/>
&#160;0.08<br/>
&#160;0.1&#160;&#160;0.2&#160;&#160;0.3&#160;&#160;0.4&#160;&#160;0.5&#160;&#160;0.6&#160;&#160;0.7&#160;&#160;0.8&#160;&#160;0.9<br/>
&#160;1<br/>
Epsilon<br/>
Figure&#160;5.4:&#160;Median&#160;distance&#160;between&#160;generated&#160;performances&#160;and&#160;recordings&#160;for&#160;different<br/><i>ε</i>'s<br/>
tween&#160;the&#160;generated&#160;pieces&#160;and&#160;recorded&#160;examples&#160;for&#160;all&#160;phrases&#160;in&#160;the&#160;testing&#160;set&#160;for&#160;each<br/>
performer.&#160;Then&#160;we&#160;take&#160;the&#160;median&#160;of&#160;all&#160;these&#160;distances&#160;for&#160;each&#160;<i>ε&#160;</i>or&#160;C.&#160;The&#160;optimal&#160;<i>ε</i><br/>
or&#160;C&#160;is&#160;the&#160;one&#160;that&#160;minimize&#160;the&#160;median&#160;of&#160;the&#160;distances.<br/>
The&#160;median&#160;distance&#160;of&#160;the&#160;generated&#160;performance&#160;from&#160;the&#160;recording&#160;for&#160;various&#160;<i>ε</i>'s<br/>
are&#160;shown&#160;in&#160;Fig.&#160;<a href="thesiss.html#53">5.4</a>.&#160;The&#160;execution&#160;time&#160;for&#160;various&#160;<i>ε</i>'s&#160;are&#160;shown&#160;in&#160;Fig.&#160;<a href="thesiss.html#54">5.5</a>.&#160;For&#160;<i>ε</i><br/>
value&#160;100&#160;and&#160;10,&#160;the&#160;termination&#160;criteria&#160;is&#160;too&#160;generous&#160;so&#160;SVM-HMM&#160;terminates&#160;almost<br/>
immediately&#160;without&#160;actually&#160;learned&#160;anything.&#160;Therefore,&#160;the&#160;outputs&#160;are&#160;a&#160;fixed&#160;value<br/>
for&#160;any&#160;input.&#160;We&#160;abandon&#160;the&#160;data&#160;points&#160;for&#160;<i>ε&#160;</i>=&#160;100&#160;or&#160;10.&#160;We&#160;can&#160;see&#160;that&#160;the&#160;distance<br/>
drops&#160;slowly&#160;when&#160;<i>ε&#160;</i>becomes&#160;smaller.&#160;We&#160;choose&#160;<i>ε&#160;</i>=&#160;0<i>.</i>1&#160;for&#160;the&#160;best&#160;accuracy-time<br/>
tradeoff.<br/>
As&#160;for&#160;different&#160;C&#160;parameter,&#160;the&#160;accuracy&#160;and&#160;execution&#160;time&#160;are&#160;shown&#160;in&#160;Fig.&#160;<a href="thesiss.html#55">5.6</a><br/>
and&#160;Fig.&#160;<a href="thesiss.html#55">5.7</a>,&#160;respectively.&#160;We&#160;can't&#160;find&#160;a&#160;clear&#160;trend&#160;in&#160;Fig.&#160;<a href="thesiss.html#55">5.6,&#160;</a>but&#160;we&#160;can&#160;find&#160;that<br/>
for&#160;C&#160;over&#160;10&#160;and&#160;under&#160;0<i>.</i>01,&#160;the&#160;model&#160;failed&#160;to&#160;produce&#160;meaning&#160;fule&#160;model&#160;(i.e.&#160;the<br/>
output&#160;is&#160;a&#160;fixed&#160;value),&#160;so&#160;the&#160;data&#160;point&#160;is&#160;omitted&#160;in&#160;the&#160;figure.&#160;Therefore,&#160;choosing&#160;a<br/>
41<br/>
<hr>
<A name=54></a><IMG src="thesis-54_1.jpg"/><br/>
&#160;30<br/>
Relative Duration<br/>
Onset Deviation<br/>
&#160;25<br/>
MIDI Velocity<br/>
&#160;20<br/>
&#160;15<br/>
&#160;10<br/>
&#160;5<br/>
Execution Time (CPU seconds)<br/>
&#160;0<br/>
&#160;0.1<br/>
&#160;1<br/>
&#160;10<br/>
&#160;100<br/>
Epsilon<br/>
Figure&#160;5.5:&#160;Execution&#160;time&#160;for&#160;different&#160;<i>ε</i>'s<br/>
C&#160;in&#160;the&#160;middle&#160;will&#160;produce&#160;more&#160;robust&#160;model.&#160;In&#160;Fig.&#160;<a href="thesiss.html#55">5.7&#160;</a>the&#160;execution&#160;time&#160;grows&#160;as<br/>
C&#160;goes&#160;larger,&#160;so&#160;considereing&#160;the&#160;robustness&#160;(always&#160;producing&#160;meaningful&#160;model)&#160;and<br/>
time&#160;tradeoff,&#160;we&#160;choose&#160;C&#160;=&#160;0.1&#160;as&#160;our&#160;optimal&#160;C.<br/>
<b>5.2.2</b><br/>
<b>Quantization&#160;Parameter</b><br/>
Besides&#160;<i>ε&#160;</i>and&#160;C,&#160;the&#160;number&#160;of&#160;quantization&#160;levels&#160;for&#160;SVM-HMM&#160;input&#160;is&#160;also&#160;has<br/>
some&#160;impact&#160;on&#160;the&#160;execution&#160;time.&#160;If&#160;the&#160;performance&#160;features&#160;are&#160;quantized&#160;into&#160;more<br/>
fine-grained&#160;levels,&#160;the&#160;quantization&#160;errors&#160;can&#160;be&#160;reduced,&#160;but&#160;the&#160;execution&#160;time&#160;and<br/>
memory&#160;usage&#160;will&#160;grow&#160;dramatically.&#160;Also,&#160;larger&#160;number&#160;of&#160;intervals&#160;doesn't&#160;imply<br/>
more&#160;accurate&#160;or&#160;robust&#160;model.&#160;Because&#160;SVM-HMM&#160;is&#160;originally&#160;used&#160;in&#160;part-of-speech<br/>
tagging&#160;problem,&#160;if&#160;we&#160;use&#160;divide&#160;the&#160;performance&#160;features&#160;into&#160;more&#160;intervals,&#160;there&#160;will<br/>
be&#160;fewer&#160;samples&#160;in&#160;each&#160;interval.&#160;But&#160;from&#160;a&#160;statistical&#160;learning&#160;point&#160;of&#160;view,&#160;it&#160;is&#160;de-<br/>
sirable&#160;to&#160;have&#160;fewer&#160;bins&#160;with&#160;more&#160;samples&#160;in&#160;each,&#160;rather&#160;than&#160;a&#160;large&#160;number&#160;of&#160;bins<br/>
with&#160;very&#160;sparse&#160;samples&#160;in&#160;each.&#160;To&#160;illustrate&#160;this&#160;point,&#160;consider&#160;a&#160;three&#160;note&#160;segment&#160;is<br/>
42<br/>
<hr>
<A name=55></a><IMG src="thesis-55_1.jpg"/><br/>
&#160;0.14<br/>
Relative Duration<br/>
Onset Deviation<br/>
&#160;0.13<br/>
MIDI Velocity<br/>
&#160;0.12<br/>
&#160;0.11<br/>
&#160;0.1<br/>
Prediction Error<br/>
&#160;0.09<br/>
&#160;0.08<br/>
&#160;0.001<br/>
&#160;0.01<br/>
&#160;0.1<br/>
&#160;1<br/>
&#160;10<br/>
&#160;100<br/>
C<br/>
Figure&#160;5.6:&#160;Median&#160;distance&#160;between&#160;generated&#160;performances&#160;and&#160;recordings&#160;for&#160;different<br/>C's<br/>
&#160;10000<br/>
Relative Duration<br/>
Onset Deviation<br/>
MIDI Velocity<br/>
&#160;1000<br/>
&#160;100<br/>
Execution Time (CPU seconds)<br/>
&#160;10<br/>
&#160;0.001<br/>
&#160;0.01<br/>
&#160;0.1<br/>
&#160;1<br/>
&#160;10<br/>
C<br/>
Figure&#160;5.7:&#160;Execution&#160;time&#160;for&#160;different&#160;C's<br/>
43<br/>
<hr>
<A name=56></a><IMG src="thesis-56_1.jpg"/><br/>
played&#160;once&#160;in&#160;the&#160;following&#160;MIDI&#160;velocity:&#160;(60,&#160;70,&#160;80),&#160;and&#160;the&#160;same&#160;segment&#160;is&#160;played<br/>
again&#160;in&#160;(60.1,&#160;69.9,&#160;80.1).&#160;If&#160;we&#160;have&#160;a&#160;quantization&#160;interval&#160;width&#160;of,&#160;say,&#160;0.05,&#160;then<br/>
60&#160;and&#160;60.1&#160;may&#160;be&#160;quantized&#160;into&#160;different&#160;bins,&#160;and&#160;70&#160;and&#160;69.9&#160;may&#160;also&#160;be&#160;quantized<br/>
into&#160;different&#160;bins,&#160;so&#160;the&#160;two&#160;phrases&#160;will&#160;be&#160;considered&#160;as&#160;two&#160;different&#160;case.&#160;However,<br/>
if&#160;the&#160;quantization&#160;interval&#160;width&#160;is&#160;1,&#160;both&#160;phrases&#160;may&#160;be&#160;quantized&#160;into&#160;the&#160;same&#160;la-<br/>
bel&#160;sequence,&#160;which&#160;is&#160;more&#160;desirable&#160;because&#160;the&#160;SVM-HMM&#160;algorithm&#160;can&#160;capture&#160;the<br/>
similarity&#160;in&#160;the&#160;two&#160;samples.<br/>
Initially,&#160;we&#160;tried&#160;to&#160;quantized&#160;the&#160;values&#160;into&#160;1025&#160;uniform&#160;width&#160;bins,&#160;wishing&#160;to<br/>
minimize&#160;the&#160;quantization&#160;error.&#160;But&#160;it&#160;take&#160;very&#160;long&#160;(hours,&#160;even&#160;days)&#160;to&#160;learn&#160;a&#160;model,<br/>
and&#160;the&#160;output&#160;only&#160;falls&#160;on&#160;a&#160;very&#160;sparse&#160;set&#160;of&#160;values.&#160;So&#160;we&#160;reduce&#160;this&#160;number&#160;to<br/>
128.&#160;This&#160;level&#160;of&#160;quantization&#160;is&#160;fine&#160;enough&#160;to&#160;capture&#160;the&#160;performance&#160;nuance.&#160;Taking<br/>
a&#160;rough&#160;estimate,&#160;onset&#160;deviation&#160;feature&#160;rarely&#160;exceeds&#160;±1,&#160;so&#160;the&#160;quantization&#160;interval<br/>
width&#160;is&#160;around&#160;1−(−1)&#160;=&#160;0<i>.</i>015625.&#160;Most&#160;duration&#160;ratios&#160;falls&#160;between&#160;zero&#160;and&#160;three,<br/>
128<br/>
so&#160;the&#160;interval&#160;width&#160;is&#160;3−0&#160;=&#160;0<i>.</i>0234375.&#160;MIDI&#160;velocity&#160;is&#160;roughly&#160;around&#160;30&#160;to&#160;90,&#160;so<br/>
128<br/>
the&#160;interval&#160;is&#160;about&#160;90−30&#160;=&#160;0<i>.</i>46875.&#160;This&#160;level&#160;of&#160;granularity&#160;is&#160;good&#160;enough&#160;for&#160;our<br/>
128<br/>
performance&#160;system,&#160;and&#160;can&#160;dramatically&#160;reduce&#160;the&#160;execution&#160;time&#160;without&#160;sacrificing<br/>
the&#160;expressiveness&#160;of&#160;the&#160;models.<br/>
We&#160;repeated&#160;the&#160;<i>ε&#160;</i>selection&#160;experiment&#160;for&#160;quantization&#160;level&#160;of&#160;1025&#160;and&#160;128.&#160;The<br/>
execution&#160;time&#160;(in&#160;CPU&#160;second)&#160;is&#160;shown&#160;in&#160;Fig.&#160;<a href="thesiss.html#57">5.8</a>.&#160;The&#160;time&#160;required&#160;for&#160;1025&#160;is&#160;larger<br/>
than&#160;128&#160;by&#160;orders&#160;of&#160;magnitudes,&#160;but&#160;the&#160;expressiveness&#160;does&#160;not&#160;improve&#160;much.<br/>
<b>5.3</b><br/>
<b>Human-like&#160;Performance</b><br/>
The&#160;goal&#160;of&#160;our&#160;system&#160;is&#160;to&#160;create&#160;expressive,&#160;non-robotic&#160;music&#160;as&#160;oppose&#160;to&#160;deadpan<br/>
MIDI.&#160;Therefore,&#160;we&#160;need&#160;to&#160;perform&#160;a&#160;subjective&#160;test&#160;to&#160;verify&#160;if&#160;people&#160;can&#160;tell&#160;our<br/>
generated&#160;performances&#160;apart&#160;from&#160;real&#160;human&#160;performances.<br/>
In&#160;this&#160;survey,&#160;1518&#160;computer&#160;generated&#160;expressive&#160;phrases&#160;and&#160;their&#160;corresponding<br/>
human&#160;recording&#160;were&#160;selected&#160;as&#160;samples.&#160;Each&#160;test&#160;subject&#160;was&#160;given&#160;10&#160;randomly&#160;se-<br/>
lected&#160;computer&#160;generated&#160;phrase&#160;and&#160;10&#160;human&#160;recordings,&#160;these&#160;20&#160;phrases&#160;are&#160;presented<br/>
in&#160;random&#160;order.&#160;He/She&#160;was&#160;asked&#160;to&#160;rate&#160;each&#160;phrase&#160;according&#160;to&#160;the&#160;following&#160;criteria,<br/>
44<br/>
<hr>
<A name=57></a><IMG src="thesis-57_1.jpg"/><br/>
&#160;1e+06<br/>
128-Duration<br/>
128-Onset<br/>
128-Velocity<br/>
&#160;100000<br/>
1025-Duration<br/>
1025-Onset<br/>
1025-Velocity<br/>
&#160;10000<br/>
&#160;1000<br/>
&#160;100<br/>
&#160;10<br/>
Execution Time (CPU second)<br/>
&#160;1<br/>
&#160;0.1<br/>
0.1<br/>
0.25<br/>
0.5<br/>
0.75<br/>
1<br/>
10<br/>
100<br/>
Epsilon<br/>
Figure&#160;5.8:&#160;Execution&#160;time&#160;for&#160;differnt&#160;number&#160;of&#160;quantization&#160;levels<br/>
which&#160;were&#160;proposed&#160;by&#160;the&#160;RenCon&#160;contess&#160;[1]:<br/>
1.&#160;Technical&#160;control:&#160;if&#160;a&#160;performance&#160;sounds&#160;like&#160;it&#160;is&#160;technically&#160;skilled&#160;thus&#160;per-<br/>
formed&#160;with&#160;accurate&#160;and&#160;secure&#160;notes,&#160;rhythms,&#160;tempo&#160;and&#160;articulation.<br/>
2.&#160;Humanness:&#160;if&#160;the&#160;performance&#160;sounds&#160;like&#160;a&#160;human&#160;was&#160;playing&#160;it.<br/>
3.&#160;Musicality:&#160;how&#160;musical&#160;the&#160;performance&#160;is&#160;in&#160;terms&#160;of&#160;tone&#160;and&#160;color,&#160;phrasing,<br/>
flow,&#160;mood&#160;and&#160;emotions<br/>
4.&#160;Expressive&#160;variation:&#160;how&#160;much&#160;expressive&#160;variation&#160;(versus&#160;deadpan)&#160;there&#160;is&#160;in<br/>
the&#160;performance.<br/>
In&#160;RenCon,&#160;each&#160;judge&#160;was&#160;asked&#160;to&#160;give&#160;separate&#160;ratings&#160;for&#160;each&#160;criteria.&#160;But&#160;we&#160;be-<br/>
lieve&#160;this&#160;is&#160;too&#160;demanding&#160;for&#160;less-experienced&#160;participant,&#160;so&#160;we&#160;asked&#160;each&#160;test&#160;subject<br/>
to&#160;give&#160;an&#160;overall&#160;rating&#160;from&#160;one&#160;to&#160;five.&#160;One&#160;being&#160;very&#160;bad,&#160;five&#160;being&#160;very&#160;good.&#160;The<br/>
test&#160;subjects&#160;are&#160;also&#160;asked&#160;to&#160;report&#160;their&#160;musical&#160;proficiency&#160;in&#160;a&#160;three&#160;level&#160;scale:<br/>
1.&#160;No&#160;experience&#160;in&#160;music<br/>
45<br/>
<hr>
<A name=58></a><IMG src="thesis-58_1.jpg"/><br/>
&#160;25<br/>
full corpus<br/>
single performer<br/>
&#160;20<br/>
&#160;15<br/>
Percent<br/>
&#160;10<br/>
&#160;5<br/>
&#160;0<br/>
-0.4<br/>
-0.3<br/>
-0.2<br/>
-0.1<br/>
&#160;0<br/>
&#160;0.1<br/>
&#160;0.2<br/>
&#160;0.3<br/>
Onset Deviation<br/>
Figure&#160;5.9:&#160;Distribution&#160;of&#160;onset&#160;deviation&#160;values&#160;from&#160;full&#160;corpus&#160;versus&#160;single&#160;per-<br/>former's&#160;corpus<br/>
2.&#160;Amateur&#160;performer<br/>
3.&#160;Professional&#160;musician,&#160;musicologist&#160;or&#160;student&#160;majored&#160;in&#160;music<br/>
To&#160;generate&#160;the&#160;expressive&#160;performance&#160;phrase.&#160;We&#160;follow&#160;a&#160;six-fold&#160;cross-validation<br/>
pattern:&#160;for&#160;each&#160;performer&#160;in&#160;the&#160;corpus,&#160;we&#160;use&#160;all&#160;his/her&#160;recorded&#160;phrases&#160;of&#160;Clementi's<br/>
Op.36&#160;No.2&#160;to&#160;No.6&#160;to&#160;train&#160;a&#160;model.&#160;Then&#160;the&#160;model&#160;is&#160;used&#160;to&#160;generate&#160;all&#160;phrases&#160;from<br/>
Clementi's&#160;Op.36&#160;No.1.&#160;The&#160;generate&#160;phrases&#160;and&#160;the&#160;performer's&#160;recordings&#160;of&#160;piece&#160;No.<br/>
1&#160;will&#160;all&#160;be&#160;included&#160;as&#160;samples.&#160;The&#160;process&#160;is&#160;repeated,&#160;but&#160;each&#160;time&#160;the&#160;piece&#160;excluded<br/>
for&#160;training&#160;will&#160;be&#160;changed&#160;to&#160;No.2,&#160;No.3&#160;and&#160;so&#160;on.&#160;So&#160;all&#160;six&#160;pieces&#160;will&#160;have&#160;a&#160;computer<br/>
generated&#160;version&#160;(trained&#160;by&#160;each&#160;player's&#160;corpus)&#160;and&#160;a&#160;recorded&#160;version.<br/>
We&#160;have&#160;also&#160;tried&#160;using&#160;all&#160;performers'&#160;recordings&#160;to&#160;train&#160;a&#160;single&#160;model.&#160;However,<br/>
the&#160;expressive&#160;variation&#160;from&#160;that&#160;model&#160;is&#160;much&#160;smaller&#160;than&#160;a&#160;model&#160;trained&#160;by&#160;a&#160;single<br/>
performer's&#160;recordings.&#160;This&#160;is&#160;because&#160;expression&#160;from&#160;different&#160;performers&#160;may&#160;cancel<br/>
each&#160;other&#160;out.&#160;This&#160;phenomena&#160;can&#160;be&#160;found&#160;in&#160;the&#160;distribution&#160;histograms&#160;for&#160;each<br/>
performance&#160;features&#160;(Fig.&#160;<a href="thesiss.html#58">5.9</a>,&#160;Fig.&#160;<a href="thesiss.html#59">5.10&#160;</a>and&#160;Fig.&#160;<a href="thesiss.html#59">5.11).&#160;</a>The&#160;features&#160;generated&#160;from&#160;the<br/>
full&#160;corpus&#160;are&#160;slightly&#160;more&#160;concentrated,&#160;which&#160;results&#160;in&#160;less&#160;dramatic&#160;expression.<br/>
We&#160;received&#160;119&#160;valid&#160;samples&#160;for&#160;the&#160;survey.&#160;Fifty&#160;of&#160;them&#160;are&#160;from&#160;people&#160;with&#160;no<br/>
46<br/>
<hr>
<A name=59></a><IMG src="thesis-59_1.jpg"/><br/>
&#160;30<br/>
full corpus<br/>
single performer<br/>
&#160;25<br/>
&#160;20<br/>
&#160;15<br/>
Percent<br/>
&#160;10<br/>
&#160;5<br/>
&#160;0<br/>
&#160;0<br/>
&#160;0.5<br/>
&#160;1<br/>
&#160;1.5<br/>
&#160;2<br/>
&#160;2.5<br/>
&#160;3<br/>
Duration Ratio<br/>
Figure&#160;5.10:&#160;Distribution&#160;of&#160;duration&#160;ratio&#160;values&#160;from&#160;full&#160;corpus&#160;versus&#160;single&#160;per-<br/>former's&#160;Corpus<br/>
&#160;40<br/>
full corpus<br/>
&#160;35<br/>
single performer<br/>
&#160;30<br/>
&#160;25<br/>
&#160;20<br/>
Percent<br/>
&#160;15<br/>
&#160;10<br/>
&#160;5<br/>
&#160;0<br/>
&#160;10<br/>
&#160;20<br/>
&#160;30<br/>
&#160;40<br/>
&#160;50<br/>
&#160;60<br/>
&#160;70<br/>
&#160;80<br/>
&#160;90<br/>
&#160;100<br/>
MIDI Velocity<br/>
Figure&#160;5.11:&#160;Distribution&#160;of&#160;MIDI&#160;velocity&#160;values&#160;from&#160;full&#160;corpus&#160;versus&#160;single&#160;per-<br/>former's&#160;corpus<br/>
47<br/>
<hr>
<A name=60></a><IMG src="thesis-60_1.jpg"/><br/>
Table&#160;5.1:&#160;Average&#160;rating&#160;for&#160;generated&#160;performance&#160;and&#160;human&#160;recording<br/>
Computer<br/>
Human<br/>
No&#160;experience<br/>
3.243<br/>
3.391<br/>
Amateur<br/>
2.798<br/>
3.289<br/>
Professional<br/>
2.430<br/>
3.010<br/>
Total<br/>
2.952<br/>
3.306<br/>
music&#160;background,&#160;59&#160;are&#160;from&#160;amateur&#160;musicians,&#160;and&#160;the&#160;rest&#160;10&#160;are&#160;from&#160;professional<br/>
musicians.&#160;The&#160;average&#160;rating&#160;given&#160;to&#160;computer&#160;generated&#160;performances&#160;and&#160;human<br/>
recordings&#160;are&#160;listed&#160;in&#160;Table&#160;<a href="thesiss.html#60">5.1.&#160;</a>It&#160;is&#160;clear&#160;that&#160;for&#160;professional&#160;and&#160;amateur&#160;musician,<br/>
the&#160;average&#160;rating&#160;given&#160;to&#160;human&#160;performances&#160;are&#160;higher&#160;than&#160;computer&#160;performances.<br/>
However,&#160;for&#160;participants&#160;who&#160;have&#160;no&#160;experience&#160;in&#160;music,&#160;the&#160;ratings&#160;are&#160;much&#160;closer.&#160;A<br/>
Student&#160;T-test&#160;on&#160;the&#160;two&#160;ratings&#160;given&#160;by&#160;participants&#160;with&#160;no&#160;experience&#160;yields&#160;a&#160;p-value<br/>
of&#160;0.0312,&#160;therefore&#160;we&#160;can't&#160;reject&#160;the&#160;null&#160;hypothesis&#160;that&#160;the&#160;two&#160;ratings&#160;are&#160;different<br/>
under&#160;a&#160;significance&#160;level&#160;of&#160;99%.&#160;Therefore&#160;we&#160;can&#160;say&#160;for&#160;participants&#160;with&#160;no&#160;music<br/>
experience,&#160;the&#160;computer&#160;generated&#160;music&#160;and&#160;human&#160;recordings&#160;are&#160;indistinguishable.<br/>
In&#160;order&#160;to&#160;get&#160;more&#160;insight&#160;from&#160;the&#160;ratings,&#160;we&#160;can&#160;further&#160;divide&#160;the&#160;performers&#160;in<br/>
the&#160;corpus&#160;into&#160;two&#160;categories&#160;by&#160;their&#160;piano&#160;skill&#160;level.&#160;By&#160;the&#160;number&#160;of&#160;mistakes&#160;made<br/>
(Table&#160;<a href="thesiss.html#44">4.2),&#160;</a>performer&#160;A&#160;and&#160;B&#160;are&#160;considered&#160;more&#160;skillful&#160;than&#160;performer&#160;C,&#160;D,&#160;E&#160;and<br/>
F.&#160;The&#160;average&#160;ratings&#160;given&#160;to&#160;the&#160;performances&#160;generated&#160;from&#160;the&#160;model&#160;trained&#160;by<br/>
samples&#160;of&#160;the&#160;two&#160;categories&#160;are&#160;listed&#160;in&#160;Table&#160;<a href="thesiss.html#61">5.2.&#160;</a>The&#160;distance&#160;between&#160;computer&#160;and<br/>
human&#160;performances&#160;are&#160;smaller&#160;for&#160;less-skillful&#160;group&#160;(C&#160;to&#160;F)&#160;than&#160;the&#160;skillful&#160;group<br/>
(A&#160;and&#160;B).&#160;This&#160;is&#160;probably&#160;because&#160;our&#160;system&#160;makes&#160;some&#160;mistakes&#160;that&#160;are&#160;similar<br/>
to&#160;the&#160;mistakes&#160;made&#160;by&#160;less-skillful&#160;performers.&#160;For&#160;example,&#160;unsteady&#160;tempo,&#160;sudden<br/>
change&#160;in&#160;loudness,&#160;hesitation&#160;are&#160;all&#160;common&#160;problems&#160;that&#160;exists&#160;in&#160;both&#160;less-skillful<br/>
performance&#160;and&#160;computer&#160;generated&#160;performance.&#160;But&#160;for&#160;skillful&#160;performers,&#160;who&#160;have<br/>
better&#160;technical&#160;control&#160;and&#160;have&#160;better&#160;sense&#160;of&#160;musical&#160;structure,&#160;the&#160;problems&#160;described<br/>
above&#160;will&#160;happen&#160;less&#160;often.&#160;This&#160;will&#160;make&#160;the&#160;generated&#160;works&#160;sound&#160;much&#160;worse<br/>
comparing&#160;to&#160;the&#160;better&#160;performance.<br/>
If&#160;we&#160;look&#160;into&#160;each&#160;individual&#160;participant,&#160;we&#160;can&#160;check&#160;if&#160;a&#160;participant&#160;gives&#160;higher<br/>
(average)&#160;rating&#160;to&#160;computer&#160;or&#160;human&#160;performances,&#160;or&#160;equal&#160;ratings&#160;for&#160;both.&#160;The&#160;num-<br/>
48<br/>
<hr>
<A name=61></a><IMG src="thesis-61_1.jpg"/><br/>
Table&#160;5.2:&#160;Average&#160;rating&#160;for&#160;generated&#160;performance&#160;and&#160;human&#160;recording&#160;under&#160;different<br/>part&#160;of&#160;the&#160;corpus<br/>
A,B<br/>
C-F<br/>
Computer<br/>
Human<br/>
Computer<br/>
Human<br/>
No&#160;experience<br/>
3.067<br/>
3.302<br/>
3.363<br/>
3.451<br/>
Amateur<br/>
2.680<br/>
3.347<br/>
2.863<br/>
3.286<br/>
Professional<br/>
2.048<br/>
3.162<br/>
2.708<br/>
2.921<br/>
Total<br/>
2.776<br/>
3.313<br/>
3.066<br/>
3.323<br/>
Table&#160;5.3:&#160;Number&#160;of&#160;participants&#160;who&#160;gives&#160;higher&#160;rating&#160;to&#160;generated&#160;performance,&#160;hu-<br/>man&#160;recordings&#160;or&#160;equal&#160;rating<br/>
Computer<br/>
Equal<br/>
Human<br/>
Total<br/>
No&#160;experience<br/>
19<br/>
7<br/>
24<br/>
50<br/>
Amateur<br/>
7<br/>
3<br/>
49<br/>
59<br/>
Professional<br/>
1<br/>
1<br/>
8<br/>
10<br/>
Total<br/>
27<br/>
11<br/>
81<br/>
119<br/>
ber&#160;of&#160;participants&#160;who&#160;fall&#160;into&#160;each&#160;categories&#160;are&#160;shown&#160;in&#160;Table&#160;<a href="thesiss.html#61">5.3.&#160;</a>Twenty-six&#160;of&#160;the<br/>
non-experienced&#160;participants&#160;give&#160;higher&#160;or&#160;equal&#160;rating&#160;to&#160;computer&#160;than&#160;human,&#160;slightly<br/>
higher&#160;than&#160;twenty-four&#160;people&#160;who&#160;gives&#160;higher&#160;rating&#160;to&#160;human.&#160;For&#160;amateur&#160;and&#160;pro-<br/>
fessional&#160;musicians,&#160;the&#160;number&#160;of&#160;people&#160;who&#160;prefers&#160;human&#160;are&#160;much&#160;higher.&#160;In&#160;Table<br/>
<a href="thesiss.html#61">5.4&#160;</a>the&#160;generated&#160;performances&#160;are&#160;split&#160;into&#160;two&#160;categories&#160;just&#160;like&#160;Table&#160;<a href="thesiss.html#61">5.2.&#160;</a>The&#160;results<br/>
are&#160;similar&#160;to&#160;Table&#160;<a href="thesiss.html#61">5.3:&#160;</a>the&#160;difference&#160;between&#160;computer&#160;and&#160;human&#160;is&#160;higher&#160;for&#160;skillful<br/>
performers&#160;(A&#160;and&#160;B)&#160;than&#160;less-skillful&#160;performers&#160;(C&#160;to&#160;F).&#160;Therefore&#160;we&#160;can&#160;conclude<br/>
that&#160;our&#160;system&#160;has&#160;the&#160;same&#160;expressive&#160;power&#160;for&#160;participants&#160;with&#160;no&#160;music&#160;background.<br/>
But&#160;for&#160;amateur&#160;and&#160;professional&#160;musician,&#160;the&#160;system&#160;requires&#160;further&#160;improvements&#160;to<br/>
be&#160;comparable&#160;to&#160;human&#160;musician.<br/>
Table&#160;5.4:&#160;Number&#160;of&#160;participants&#160;who&#160;gives&#160;higher&#160;rating&#160;to&#160;generated&#160;performance,&#160;hu-<br/>man&#160;recordings&#160;or&#160;equal&#160;rating&#160;under&#160;different&#160;part&#160;of&#160;the&#160;corpus<br/>
A,B<br/>
C-F<br/>
Total<br/>
Computer<br/>
Equal<br/>
Human<br/>
Computer<br/>
Equal<br/>
Human<br/>
No&#160;experience<br/>
5<br/>
4<br/>
6<br/>
14<br/>
3<br/>
18<br/>
50<br/>
Amateur<br/>
2<br/>
1<br/>
18<br/>
5<br/>
2<br/>
31<br/>
59<br/>
Professional<br/>
0<br/>
1<br/>
3<br/>
1<br/>
0<br/>
5<br/>
10<br/>
Total&#160;Result<br/>
7<br/>
6<br/>
27<br/>
20<br/>
5<br/>
54<br/>
119<br/>
49<br/>
<hr>
<A name=62></a><IMG src="thesis-62_1.jpg"/><br/>
<b>Chapter&#160;6</b><br/>
<b>Conclusions</b><br/>
We&#160;have&#160;created&#160;a&#160;system&#160;that&#160;can&#160;perform&#160;monophonic&#160;score&#160;expressively.&#160;The&#160;ex-<br/>
pressive&#160;performance&#160;knowledge&#160;is&#160;learned&#160;from&#160;hum&#160;an&#160;recording&#160;using&#160;structural&#160;sup-<br/>
port&#160;vector&#160;machine&#160;with&#160;hidden&#160;Markov&#160;model&#160;output&#160;(SVM-HMM).&#160;We&#160;have&#160;also&#160;cre-<br/>
ated&#160;a&#160;corpus&#160;consisting&#160;of&#160;scores&#160;and&#160;MIDI&#160;recordings.&#160;From&#160;our&#160;subjective&#160;test,&#160;we<br/>
show&#160;that&#160;although&#160;the&#160;amateur&#160;and&#160;professional&#160;musician&#160;can&#160;still&#160;differentiate&#160;the&#160;gen-<br/>
erated&#160;performance&#160;from&#160;human&#160;recordings,&#160;test&#160;subjects&#160;with&#160;no&#160;music&#160;background&#160;are<br/>
giving&#160;equal&#160;ratings&#160;to&#160;the&#160;generated&#160;performance&#160;and&#160;human&#160;recordings,&#160;which&#160;means<br/>
our&#160;system&#160;has&#160;the&#160;same&#160;expressive&#160;power&#160;as&#160;human.<br/>
There&#160;are&#160;many&#160;room&#160;for&#160;improvement.&#160;Structural&#160;expressions&#160;such&#160;as&#160;phrasing,&#160;con-<br/>
trast&#160;between&#160;sections,&#160;or&#160;even&#160;contrast&#160;between&#160;movements&#160;can&#160;be&#160;added,&#160;which&#160;requires<br/>
automatic&#160;structural&#160;analysis.&#160;Other&#160;information&#160;like&#160;text&#160;notations,&#160;harmonic&#160;analysis<br/>
and&#160;other&#160;musicological&#160;analysis&#160;can&#160;be&#160;added&#160;to&#160;the&#160;learning&#160;process.&#160;Supporting&#160;ho-<br/>
mophonic&#160;or&#160;polyphonic&#160;music&#160;is&#160;also&#160;important&#160;for&#160;the&#160;system&#160;to&#160;be&#160;useful.&#160;Sub-note<br/>
expressions&#160;like&#160;physical&#160;model&#160;synthesizer&#160;or&#160;envelope&#160;shaping&#160;can&#160;also&#160;be&#160;applied&#160;to<br/>
generate&#160;performances&#160;for&#160;specific&#160;musical&#160;instruments.&#160;It's&#160;also&#160;crucial&#160;to&#160;test&#160;the&#160;sys-<br/>
tem&#160;on&#160;more&#160;samples&#160;of&#160;different&#160;genre&#160;or&#160;music&#160;style.&#160;We&#160;also&#160;believe&#160;that&#160;combining<br/>
rule-based&#160;model&#160;and&#160;machine-learning&#160;model&#160;may&#160;be&#160;a&#160;possible&#160;direction&#160;for&#160;computer<br/>
expressive&#160;music&#160;performance&#160;research.&#160;With&#160;rules&#160;serving&#160;as&#160;a&#160;high&#160;level&#160;guideline&#160;for<br/>
structural&#160;expression,&#160;the&#160;machine-learning&#160;model&#160;can&#160;focus&#160;on&#160;note&#160;or&#160;sub-note&#160;level&#160;ex-<br/>
pression.&#160;User&#160;can&#160;gain&#160;more&#160;control&#160;by&#160;tweaking&#160;the&#160;rules.<br/>
50<br/>
<hr>
<A name=63></a><IMG src="thesis-63_1.jpg"/><br/>
<b>Bibliography</b><br/>
[1]&#160;R.&#160;Hiraga,&#160;R.&#160;Bresin,&#160;K.&#160;Hirata,&#160;and&#160;H.&#160;Katayose,&#160;“Rencon&#160;2004:&#160;Turing&#160;test&#160;for&#160;mu-<br/>
sical&#160;expression&#160;,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;2004&#160;conference&#160;on&#160;New&#160;Interfaces&#160;for&#160;Mu-</i><br/>
<i>sical&#160;Expression&#160;(NIME&#160;'04)&#160;</i>(Y.&#160;Nagashima&#160;and&#160;M.&#160;Lyons,&#160;eds.),&#160;(Hamatsu,&#160;Japan),<br/>
pp.&#160;120--123,&#160;ACM&#160;Press,&#160;2004.<br/>
[2]&#160;“Finale&#160;[Computer&#160;Software].”&#160;<a href="http://www.finalemusic.com/">http://www.finalemusic.com/.&#160;</a>[Online;<br/>
accessed&#160;2014-05-20].<br/>
[3]&#160;“Sibelius&#160;[Computer&#160;Software].”&#160;<a href="http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share">http://www.avid.com/us/products/</a><br/>
<a href="http://www.avid.com/us/products/sibelius/pc/Play-perform-and-share">sibelius/pc/Play-perform-and-share</a>.<br/>
[Online;&#160;accessed&#160;2014-05-<br/>
20].<br/>
[4]&#160;“Rachmianinoff&#160;-&#160;Plays&#160;Rachmaninoff&#160;[CD].”&#160;Zenph&#160;Music,&#160;2009.<br/>
[5]&#160;“Cadenza&#160;[Computer&#160;Software].”&#160;<a href="http://www.sonation.net/">http://www.sonation.net/</a>.&#160;[Online;&#160;ac-<br/>
cessed&#160;2014-05-20].<br/>
[6]&#160;A.&#160;Kirke&#160;and&#160;E.&#160;R.&#160;Miranda,&#160;“An&#160;Overview&#160;of&#160;Computer&#160;Systems&#160;for&#160;Expressive<br/>
Music&#160;Performance,”&#160;in&#160;<i>Guide&#160;to&#160;Computing&#160;for&#160;Expressive&#160;Music&#160;Performance</i><br/>
(A.&#160;Kirke&#160;and&#160;E.&#160;R.&#160;Miranda,&#160;eds.),&#160;pp.&#160;1--47,&#160;Springer,&#160;2013.<br/>
[7]&#160;A.&#160;Friberg,&#160;R.&#160;Bresin,&#160;and&#160;J.&#160;Sundberg,&#160;“Overview&#160;of&#160;the&#160;KTH&#160;rule&#160;system&#160;for&#160;mu-<br/>
sical&#160;performance,”&#160;<i>Advances&#160;in&#160;Cognitive&#160;Psychology</i>,&#160;vol.&#160;2,&#160;pp.&#160;145--161,&#160;Jan.<br/>
2006.<br/>
[8]&#160;M.&#160;Hashida,&#160;N.&#160;Nagata,&#160;and&#160;H.&#160;Katayose,&#160;“Pop-E:&#160;a&#160;performance&#160;rendering&#160;system<br/>
for&#160;the&#160;ensemble&#160;music&#160;that&#160;considered&#160;group&#160;expression,”&#160;in&#160;<i>Proceedings&#160;of&#160;9th&#160;In-</i><br/>
51<br/>
<hr>
<A name=64></a><IMG src="thesis-64_1.jpg"/><br/>
<i>ternational&#160;Conference&#160;on&#160;Music&#160;Perception&#160;and&#160;Cognition&#160;</i>(M.&#160;Baroni,&#160;R.&#160;Addessi,<br/>
R.&#160;Caterina,&#160;and&#160;M.&#160;Costa,&#160;eds.),&#160;(Bologna,&#160;Spain),&#160;pp.&#160;526--534,&#160;ICMPC,&#160;2006.<br/>
[9]&#160;S.&#160;R.&#160;Livingstone,&#160;R.&#160;Mühlberger,&#160;A.&#160;R.&#160;Brown,&#160;and&#160;A.&#160;Loch,&#160;“Controlling&#160;musical<br/>
emotionality:&#160;an&#160;affective&#160;computational&#160;architecture&#160;for&#160;influencing&#160;musical&#160;emo-<br/>
tions,”&#160;<i>Digital&#160;Creativity</i>,&#160;vol.&#160;18,&#160;pp.&#160;43--53,&#160;Mar.&#160;2007.<br/>
[10]&#160;N.&#160;P.&#160;M.&#160;Todd,&#160;“A&#160;computational&#160;model&#160;of&#160;rubato,”&#160;<i>Contemporary&#160;Music&#160;Review</i>,<br/>
vol.&#160;3,&#160;pp.&#160;69--88,&#160;Jan.&#160;1989.<br/>
[11]&#160;N.&#160;P.&#160;McAngus&#160;Todd,&#160;“The&#160;dynamics&#160;of&#160;dynamics:&#160;A&#160;model&#160;of&#160;musical&#160;expression,”<br/>
<i>The&#160;Journal&#160;of&#160;the&#160;Acoustical&#160;Society&#160;of&#160;America</i>,&#160;vol.&#160;91,&#160;p.&#160;3540,&#160;June&#160;1992.<br/>
[12]&#160;N.&#160;P.&#160;M.&#160;Todd,&#160;“The&#160;kinematics&#160;of&#160;musical&#160;expression,”&#160;<i>The&#160;Journal&#160;of&#160;the&#160;Acoustical</i><br/>
<i>Society&#160;of&#160;America</i>,&#160;vol.&#160;97,&#160;p.&#160;1940,&#160;Mar.&#160;1995.<br/>
[13]&#160;M.&#160;Clynes,&#160;“Generative&#160;principles&#160;of&#160;musical&#160;thought:&#160;Integration&#160;of&#160;microstructure<br/>
with&#160;structure,”&#160;<i>Journal&#160;For&#160;The&#160;Integrated&#160;Study&#160;of&#160;Artificial&#160;Intelligence</i>,&#160;1986.<br/>
[14]&#160;M.&#160;Clynes,&#160;“Microstructural&#160;musical&#160;linguistics:&#160;composers'&#160;pulses&#160;are&#160;liked&#160;most<br/>
by&#160;the&#160;best&#160;musicians,”&#160;<i>Cognition</i>,&#160;1995.<br/>
[15]&#160;M.&#160;Johnson,&#160;“Toward&#160;an&#160;expert&#160;system&#160;for&#160;expressive&#160;musical&#160;performance,”&#160;<i>Com-</i><br/>
<i>puter</i>,&#160;vol.&#160;24,&#160;pp.&#160;30--34,&#160;July&#160;1991.<br/>
[16]&#160;R.&#160;B.&#160;Dannenberg&#160;and&#160;I.&#160;Derenyi,&#160;“Combining&#160;instrument&#160;and&#160;performance&#160;models<br/>
for&#160;high-quality&#160;music&#160;synthesis,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,&#160;vol.&#160;27,&#160;pp.&#160;211-<br/>
-238,&#160;Sept.&#160;1998.<br/>
[17]&#160;R.&#160;B.&#160;Dannenberg,&#160;H.&#160;Pellerin,&#160;and&#160;I.&#160;Derenyi,&#160;“A&#160;Study&#160;of&#160;Trumpet&#160;Envelopes,”&#160;in<br/>
<i>Proceedings&#160;of&#160;the&#160;1998&#160;international&#160;computer&#160;music&#160;conference&#160;</i>(O.&#160;1998,&#160;ed.),<br/>
(Ann&#160;Arbor,&#160;Michigan),&#160;pp.&#160;57--61,&#160;International&#160;Computer&#160;Music&#160;Association,<br/>
1998.<br/>
[18]&#160;G.&#160;Mazzola&#160;and&#160;O.&#160;Zahorka,&#160;“Tempo&#160;curves&#160;revisited:&#160;Hierarchies&#160;of&#160;performance<br/>
fields,”&#160;<i>Computer&#160;Music&#160;Journal</i>,&#160;vol.&#160;18,&#160;no.&#160;1,&#160;pp.&#160;40--52,&#160;1994.<br/>
52<br/>
<hr>
<A name=65></a><IMG src="thesis-65_1.jpg"/><br/>
[19]&#160;G.&#160;Mazzola,&#160;<i>The&#160;Topos&#160;of&#160;Music:&#160;Geometric&#160;Logic&#160;of&#160;Concepts,&#160;Theory,&#160;and&#160;Per-</i><br/>
<i>formance</i>.&#160;Basel/Boston:&#160;Birkhäuser,&#160;2002.<br/>
[20]&#160;W.&#160;A.&#160;Sethares,&#160;<i>Tuning,&#160;Timbre,&#160;Spectrum,&#160;Scale</i>.&#160;Springer,&#160;2005.<br/>
[21]&#160;H.&#160;Katayose,&#160;T.&#160;Fukuoka,&#160;K.&#160;Takami,&#160;and&#160;S.&#160;Inokuchi,&#160;“Expression&#160;extraction&#160;in<br/>
virtuoso&#160;music&#160;performances,”&#160;in&#160;<i>Proceedings&#160;of&#160;10th&#160;International&#160;Conference&#160;on</i><br/>
<i>Pattern&#160;Recognition</i>,&#160;vol.&#160;I,&#160;pp.&#160;780--784,&#160;IEEE&#160;Computer&#160;Society&#160;Press,&#160;1990.<br/>
[22]&#160;H.&#160;Katayose,&#160;T.&#160;Fukuoka,&#160;K.&#160;Takami,&#160;and&#160;S.&#160;Inokuchi,&#160;“Extraction&#160;of&#160;expression<br/>
parameters&#160;with&#160;multiple&#160;regression&#160;analysis,”&#160;<i>Journal&#160;of&#160;Information&#160;Processing</i><br/>
<i>Society&#160;of&#160;Japan</i>,&#160;no.&#160;38,&#160;pp.&#160;1473--1481,&#160;1997.<br/>
[23]&#160;O.&#160;Ishikawa,&#160;Y.&#160;Aono,&#160;H.&#160;Katayose,&#160;and&#160;S.&#160;Inokuchi,&#160;“Extraction&#160;of&#160;Musical&#160;Per-<br/>
formance&#160;Rules&#160;Using&#160;a&#160;Modified&#160;Algorithm&#160;of&#160;Multiple&#160;Regression&#160;Analysis,”&#160;in<br/>
<i>International&#160;Computer&#160;Music&#160;Conference&#160;Proceedings</i>,&#160;(Berlin,&#160;Germany),&#160;pp.&#160;348-<br/>
-351,&#160;International&#160;Computer&#160;Music&#160;Association,&#160;San&#160;Francisco,&#160;2000.<br/>
[24]&#160;S.&#160;Canazza,&#160;G.&#160;De&#160;Poli,&#160;C.&#160;Drioli,&#160;A.&#160;Rodà,&#160;and&#160;A.&#160;Vidolin,&#160;“Audio&#160;Morphing&#160;Differ-<br/>
ent&#160;Expressive&#160;Intentions&#160;for&#160;Multimedia&#160;Systems,”&#160;<i>IEEE&#160;Multimedia</i>,&#160;vol.&#160;7,&#160;pp.&#160;79-<br/>
-83,&#160;July&#160;2000.<br/>
[25]&#160;S.&#160;Canazza,&#160;A.&#160;Vidolin,&#160;G.&#160;De&#160;Poli,&#160;C.&#160;Drioli,&#160;and&#160;A.&#160;Rodà,&#160;“Expressive&#160;Morphing<br/>
for&#160;Interactive&#160;Performance&#160;of&#160;Musical&#160;Scores,”&#160;in&#160;<i>Proceedings&#160;of&#160;1st&#160;International</i><br/>
<i>Conference&#160;on&#160;Web&#160;Delivering&#160;of&#160;Music</i>,&#160;p.&#160;116,&#160;IEEE&#160;Computer&#160;Society,&#160;Nov.&#160;2001.<br/>
[26]&#160;S.&#160;Canazza,&#160;G.&#160;De&#160;Poli,&#160;A.&#160;Rodà,&#160;and&#160;A.&#160;Vidolin,&#160;“An&#160;Abstract&#160;Control&#160;Space&#160;for<br/>
Communication&#160;of&#160;Sensory&#160;Expressive&#160;Intentions&#160;in&#160;Music&#160;Performance,”&#160;<i>Journal</i><br/>
<i>of&#160;New&#160;Music&#160;Research</i>,&#160;vol.&#160;32,&#160;pp.&#160;281--294,&#160;Sept.&#160;2003.<br/>
[27]&#160;R.&#160;Bresin,&#160;“Artificial&#160;neural&#160;networks&#160;based&#160;models&#160;for&#160;automatic&#160;performance&#160;of<br/>
musical&#160;scores,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,&#160;vol.&#160;27,&#160;pp.&#160;239--270,&#160;Sept.&#160;1998.<br/>
53<br/>
<hr>
<A name=66></a><IMG src="thesis-66_1.jpg"/><br/>
[28]&#160;A.&#160;Camurri,&#160;R.&#160;Dillon,&#160;and&#160;A.&#160;Saron,&#160;“An&#160;experiment&#160;on&#160;analysis&#160;and&#160;synthesis&#160;of<br/>
musical&#160;expressivity,”&#160;in&#160;<i>Proceedings&#160;of&#160;13th&#160;colloquium&#160;on&#160;musical&#160;informatics</i>,<br/>
(L'Aquila,&#160;Italy),&#160;2000.<br/>
[29]&#160;G.&#160;Grindlay,&#160;<i>Modeling&#160;expressive&#160;musical&#160;performance&#160;with&#160;Hidden&#160;Markov&#160;Models</i>.<br/>
PhD&#160;thesis,&#160;University&#160;of&#160;Santa&#160;Cruz,&#160;CA,&#160;2005.<br/>
[30]&#160;C.&#160;Raphael,&#160;“Can&#160;the&#160;computer&#160;learn&#160;to&#160;play&#160;music&#160;expressively?,”&#160;in&#160;<i>Proceedings&#160;of</i><br/>
<i>the&#160;8th&#160;International&#160;Workshop&#160;on&#160;Artificial&#160;Intelligence&#160;and&#160;Statistics&#160;</i>(T.&#160;Jaakkola<br/>
and&#160;T.&#160;Richardson,&#160;eds.),&#160;pp.&#160;113--120,&#160;Morgan&#160;Kaufmann,&#160;San&#160;Francisco,&#160;2001.<br/>
[31]&#160;C.&#160;Raphael,&#160;“A&#160;Bayesian&#160;Network&#160;for&#160;Real-Time&#160;Musical&#160;Accompaniment.,”&#160;<i>Neural</i><br/>
<i>Information&#160;Processing&#160;Systems</i>,&#160;no.&#160;14,&#160;pp.&#160;1433--1440,&#160;2001.<br/>
[32]&#160;C.&#160;Raphael,&#160;“Orchestra&#160;in&#160;a&#160;box:&#160;A&#160;system&#160;for&#160;real-time&#160;musical&#160;accompaniment,”&#160;in<br/>
<i>Proceedings&#160;of&#160;2003&#160;International&#160;Joint&#160;conference&#160;on&#160;Artifical&#160;Intelligence&#160;(Work-</i><br/>
<i>ing&#160;Notes&#160;of&#160;IJCAI-03&#160;Rencon&#160;Workshop)&#160;</i>(G.&#160;Gottob&#160;and&#160;T.&#160;Walsh,&#160;eds.),&#160;(Acapulco,<br/>
Mexico),&#160;pp.&#160;5--10,&#160;Morgan&#160;Kaufmann,&#160;San&#160;Francisco,&#160;2003.<br/>
[33]&#160;L.&#160;Dorard,&#160;D.&#160;Hardoon,&#160;and&#160;J.&#160;Shawe-Taylor,&#160;“Can&#160;style&#160;be&#160;learned?&#160;A&#160;machine<br/>
learning&#160;approach&#160;towards&#160;‘performing’as&#160;famous&#160;pianists.,”&#160;in&#160;<i>Proceedings&#160;of</i><br/>
<i>the&#160;Music,&#160;Brain&#160;and&#160;Cognition&#160;Workshop&#160;--&#160;Neural&#160;Information&#160;Processing&#160;Systems</i>,<br/>
Whistler,&#160;Canada,&#160;2007.<br/>
[34]&#160;M.&#160;Wright&#160;and&#160;E.&#160;Berdahl,&#160;“Towards&#160;machine&#160;learning&#160;of&#160;expressive&#160;microtiming<br/>
in&#160;Brazilian&#160;drumming,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;2006&#160;International&#160;Computer&#160;Music</i><br/>
<i>Conference&#160;</i>(I.&#160;Zannos,&#160;ed.),&#160;(New&#160;Orleans,&#160;USA),&#160;pp.&#160;572--575,&#160;ICMA,&#160;San&#160;Fran-<br/>
cisco,&#160;2006.<br/>
[35]&#160;R.&#160;Ramirez&#160;and&#160;A.&#160;Hazan,&#160;“Modeling&#160;Expressive&#160;Music&#160;Performance&#160;in&#160;Jazz.,”&#160;in<br/>
<i>Proceedings&#160;of&#160;18th&#160;international&#160;Florida&#160;Artificial&#160;Intelligence&#160;Research&#160;Society</i><br/>
<i>Sonference&#160;(AI&#160;in&#160;Music&#160;and&#160;Art)</i>,&#160;(Clearwater&#160;Beach,&#160;FL,&#160;USA),&#160;pp.&#160;86--91,&#160;AAAI<br/>
Press,&#160;Menlo&#160;Park,&#160;2005.<br/>
54<br/>
<hr>
<A name=67></a><IMG src="thesis-67_1.jpg"/><br/>
[36]&#160;R.&#160;Ramirez&#160;and&#160;A.&#160;Hazan,&#160;“Inducing&#160;a&#160;generative&#160;expressive&#160;performance&#160;model<br/>
using&#160;a&#160;sequential-covering&#160;genetic&#160;algorithm,”&#160;in&#160;<i>Proceedings&#160;of&#160;2007&#160;annual&#160;con-</i><br/>
<i>ference&#160;on&#160;Genetic&#160;and&#160;evolutionary&#160;computation</i>,&#160;(London,&#160;UK),&#160;ACM&#160;Press,&#160;New<br/>
York,&#160;2007.<br/>
[37]&#160;Q.&#160;Zhang&#160;and&#160;E.&#160;Miranda,&#160;“Towards&#160;an&#160;evolution&#160;model&#160;of&#160;expressive&#160;music&#160;perfor-<br/>
mance,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;6th&#160;International&#160;Conference&#160;on&#160;Intelligent&#160;Systems</i><br/>
<i>Design&#160;and&#160;Applications&#160;</i>(Y.&#160;Chen&#160;and&#160;A.&#160;Abraham,&#160;eds.),&#160;(Jinan,&#160;China),&#160;pp.&#160;1189-<br/>
-1194,&#160;IEEE&#160;Computer&#160;Society,&#160;Washington,&#160;DC,&#160;2006.<br/>
[38]&#160;E.&#160;Miranda,&#160;A.&#160;Kirke,&#160;and&#160;Q.&#160;Zhang,&#160;“Artificial&#160;evolution&#160;of&#160;expressive&#160;performance<br/>
of&#160;music:&#160;An&#160;imitative&#160;multi-agent&#160;systems&#160;approach,”&#160;<i>Computer&#160;Music&#160;Journal</i>,<br/>
vol.&#160;34,&#160;no.&#160;1,&#160;pp.&#160;80--96,&#160;2010.<br/>
[39]&#160;Q.&#160;Zhang&#160;and&#160;E.&#160;R.&#160;Miranda,&#160;“Evolving&#160;Expressive&#160;Music&#160;Performance&#160;through&#160;In-<br/>
teraction&#160;of&#160;Artificial&#160;Agent&#160;Performers,”&#160;in&#160;<i>Proceedings&#160;of&#160;ECAL&#160;2007&#160;workshop</i><br/>
<i>on&#160;music&#160;and&#160;artificial&#160;life&#160;(MusicAL&#160;2007)</i>,&#160;(Lisbon,&#160;Portugal),&#160;2007.<br/>
[40]&#160;J.&#160;L.&#160;Arcos,&#160;R.&#160;L.&#160;De&#160;Mántaras,&#160;and&#160;X.&#160;Serra,&#160;“SaxEx:&#160;a&#160;case-based&#160;reasoning&#160;system<br/>
for&#160;generating&#160;expressive&#160;musical&#160;performances,”&#160;in&#160;<i>Proceedings&#160;of&#160;1997&#160;Interna-</i><br/>
<i>tional&#160;Computer&#160;Music&#160;Conference&#160;</i>(P.&#160;Cook,&#160;ed.),&#160;(Thessalonikia,&#160;Greece),&#160;pp.&#160;329-<br/>
-336,&#160;ICMA,&#160;San&#160;Francisco,&#160;1997.<br/>
[41]&#160;J.&#160;L.&#160;Arcos,&#160;R.&#160;L.&#160;De&#160;Mántaras,&#160;and&#160;X.&#160;Serra,&#160;“SaxEx:&#160;A&#160;case-based&#160;reasoning&#160;system<br/>
for&#160;generating&#160;expressive&#160;musical&#160;performances,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,<br/>
vol.&#160;27,&#160;no.&#160;3,&#160;pp.&#160;194--210,&#160;1998.<br/>
[42]&#160;J.&#160;L.&#160;Arcos&#160;and&#160;R.&#160;L.&#160;De&#160;Mántaras,&#160;“An&#160;Interactive&#160;Case-Based&#160;Reasoning&#160;Approach<br/>
for&#160;Generating&#160;Expressive&#160;Music,”&#160;<i>Journal&#160;of&#160;Applied&#160;Intelligence</i>,&#160;vol.&#160;14,&#160;pp.&#160;115-<br/>
-129,&#160;Jan.&#160;2001.<br/>
[43]&#160;T.&#160;Suzuki,&#160;T.&#160;Tokunaga,&#160;and&#160;H.&#160;Tanaka,&#160;“A&#160;case&#160;based&#160;approach&#160;to&#160;the&#160;generation&#160;of<br/>
musical&#160;expression,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;16th&#160;International&#160;Joint&#160;Conference&#160;on</i><br/>
55<br/>
<hr>
<A name=68></a><IMG src="thesis-68_1.jpg"/><br/>
<i>Artificial&#160;Intelligence</i>,&#160;(Stockholm,&#160;Sweden),&#160;pp.&#160;642--648,&#160;Morgan&#160;Kaufmann,&#160;San<br/>
Francisco,&#160;1999.<br/>
[44]&#160;T.&#160;Suzuki,&#160;“Kagurame&#160;phase-II,”&#160;in&#160;<i>Proceedings&#160;of&#160;2003&#160;International&#160;Joint&#160;Con-</i><br/>
<i>ference&#160;on&#160;Artificial&#160;Intelligence&#160;(working&#160;Notes&#160;of&#160;RenCon&#160;Workshop)&#160;</i>(G.&#160;Gottlob<br/>
and&#160;T.&#160;Walsh,&#160;eds.),&#160;(Acapulco,&#160;Mexico),&#160;Morgan&#160;Kaufmann,&#160;Los&#160;Altos,&#160;2003.<br/>
[45]&#160;K.&#160;Hirata&#160;and&#160;R.&#160;Hiraga,&#160;“Ha-Hi-Hun:&#160;Performance&#160;rendering&#160;system&#160;of&#160;high&#160;con-<br/>
trollability,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;ICAD&#160;2002&#160;Rencon&#160;Workshop&#160;on&#160;performance</i><br/>
<i>rendering&#160;systems</i>,&#160;(Kyoto,&#160;Japan),&#160;pp.&#160;40--46,&#160;2002.<br/>
[46]&#160;G.&#160;Widmer,&#160;“Large-scale&#160;Induction&#160;of&#160;Expressive&#160;Performance&#160;Rules:&#160;First&#160;Quantita-<br/>
tive&#160;Results,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;2000&#160;International&#160;Computer&#160;Music&#160;Conference</i><br/>
(I.&#160;Zannos,&#160;ed.),&#160;(Berlin,&#160;Germany),&#160;pp.&#160;344--347,&#160;International&#160;Computer&#160;Music<br/>
Association,&#160;San&#160;Francisco,&#160;2000.<br/>
[47]&#160;G.&#160;Widmer&#160;and&#160;A.&#160;Tobudic,&#160;“Machine&#160;discoveries:&#160;A&#160;few&#160;simple,&#160;robust&#160;local&#160;ex-<br/>
pression&#160;principles,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,&#160;vol.&#160;32,&#160;pp.&#160;259--268,&#160;2002.<br/>
[48]&#160;G.&#160;Widmer,&#160;“Discovering&#160;simple&#160;rules&#160;in&#160;complex&#160;data:&#160;A&#160;meta-learning&#160;algorithm<br/>
and&#160;some&#160;surprising&#160;musical&#160;discoveries,”&#160;<i>Artificial&#160;Intelligence</i>,&#160;vol.&#160;146,&#160;pp.&#160;129-<br/>
-148,&#160;2003.<br/>
[49]&#160;G.&#160;Widmer&#160;and&#160;A.&#160;Tobudic,&#160;“Playing&#160;Mozart&#160;by&#160;Analogy:&#160;Learning&#160;Multi-level&#160;Tim-<br/>
ing&#160;and&#160;Dynamics&#160;Strategies,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,&#160;vol.&#160;32,&#160;pp.&#160;259--<br/>
268,&#160;Sept.&#160;2003.<br/>
[50]&#160;A.&#160;Tobudic&#160;and&#160;G.&#160;Widmer,&#160;“Relational&#160;IBL&#160;in&#160;music&#160;with&#160;a&#160;new&#160;structural&#160;similarity<br/>
measure,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;13th&#160;International&#160;Conference&#160;on&#160;Inductive&#160;Logic</i><br/>
<i>Programming&#160;</i>(T.&#160;Horvath&#160;and&#160;A.&#160;Yamamoto,&#160;eds.),&#160;pp.&#160;365--382,&#160;Springer&#160;Verlag,<br/>
Berlin,&#160;2003.<br/>
[51]&#160;A.&#160;Tobudic&#160;and&#160;G.&#160;Widmer,&#160;“Learning&#160;to&#160;play&#160;Mozart:&#160;Recent&#160;improvements,”&#160;in<br/>
<i>Proceedings&#160;of&#160;2003&#160;International&#160;Joint&#160;conference&#160;on&#160;Artifical&#160;Intelligence&#160;(Work-</i><br/>
<i>ing&#160;Notes&#160;of&#160;IJCAI-03&#160;Rencon&#160;Workshop)&#160;</i>(K.&#160;Hirata,&#160;ed.),&#160;(Acapulco,&#160;Mexico),&#160;2003.<br/>
56<br/>
<hr>
<A name=69></a><IMG src="thesis-69_1.jpg"/><br/>
[52]&#160;P.&#160;Dahlstedt,&#160;“Autonomous&#160;evolution&#160;of&#160;complete&#160;piano&#160;pieces&#160;and&#160;performances,”<br/>
in&#160;<i>Proceedings&#160;of&#160;ECAL&#160;2007&#160;workshop&#160;on&#160;music&#160;and&#160;artificial&#160;life&#160;(Music&#160;AL&#160;2007)</i>,<br/>
(Lisbon,&#160;Portugal),&#160;2007.<br/>
[53]&#160;A.&#160;Kirke&#160;and&#160;E.&#160;Miranda,&#160;“Using&#160;a&#160;biophysically-constrained&#160;multi-agent&#160;system&#160;to<br/>
combine&#160;expressive&#160;performance&#160;with&#160;algorithmic&#160;composition,”&#160;2008.<br/>
[54]&#160;L.&#160;Carlson,&#160;A.&#160;Nordmark,&#160;and&#160;R.&#160;Wikilander,&#160;<i>Reason&#160;version&#160;2.5&#160;--&#160;Getting&#160;Started</i>.<br/>
Propellerhead&#160;Software,&#160;2003.<br/>
[55]&#160;Y.-H.&#160;Kuo,&#160;W.-C.&#160;Chang,&#160;T.-M.&#160;Wang,&#160;and&#160;A.&#160;W.&#160;Su,&#160;“TELPC&#160;BASED&#160;RE-<br/>
SYNTHESIS&#160;METHOD&#160;FOR&#160;ISOLATED&#160;NOTES&#160;OF&#160;POLYPHONIC&#160;INSTRU-<br/>
MENTAL&#160;MUSIC&#160;RECORDINGS,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;16th&#160;International&#160;Con-</i><br/>
<i>ference&#160;on&#160;Digital&#160;Audio&#160;Effects&#160;(DAFx-13)</i>,&#160;(Maynooth,&#160;Ireland),&#160;pp.&#160;1--6,&#160;2013.<br/>
[56]&#160;T.&#160;Joachims,&#160;T.&#160;Finley,&#160;and&#160;C.-N.&#160;J.&#160;Yu,&#160;“Cutting-plane&#160;training&#160;of&#160;structural&#160;SVMs,”<br/>
<i>Machine&#160;Learning</i>,&#160;vol.&#160;77,&#160;pp.&#160;27--59,&#160;May&#160;2009.<br/>
[57]&#160;I.&#160;Tsochantaridis,&#160;T.&#160;Joachims,&#160;T.&#160;Hofmann,&#160;and&#160;Y.&#160;Altun,&#160;“Large&#160;Margin&#160;Methods<br/>
for&#160;Structured&#160;and&#160;Interdependent&#160;Output&#160;Variables,”&#160;<i>Journal&#160;of&#160;Machine&#160;Learning</i><br/>
<i>Research</i>,&#160;vol.&#160;6,&#160;pp.&#160;1453--1484,&#160;2005.<br/>
[58]&#160;Y.&#160;Altun,&#160;I.&#160;Tsochantaridis,&#160;and&#160;T.&#160;Hofmann,&#160;“Hidden&#160;Markov&#160;Support&#160;Vector&#160;Ma-<br/>
chines,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;20th&#160;International&#160;Conference&#160;on&#160;Machine&#160;Learning</i>,<br/>
vol.&#160;3,&#160;(Washington&#160;DC,&#160;USA),&#160;pp.&#160;3--10,&#160;2003.<br/>
[59]&#160;M.&#160;Cuthbert&#160;and&#160;C.&#160;Ariza,&#160;“music21&#160;[computer&#160;software].”&#160;http://web.mit.edu/mu-<br/>
sic21/.&#160;[accessed&#160;2014-05-20].<br/>
[60]&#160;MIDI&#160;Manufacturers&#160;Association,&#160;“The&#160;Complete&#160;MIDI&#160;1.0&#160;Detailed&#160;Specification.”<br/>
http://www.midi.org/techspecs/midispec.php.&#160;[Online;&#160;accessed&#160;2014-05-20].<br/>
[61]&#160;S.&#160;H.&#160;Lyu&#160;and&#160;S.-K.&#160;Jeng,&#160;“COMPUTER&#160;EXPRESSIVE&#160;MUSIC&#160;PERFORMANCE<br/>
BY&#160;PHRASE-WISE&#160;MODELING,”&#160;in&#160;<i>Workshop&#160;on&#160;Computer&#160;Music&#160;and&#160;Audio</i><br/>
<i>Technology</i>,&#160;2012.<br/>
57<br/>
<hr>
<A name=70></a><IMG src="thesis-70_1.jpg"/><br/>
[62]&#160;T.&#160;Joachims,&#160;“SVMˆhmm:&#160;Sequence&#160;Tagging&#160;with&#160;Structural&#160;Support&#160;Vector<br/>
Machines.”<br/>
<a href="http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html">http://www.cs.cornell.edu/people/tj/svm_light/</a><br/>
<a href="http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html">svm_hmm.html</a>.&#160;[Online;&#160;accessed&#160;2014-05-20].<br/>
[63]&#160;“MusicXML<br/>
3.0<br/>
Specification.”<br/>
<a href="http://www.musicxml.com/for-developers/">http://www.musicxml.com/</a><br/>
<a href="http://www.musicxml.com/for-developers/">for-developers/</a>.&#160;[Online;&#160;accessed&#160;2014-05-20].<br/>
[64]&#160;R.&#160;P.&#160;Brent,&#160;<i>Algorithms&#160;for&#160;Minimization&#160;Without&#160;Derivatives</i>.&#160;2013.<br/>
[65]&#160;M.&#160;Hashida,&#160;T.&#160;Matsui,&#160;and&#160;H.&#160;Katayose,&#160;“A&#160;New&#160;Music&#160;Database&#160;Describing&#160;Devia-<br/>
tion&#160;Information&#160;of&#160;Performance&#160;Expressions,”&#160;in&#160;<i>International&#160;Conference&#160;of&#160;Music</i><br/>
<i>Information&#160;Retrival&#160;(ISMIR)</i>,&#160;pp.&#160;489--494,&#160;2008.<br/>
[66]&#160;S.&#160;Flossmann,&#160;W.&#160;Goebl,&#160;M.&#160;Grachten,&#160;B.&#160;Niedermayer,&#160;and&#160;G.&#160;Widmer,&#160;“The&#160;Ma-<br/>
galoff&#160;project:&#160;An&#160;interim&#160;report,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,&#160;vol.&#160;39,&#160;no.&#160;4,<br/>
pp.&#160;363--377,&#160;2010.<br/>
[67]&#160;W.&#160;Goebl,&#160;S.&#160;Flossmann,&#160;and&#160;G.&#160;Widmer,&#160;“Computational&#160;investigations&#160;into<br/>
between-hand&#160;synchronization&#160;in&#160;piano&#160;playing:&#160;Magaloff's&#160;complete&#160;Chopin,”&#160;in<br/>
<i>Proceedings&#160;of&#160;the&#160;Sixth&#160;Sound&#160;and&#160;Music&#160;Computing&#160;Conference</i>,&#160;pp.&#160;291--296,<br/>
2009.<br/>
[68]&#160;M.&#160;Grachten&#160;and&#160;G.&#160;Widmer,&#160;“Explaining&#160;musical&#160;expression&#160;as&#160;a&#160;mixture&#160;of&#160;basis<br/>
functions,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;8th&#160;Sound&#160;and&#160;Music&#160;Computing&#160;Conference&#160;(SMC</i><br/>
<i>2011)</i>,&#160;2011.<br/>
[69]&#160;S.&#160;Flossmann,&#160;W.&#160;Goebl,&#160;and&#160;G.&#160;Widmer,&#160;“Maintaining&#160;skill&#160;across&#160;the&#160;life&#160;span:<br/>
Magaloff's&#160;entire&#160;Chopin&#160;at&#160;age&#160;77,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;International&#160;Symposium</i><br/>
<i>on&#160;Performance&#160;Science</i>,&#160;2009.<br/>
[70]&#160;M.&#160;Grachten&#160;and&#160;G.&#160;Widmer,&#160;“Linear&#160;basis&#160;models&#160;for&#160;prediction&#160;and&#160;analysis&#160;of<br/>
musical&#160;expression,”&#160;<i>Journal&#160;of&#160;New&#160;Music&#160;Research</i>,&#160;2012.<br/>
58<br/>
<hr>
<A name=71></a><IMG src="thesis-71_1.jpg"/><br/>
[71]&#160;S.&#160;Flossmann,&#160;M.&#160;Grachten,&#160;and&#160;G.&#160;Widmer,&#160;“Expressive&#160;performance&#160;rendering<br/>
with&#160;probabilistic&#160;models,”&#160;in&#160;<i>Guide&#160;to&#160;Computing&#160;for&#160;Expressive&#160;Music&#160;Perfor-</i><br/>
<i>mance&#160;</i>(A.&#160;Kirke&#160;and&#160;E.&#160;R.&#160;Miranda,&#160;eds.),&#160;pp.&#160;75--98,&#160;Springer&#160;London,&#160;2013.<br/>
[72]&#160;S.&#160;Flossman&#160;and&#160;G.&#160;Widmer,&#160;“Toward&#160;a&#160;model&#160;of&#160;performance&#160;errors:&#160;A&#160;qualitative<br/>
review&#160;of&#160;Magaloff's&#160;Chopin,”&#160;in&#160;<i>International&#160;Symposium&#160;on&#160;Performance&#160;Science</i>,<br/>
(Utrecht),&#160;AEC,&#160;2011.<br/>
[73]&#160;S.&#160;Flossmann,&#160;W.&#160;Goebl,&#160;and&#160;G.&#160;Widmer,&#160;“The&#160;Magaloff&#160;corpus:&#160;An&#160;empirical&#160;error<br/>
study,”&#160;in&#160;<i>Proceedings&#160;of&#160;the&#160;11th&#160;ICMPC</i>,&#160;(Seattle,&#160;Washington,&#160;USA),&#160;2010.<br/>
[74]&#160;G.&#160;Widmer,&#160;S.&#160;Flossmann,&#160;and&#160;M.&#160;Grachten,&#160;“YQX&#160;Plays&#160;Chopin,”&#160;<i>AI&#160;Magazine</i>,<br/>
vol.&#160;30,&#160;p.&#160;35,&#160;July&#160;2009.<br/>
[75]&#160;F.&#160;Lerdahl&#160;and&#160;R.&#160;S.&#160;Jackendoff,&#160;<i>A&#160;Generative&#160;Theory&#160;of&#160;Tonal&#160;Music</i>.&#160;1983.<br/>
[76]&#160;“KernScores.”&#160;<a href="http://kern.ccarh.org/">http://kern.ccarh.org/.&#160;</a>[Online;&#160;accessed&#160;2014-05-20].<br/>
[77]&#160;M.&#160;Clementi,&#160;<i>SONATINES&#160;pour&#160;Piano&#160;a&#160;2&#160;mains&#160;Op.&#160;36&#160;VOLUME&#160;I&#160;[Musical&#160;Score]</i>.<br/>
Paris:&#160;Durand&#160;&amp;&#160;Cie.,&#160;plate&#160;d.&#160;&amp;&#160;c.&#160;9318&#160;ed.,&#160;1915.<br/>
[78]&#160;P.&#160;Eggert,&#160;M.&#160;Haertel,&#160;D.&#160;Hayes,&#160;R.&#160;Stallman,&#160;and&#160;L.&#160;Tower,&#160;“diff&#160;[Computer&#160;Pro-<br/>
gram].”<br/>
[79]&#160;M.&#160;Good,&#160;“MusicXML:&#160;An&#160;Internet-Friendly&#160;Format&#160;for&#160;Sheet&#160;Music,”&#160;in&#160;<i>XML&#160;Con-</i><br/>
<i>ference&#160;hosted&#160;by&#160;IDEAlliance</i>,&#160;2001.<br/>
[80]&#160;E.&#160;Selfridge-Field,&#160;<i>Beyond&#160;MIDI:&#160;The&#160;Handbook&#160;of&#160;Musical&#160;Codes</i>.&#160;MIT&#160;Press,<br/>
1997.<br/>
[81]&#160;“LilyPond.”&#160;<a href="http://www.lilypond.org">http://www.lilypond.org</a>.&#160;[Online;&#160;accessed&#160;2014-05-20].<br/>
59<br/>
<hr>
<A name=72></a><IMG src="thesis-72_1.jpg"/><br/>
<b>Appendix&#160;A</b><br/>
<b>Software&#160;Tools&#160;Used&#160;in&#160;This&#160;Research</b><br/>
This&#160;research&#160;won't&#160;come&#160;into&#160;reality&#160;without&#160;many&#160;free&#160;and&#160;open-source&#160;software<br/>
tools&#160;and&#160;free&#160;resources,&#160;we&#160;will&#160;walk&#160;you&#160;through&#160;a&#160;brief&#160;introduction&#160;to&#160;the&#160;softwares<br/>
we&#160;used&#160;in&#160;this&#160;research.<br/>
<b>Linux&#160;Operating&#160;System</b><br/>
Most&#160;of&#160;the&#160;tools&#160;introduced&#160;below&#160;runs&#160;on&#160;modern&#160;Linux&#160;distributions.&#160;The&#160;distribu-<br/>
tion&#160;we&#160;are&#160;using&#160;is&#160;<b>Linux&#160;Mint&#160;Debian&#160;Edition&#160;(LMDE)</b><a href="">1&#160;</a>(Linux&#160;kernel&#160;3.10),&#160;which&#160;is<br/>
a&#160;user-friendly&#160;Linux&#160;distribution&#160;based&#160;on&#160;Debian&#160;Testing.&#160;User&#160;who&#160;want&#160;to&#160;try&#160;music-<br/>
related&#160;softwares&#160;without&#160;installing&#160;Linux&#160;on&#160;their&#160;harddrive&#160;can&#160;try&#160;<b>64&#160;Studio</b><a href="">2&#160;</a>Linux,<br/>
which&#160;is&#160;a&#160;live&#160;CD&#160;distribution&#160;with&#160;many&#160;music-related&#160;software&#160;pre-installed.&#160;It&#160;also&#160;has<br/>
many&#160;kernel&#160;optimizations&#160;for&#160;real-time&#160;music&#160;manipulation.&#160;<b>Ubuntu&#160;<a href="">Studio</b>3&#160;</a>is&#160;also&#160;an<br/>
option,&#160;which&#160;has&#160;many&#160;pre-installed&#160;music&#160;softwares&#160;and&#160;is&#160;based&#160;on&#160;the&#160;popular&#160;Ubuntu<br/>
Linux.<br/>
Many&#160;Linux&#160;distributions&#160;use&#160;PulseAudio&#160;audio&#160;server&#160;to&#160;manage&#160;audio&#160;device.&#160;But&#160;a<br/>
badly&#160;configured&#160;PulseAudio&#160;server&#160;will&#160;introduce&#160;severe&#160;latency,&#160;which&#160;is&#160;not&#160;acceptable<br/>
while&#160;doing&#160;MIDI&#160;recording.&#160;One&#160;workaround&#160;is&#160;to&#160;remove&#160;PulseAudio&#160;and&#160;use&#160;raw<br/>
ALSA&#160;(Advanced&#160;Linux&#160;Sound&#160;Architecture)&#160;driver&#160;instead.&#160;But&#160;be&#160;careful,&#160;hardware<br/>
1<a href="http://www.linuxmint.com/download_lmde.php">http://www.linuxmint.com/download_lmde.php<br/></a>2<a href="http://www.64studio.com/">http://www.64studio.com/<br/></a>3<a href="http://ubuntustudio.org/">http://ubuntustudio.org/</a><br/>
60<br/>
<hr>
<A name=73></a><IMG src="thesis-73_1.jpg"/><br/>
volume&#160;keys&#160;may&#160;not&#160;work&#160;without&#160;PulseAudio.<br/>
<b>Programming&#160;Languages</b><br/>
<b>Python</b><br/>
Many&#160;researcher&#160;will&#160;choose&#160;Matlab&#160;or&#160;Octave&#160;for&#160;scientific&#160;projects&#160;because&#160;they<br/>
have&#160;many&#160;useful&#160;toolboxes&#160;included.&#160;However,&#160;we&#160;believe&#160;that&#160;research&#160;project&#160;“doesn't<br/>
exist&#160;in&#160;vacuum”.&#160;Drawing&#160;insight&#160;from&#160;the&#160;famous&#160;80-20&#160;rule,&#160;only&#160;20%&#160;of&#160;the&#160;code&#160;are<br/>
actually&#160;doing&#160;the&#160;core&#160;algorithm,&#160;the&#160;rest&#160;80%&#160;are&#160;doing&#160;file&#160;manipulation,&#160;configura-<br/>
tion,&#160;user&#160;interaction,&#160;and&#160;visualization.&#160;Therefore,&#160;choosing&#160;a&#160;powerful&#160;and&#160;easy&#160;to&#160;write<br/>
general-purpose&#160;programming&#160;language&#160;is&#160;extremely&#160;crucial.&#160;<b>Python</b><a href="">4&#160;</a>construct&#160;most&#160;of<br/>
the&#160;infrastructure&#160;code&#160;for&#160;this&#160;project.&#160;Python&#160;is&#160;super&#160;easy&#160;to&#160;code,&#160;and&#160;has&#160;almost&#160;every<br/>
tool&#160;you&#160;need&#160;to&#160;construct&#160;a&#160;fully&#160;functional&#160;experiment&#160;environment.&#160;We&#160;will&#160;highlight<br/>
some&#160;useful&#160;module:<br/>
<b>Music21</b><a href="">5</a><br/>
We&#160;would&#160;like&#160;to&#160;give&#160;special&#160;thanks&#160;to&#160;the&#160;music21&#160;developemnt&#160;team.&#160;Music21<br/>
is&#160;a&#160;Python&#160;toolbox&#160;for&#160;music&#160;notation&#160;manipulation&#160;and&#160;analysis,&#160;developed&#160;by&#160;MIT.<br/>
Music21&#160;can&#160;parse&#160;many&#160;score&#160;notations&#160;like&#160;MusicXML,&#160;<a href="">MIDI6&#160;</a>and&#160;more&#160;into&#160;a&#160;very<br/>
convenient&#160;music21&#160;object&#160;data&#160;structure.&#160;Researcher&#160;can&#160;easily&#160;filter,&#160;split,&#160;search,<br/>
and&#160;transform&#160;music&#160;notations.&#160;There&#160;are&#160;also&#160;many&#160;music&#160;analysis&#160;routines&#160;and&#160;feature<br/>
extractors&#160;included.&#160;If&#160;you&#160;want&#160;to&#160;do&#160;computer&#160;music&#160;research,&#160;music21&#160;is&#160;a&#160;god-sent<br/>
resource.<br/>
4<a href="https://www.python.org/">https://www.python.org/<br/></a>5<a href="http://web.mit.edu/music21/">http://web.mit.edu/music21/<br/></a>6By&#160;default,&#160;music21&#160;will&#160;quantize&#160;MIDI&#160;input,&#160;so&#160;if&#160;you&#160;want&#160;to&#160;import&#160;MIDI&#160;recorded&#160;from&#160;human<br/>
performance,&#160;you&#160;need&#160;to&#160;bypass&#160;the&#160;default&#160;parser&#160;and&#160;manually&#160;disable&#160;the&#160;quantizer.<br/>
61<br/>
<hr>
<A name=74></a><IMG src="thesis-74_1.jpg"/><br/>
<b>SciPy,&#160;NumPy&#160;and&#160;<a href="">Matplotlib</b>7</a><br/>
SciPy&#160;is&#160;a&#160;project&#160;that&#160;contains&#160;many&#160;useful&#160;toolboxes&#160;for&#160;scientific&#160;computation&#160;in<br/>
Python.&#160;The&#160;SciPy&#160;core&#160;library&#160;and&#160;NumPy&#160;provides&#160;numerical&#160;and&#160;vector&#160;calculation&#160;for<br/>
Python,&#160;with&#160;similar&#160;capability&#160;to&#160;Matlab.&#160;Matplotlib&#160;provides&#160;plotting&#160;tools&#160;also&#160;similar<br/>
to&#160;Matlab.&#160;It's&#160;useful&#160;for&#160;small&#160;scale&#160;calculation,&#160;but&#160;heavy&#160;duty&#160;mathematical&#160;calculation,<br/>
we&#160;suggest&#160;R&#160;programming&#160;language,&#160;which&#160;will&#160;be&#160;discussed&#160;in&#160;later&#160;section.<br/>
<b>Simplejson</b><br/>
JSON&#160;(JavaScript&#160;Object&#160;Notation)&#160;is&#160;a&#160;plaintext&#160;data-interchange&#160;format,&#160;similar&#160;to<br/>
XML&#160;but&#160;much&#160;light-weight.&#160;JSON&#160;is&#160;useful&#160;in&#160;experiment&#160;code&#160;for&#160;two&#160;purpose:&#160;first,<br/>
JSON&#160;can&#160;serve&#160;as&#160;configuration&#160;file,&#160;it&#160;easy&#160;to&#160;parse&#160;and&#160;easy&#160;to&#160;edit.&#160;Second,&#160;JSON<br/>
can&#160;serve&#160;as&#160;intermediate&#160;data&#160;file&#160;between&#160;each&#160;experiment&#160;module.&#160;For&#160;example,&#160;we<br/>
use&#160;JSON&#160;to&#160;send&#160;extracted&#160;features&#160;from&#160;feature&#160;extractors&#160;to&#160;the&#160;machine&#160;learning&#160;mod-<br/>
ule.&#160;Although&#160;plaintext&#160;takes&#160;more&#160;storage&#160;than&#160;binary&#160;file,&#160;but&#160;it's&#160;much&#160;easier&#160;to&#160;debug<br/>
because&#160;it's&#160;human&#160;readable.&#160;And&#160;you&#160;can&#160;simply&#160;parse&#160;the&#160;intermediate&#160;values&#160;and&#160;plot<br/>
it&#160;using&#160;other&#160;plotting&#160;program.&#160;Python&#160;provides&#160;build-in&#160;support&#160;for&#160;JSON&#160;format&#160;via<br/>
json&#160;and&#160;simplejson&#160;packages.<br/>
<b>Argparse</b><br/>
Argparse&#160;provides&#160;command&#160;line&#160;argument&#160;parser&#160;for&#160;Python&#160;scripts,&#160;using&#160;com-<br/>
mandline&#160;arguments&#160;with&#160;configuration&#160;file&#160;in&#160;JSON,&#160;you&#160;can&#160;create&#160;very&#160;flexible,&#160;ex-<br/>
tendible&#160;scripts&#160;that&#160;are&#160;easy&#160;to&#160;automate.<br/>
<b>Logging</b><br/>
The&#160;built&#160;in&#160;logging&#160;module&#160;can&#160;print&#160;logging&#160;information&#160;with&#160;predefined&#160;format,<br/>
and&#160;it&#160;supports&#160;log&#160;level.&#160;By&#160;using&#160;log&#160;level,&#160;you&#160;can&#160;print&#160;debug&#160;information&#160;during<br/>
development,&#160;and&#160;hide&#160;all&#160;debug&#160;message&#160;during&#160;production&#160;simply&#160;by&#160;changing&#160;the&#160;log<br/>
7<a href="http://www.scipy.org/">http://www.scipy.org/</a><br/>
62<br/>
<hr>
<A name=75></a><IMG src="thesis-75_1.jpg"/><br/>
level&#160;flag.<br/>
<b>R</b><a href="">8</a><br/>
R&#160;is&#160;a&#160;programming&#160;language&#160;for&#160;statistical&#160;calculation,&#160;but&#160;it&#160;can&#160;also&#160;do&#160;general&#160;pur-<br/>
pose&#160;math&#160;and&#160;plotting&#160;very&#160;well.&#160;R&#160;follows&#160;a&#160;functional&#160;programming&#160;design,&#160;so&#160;it&#160;may<br/>
take&#160;some&#160;time&#160;to&#160;learn&#160;for&#160;people&#160;who&#160;only&#160;have&#160;experience&#160;in&#160;C/C++,&#160;Java&#160;or&#160;other&#160;im-<br/>
perative&#160;and/or&#160;Object-oriented&#160;programming&#160;language.&#160;But&#160;it&#160;is&#160;a&#160;great&#160;tool&#160;for&#160;statistical<br/>
computation,&#160;data&#160;analysis&#160;and&#160;visualization.&#160;We&#160;use&#160;R&#160;for&#160;experiment&#160;data&#160;analysis&#160;and<br/>
for&#160;linear&#160;regression&#160;in&#160;early&#160;version&#160;of&#160;this&#160;research.&#160;R&#160;and&#160;Python&#160;can&#160;work&#160;seamlessly<br/>
through&#160;the&#160;rpy&#160;package.<br/>
<b>Score&#160;Manipulation&#160;and&#160;Corpora</b><br/>
<b>MusicXML&#160;and&#160;MuseScore</b><br/>
<b>MusicXML</b><a href="">9&#160;</a>is&#160;a&#160;digital&#160;score&#160;notation&#160;format&#160;based&#160;on&#160;XML.&#160;It&#160;is&#160;well&#160;supported&#160;in<br/>
most&#160;commercial&#160;music&#160;typesetting&#160;software.&#160;To&#160;view&#160;and&#160;edit&#160;musicXML&#160;score,&#160;we&#160;use<br/>
the&#160;open-source&#160;software&#160;<b>MuseScore</b><a href="">10</a>,&#160;it&#160;provides&#160;basic&#160;editing&#160;capability,&#160;and&#160;can&#160;export<br/>
score&#160;as&#160;PDF.&#160;However,&#160;MuseScore&#160;often&#160;crash&#160;while&#160;loading&#160;bad-formatted&#160;musicXML<br/>
file,&#160;so&#160;sometimes&#160;you&#160;need&#160;to&#160;look&#160;into&#160;it&#160;log&#160;file&#160;and&#160;fix&#160;the&#160;ill-formated&#160;XML&#160;via&#160;a&#160;text<br/>
editor.<br/>
<b>Corpora</b><br/>
Music21&#160;contains&#160;a&#160;corpus<a href="">11</a>,&#160;which&#160;will&#160;be&#160;automatically&#160;installed&#160;if&#160;you&#160;accept&#160;the<br/>
licence&#160;term&#160;during&#160;music21&#160;installation.&#160;It&#160;covers&#160;a&#160;wide&#160;range&#160;of&#160;composers&#160;from&#160;early<br/>
music,&#160;classical&#160;music&#160;to&#160;folk&#160;songs,&#160;with&#160;various&#160;genre&#160;and&#160;musical&#160;style.&#160;Another&#160;public<br/>
8<a href="http://www.r-project.org/">http://www.r-project.org/<br/></a>9<a href="http://www.musicxml.com/">http://www.musicxml.com/</a><br/>
10<a href="http://musescore.org/">http://musescore.org/<br/></a>11<a href="http://web.mit.edu/music21/doc/systemReference/referenceCorpus.html">http://web.mit.edu/music21/doc/systemReference/referenceCorpus.html</a><br/>
63<br/>
<hr>
<A name=76></a><IMG src="thesis-76_1.jpg"/><br/>
available&#160;corpus&#160;is&#160;called&#160;<b>KernScore</b><a href="">12</a>,&#160;which&#160;provides&#160;a&#160;better&#160;search&#160;engine.&#160;You&#160;can<br/>
find&#160;works&#160;by&#160;composer,&#160;genre,&#160;form&#160;or&#160;other&#160;criteria.&#160;There&#160;are&#160;even&#160;a&#160;special&#160;section<br/>
containing&#160;monophonic&#160;works.&#160;Scores&#160;from&#160;both&#160;corpus&#160;can&#160;be&#160;loaded&#160;and&#160;transformed<br/>
in&#160;to&#160;desired&#160;format&#160;via&#160;music21.<br/>
<b>MIDI&#160;Recording</b><br/>
<b>Rosegarden</b><a href="">13&#160;</a>is&#160;a&#160;digital&#160;audio&#160;workstation&#160;(DAW)&#160;software&#160;designed&#160;specifically&#160;for<br/>
MIDI.&#160;It&#160;can&#160;record,&#160;edit,&#160;mix&#160;and&#160;export&#160;MIDI&#160;tracks.&#160;To&#160;actually&#160;hear&#160;the&#160;music,&#160;you<br/>
need&#160;a&#160;MIDI&#160;synthesizer&#160;to&#160;work&#160;with&#160;Rosegarden.&#160;<b>T<a href="">imidity++</b>14&#160;</a>is&#160;built-in&#160;in&#160;many&#160;Linux<br/>
distribution,&#160;and&#160;it&#160;provides&#160;a&#160;commandline&#160;interface&#160;to&#160;synthesize&#160;MIDI&#160;directly&#160;into&#160;a<br/>
WAV&#160;file.&#160;However,&#160;the&#160;default&#160;sound&#160;quality&#160;from&#160;Timidity++&#160;is&#160;not&#160;very&#160;satisfying,&#160;so<br/>
we&#160;suggest&#160;qSynth,&#160;which&#160;is&#160;a&#160;QT&#160;front&#160;end&#160;for&#160;<b>FluidSynth</b><a href="">15</a>.&#160;The&#160;default&#160;soundfont&#160;that<br/>
comes&#160;with&#160;FludiSynth&#160;has&#160;very&#160;good&#160;sound&#160;quality.<br/>
With&#160;all&#160;these&#160;music&#160;software,&#160;it&#160;will&#160;soon&#160;be&#160;very&#160;hard&#160;to&#160;control&#160;the&#160;interconnection<br/>
between&#160;programs.&#160;This&#160;is&#160;when&#160;<a href=""><b>JACK</b>16&#160;</a>comes&#160;to&#160;help.&#160;JACK&#160;is&#160;like&#160;a&#160;virtual&#160;“plug-<br/>
board”&#160;for&#160;software&#160;that&#160;implements&#160;the&#160;JACK&#160;interface.&#160;It&#160;provides&#160;a&#160;central&#160;place&#160;in<br/>
which&#160;you&#160;can&#160;control&#160;how&#160;the&#160;music&#160;data&#160;flows&#160;between&#160;programs&#160;and&#160;hardware.<br/>
<b>Audio&#160;Manipulation</b><br/>
When&#160;MIDI&#160;files&#160;are&#160;synthesized&#160;into&#160;WAV&#160;format,&#160;there&#160;are&#160;many&#160;tools&#160;that&#160;can&#160;help<br/>
editing&#160;them.&#160;The&#160;most&#160;easy&#160;to&#160;use&#160;software&#160;with&#160;GUI&#160;is&#160;<b>Audacity</b><a href="">17</a>,&#160;it&#160;can&#160;edit&#160;and<br/>
mix&#160;audio&#160;tracks.&#160;For&#160;commandline&#160;tools&#160;(in&#160;case&#160;you&#160;need&#160;automation),&#160;<b>oggenc</b><a href="">18</a>(ogg<br/>
12<a href="http://kern.ccarh.org/">http://kern.ccarh.org/<br/></a>13<a href="http://www.rosegardenmusic.com/">http://www.rosegardenmusic.com/<br/></a>14<a href="http://timidity.sourceforge.net/">http://timidity.sourceforge.net/<br/></a>15<a href="http://sourceforge.net/projects/fluidsynth/">http://sourceforge.net/projects/fluidsynth/<br/></a>16<a href="http://jackaudio.org/">http://jackaudio.org/<br/></a>17<a href="http://audacity.sourceforge.net/">http://audacity.sourceforge.net/<br/></a>18<a href="http://www.vorbis.com/">http://www.vorbis.com/</a><br/>
64<br/>
<hr>
<A name=77></a><IMG src="thesis-77_1.jpg"/><br/>
vorbis&#160;encoder),&#160;<b>lame</b><a href="">19(MP320&#160;</a>encoder)&#160;and&#160;<b>FFmpeg</b><a href="">21&#160;</a>are&#160;very&#160;helpful&#160;for&#160;file&#160;format<br/>
transformation.&#160;To&#160;cut&#160;and&#160;combine&#160;audio&#160;tracks&#160;from&#160;commandline,&#160;use&#160;<b>SoX</b><a href="">22</a>.<br/>
<b>Data&#160;Visualization</b><br/>
As&#160;mentioned&#160;before,&#160;R&#160;and&#160;Matplotlib&#160;are&#160;good&#160;candidate&#160;for&#160;visualizing&#160;experi-<br/>
ment&#160;data.&#160;But&#160;if&#160;you&#160;don't&#160;want&#160;to&#160;learn&#160;the&#160;syntax&#160;of&#160;R&#160;or&#160;Python,&#160;you&#160;can&#160;try&#160;<a href=""><b>gnuplot</b>23</a>.<br/>
Gnuplot&#160;is&#160;a&#160;interactive&#160;(and&#160;scriptting)&#160;environment&#160;for&#160;generating&#160;various&#160;types&#160;of&#160;plot<br/>
like&#160;line&#160;plots&#160;or&#160;bar&#160;charts.&#160;It&#160;works&#160;particularly&#160;well&#160;if&#160;you&#160;use&#160;grep&#160;to&#160;extract&#160;data&#160;for<br/>
many&#160;files,&#160;say,&#160;extracting&#160;execution&#160;time&#160;information&#160;from&#160;logs.<br/>
<b>SVMhmm</b><br/>
SVMhmm<a href="">24&#160;</a>is&#160;an&#160;implementation&#160;for&#160;structural&#160;support&#160;vector&#160;machine&#160;with&#160;hidden<br/>
Markov&#160;model&#160;output.&#160;It's&#160;developed&#160;by&#160;Thorsten&#160;Joachims&#160;from&#160;Cornell&#160;University.&#160;It<br/>
is&#160;based&#160;on&#160;SVMstruct,&#160;a&#160;more&#160;general&#160;framework&#160;for&#160;structural&#160;support&#160;vector&#160;machine.<br/>
There&#160;are&#160;many&#160;other&#160;SVMstruct&#160;extensions&#160;such&#160;as&#160;Python&#160;or&#160;Matlab&#160;API.<br/>
<b>Other</b><br/>
Sometimes&#160;the&#160;machine&#160;learning&#160;algorithm&#160;will&#160;run&#160;for&#160;a&#160;very&#160;long&#160;time.&#160;Then&#160;it's<br/>
better&#160;if&#160;you&#160;can&#160;find&#160;a&#160;server&#160;that&#160;runs&#160;24-7&#160;in&#160;your&#160;home&#160;or&#160;laboratory.&#160;You&#160;can&#160;install<br/>
a&#160;<b>ssh&#160;</b>server&#160;on&#160;that&#160;machine,&#160;and&#160;controls&#160;the&#160;experiment&#160;execution&#160;remotely.&#160;However,<br/>
the&#160;experiment&#160;program&#160;will&#160;be&#160;terminated&#160;once&#160;you&#160;log&#160;out&#160;the&#160;ssh&#160;session.&#160;You&#160;can<br/>
run&#160;your&#160;experiment&#160;program&#160;in&#160;<b>tmux</b><a href="">25</a>,&#160;a&#160;terminal&#160;multiplexer,&#160;instead.&#160;It&#160;will&#160;keep&#160;your<br/>
program&#160;running&#160;even&#160;if&#160;you&#160;log&#160;out&#160;of&#160;your&#160;SSH&#160;session.<br/>
19<a href="http://lame.sourceforge.net/">http://lame.sourceforge.net/<br/></a>20Please&#160;consider&#160;open&#160;format&#160;like&#160;ogg&#160;first,&#160;MP3&#160;is&#160;a&#160;closed&#160;format&#160;and&#160;may&#160;have&#160;patent&#160;issues.<br/>21<a href="http://www.ffmpeg.org/">http://www.ffmpeg.org/<br/></a>22<a href="http://sox.sourceforge.net/">http://sox.sourceforge.net/<br/></a>23<a href="http://www.gnuplot.info/">http://www.gnuplot.info/<br/></a>24<a href="http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html">http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html<br/></a>25<a href="http://tmux.sourceforge.net/">http://tmux.sourceforge.net/</a><br/>
65<br/>
<hr>
<A name=78></a><IMG src="thesis-78_1.jpg"/><br/>
Modern&#160;machines&#160;often&#160;have&#160;multi-core&#160;CPUs.&#160;But&#160;if&#160;your&#160;program&#160;only&#160;runs&#160;in&#160;one<br/>
core,&#160;you&#160;waste&#160;the&#160;CPU&#160;resources&#160;and&#160;also&#160;your&#160;time.&#160;<b>Gnu-parallel</b><a href="">26&#160;</a>can&#160;dispatch&#160;mul-<br/>
tiple&#160;instances&#160;of&#160;your&#160;script&#160;or&#160;program&#160;to&#160;each&#160;core.&#160;It&#160;will&#160;automatically&#160;find&#160;new&#160;job<br/>
to&#160;run&#160;when&#160;the&#160;previous&#160;one&#160;is&#160;finished,&#160;so&#160;the&#160;CPU&#160;will&#160;always&#160;run&#160;on&#160;its&#160;full&#160;capacity.<br/>
Finally,&#160;We&#160;use&#160;<b>git</b><a href="">27&#160;</a>for&#160;version&#160;control&#160;(including&#160;code&#160;and&#160;document).&#160;And&#160;LATEX<a href="">28</a><br/>
is&#160;used&#160;to&#160;typeset&#160;this&#160;document.<br/>
<b>Summary</b><br/>
We&#160;have&#160;reviewed&#160;many&#160;software&#160;tools&#160;used&#160;to&#160;construct&#160;this&#160;research.&#160;We&#160;want&#160;to<br/>
emphasize&#160;that&#160;it&#160;is&#160;totally&#160;possible&#160;to&#160;use&#160;<i>only&#160;</i>free&#160;and&#160;open-source&#160;software&#160;to&#160;do&#160;all<br/>
these&#160;heavy&#160;lifiting.&#160;We&#160;encourge&#160;the&#160;reader&#160;to&#160;try&#160;these&#160;tools&#160;out,&#160;spread&#160;the&#160;words&#160;and<br/>
even&#160;contribute&#160;to&#160;these&#160;projects.&#160;By&#160;doing&#160;so&#160;we&#160;can&#160;create&#160;a&#160;more&#160;friendly&#160;scientific<br/>
computing&#160;community&#160;and&#160;make&#160;the&#160;world&#160;a&#160;better&#160;place.<br/>
26<a href="http://www.gnu.org/software/parallel/">http://www.gnu.org/software/parallel/<br/></a>27<a href="http://git-scm.com/">http://git-scm.com/<br/></a>28<a href="http://latex-project.org/">http://latex-project.org/</a><br/>
66<br/>
<hr>
<A name="outline"></a><h1>Document Outline</h1>
<ul>
<li><A href="thesiss.html#1">口試委員會審定書</A></li>
<li><A href="thesiss.html#4">致謝</A></li>
<li><A href="thesiss.html#5">中文摘要</A></li>
<li><A href="thesiss.html#6">Abstract</A></li>
<li><A href="thesiss.html#7">Table of Contents</A></li>
<li><A href="thesiss.html#10">List of Figures</A></li>
<li><A href="thesiss.html#12">List of Tables</A></li>
<li><A href="thesiss.html#13">Introduction</A>
<ul>
<li><A href="thesiss.html#13">Motivation</A></li>
<li><A href="thesiss.html#14">Goal and Contribution</A></li>
<li><A href="thesiss.html#14">Chapter Organization</A></li>
</ul>
</li>
<li><A href="thesiss.html#15">Previous Works</A>
<ul>
<li><A href="thesiss.html#15">Various Goals and Evaluation</A></li>
<li><A href="thesiss.html#17">Researches Classified by Methods Used</A></li>
<li><A href="thesiss.html#19">Additional Specialties</A></li>
</ul>
</li>
<li><A href="thesiss.html#21">Proposed Method</A>
<ul>
<li><A href="thesiss.html#21">Overview</A></li>
<li><A href="thesiss.html#22">A Brief Introduction to SVM-HMM</A></li>
<li><A href="thesiss.html#27">Learning Performance Knowledge</A>
<ul>
<li><A href="thesiss.html#28">Training Sample Loader</A></li>
<li><A href="thesiss.html#28">Features Extraction</A></li>
<li><A href="thesiss.html#29">SVM-HMM Learning</A></li>
</ul>
</li>
<li><A href="thesiss.html#31">Performing Expressively</A>
<ul>
<li><A href="thesiss.html#32">SVM-HMM Generation</A></li>
<li><A href="thesiss.html#32">MIDI Generation and Synthesis</A></li>
</ul>
</li>
<li><A href="thesiss.html#33">Features</A>
<ul>
<li><A href="thesiss.html#33">Score Features</A></li>
<li><A href="thesiss.html#35">Performance Features</A></li>
<li><A href="thesiss.html#36">Normalizing Onset Deviation</A></li>
</ul>
</li>
</ul>
</li>
<li><A href="thesiss.html#38">Corpus Preparation</A>
<ul>
<li><A href="thesiss.html#38">Existing Corpora</A></li>
<li><A href="thesiss.html#39">Corpus Specification</A></li>
<li><A href="thesiss.html#42">Implementation</A>
<ul>
<li><A href="thesiss.html#42">Score Preparation</A></li>
<li><A href="thesiss.html#42">MIDI Recording</A></li>
<li><A href="thesiss.html#43">MIDI Cleaning and Phrase Splitting</A></li>
</ul>
</li>
<li><A href="thesiss.html#43">Results</A></li>
</ul>
</li>
<li><A href="thesiss.html#48">Experiments</A>
<ul>
<li><A href="thesiss.html#48">Onset Deviation Normalization</A></li>
<li><A href="thesiss.html#52">Parameter Selection</A>
<ul>
<li><A href="thesiss.html#52">SVM-HMM-related Parameters</A></li>
<li><A href="thesiss.html#54">Quantization Parameter</A></li>
</ul>
</li>
<li><A href="thesiss.html#56">Human-like Performance</A></li>
</ul>
</li>
<li><A href="thesiss.html#62">Conclusions</A></li>
<li><A href="thesiss.html#62">Bibliography</A></li>
<li><A href="thesiss.html#72">Software Tools Used in This Research</A></li>
</ul>
<hr>
</BODY>
</HTML>
