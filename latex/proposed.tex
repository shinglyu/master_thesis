\chapter{Proposed Method}
\label{chap:proposed}
\section{Overview}
   %input output only melodic constrains
   %flow chart
      \begin{figure*}[tp]
         \begin{center}
            \includegraphics[width=\textwidth]{fig/high_lev_arch}
         \end{center}
         \caption{High-level system architecture} 
         \label{fig:flow}
      \end{figure*}
The high-level architecture of the purposed system is shown in Fig. \ref{fig:flow}. The system has two phases, the upper half of the figure is the learning phase, the lower half is the performing phase. In the learning phase, score and expressive human recording pairs, split into phrases by human, are used as training examples for structural support vector machine with hidden Markov model output (SVM-HMM) algorithm to learn performance knowledge model. In the performing phase, a score will be given to the system for expressive performance. The SVM-HMM generation module will use the performance knowledge learned in the previous phase to produce expressive performance. The SVM-HMM output then go through a MIDI generator and MIDI synthesizer to produce audible performance.
%TODO: refer to feature exction and corpus chapter

All the scores and recordings are monophonic and contains only one musical phrase. The phrasing is done by human, thus the system is semi-automatic. The learning algorithm, namely SVM-HMM, can only perform off-line learning, so the learning phase can only work in a non-realtime scenario. The generating phase can work much faster, expressive music can be generated almost instantaneously. 

There are many ways the user can control the performance style of the final output: first, the user can choose the training corpus. Theoratically, a model of a particular style can be learned from a set of samples with that particular style. Second, the user can control the structural expression by assigning the phrasing.


In the following sections, we will give an overview of the theroratical background behind SVM-HMM, and then walk through the detail steps in the learning and performing phases, and some implementation detail. The features used will be presented in the end of this chapter.


%TODO: START COPY-PASTE
\input{theoraticalbg}
\section{Learning Performance Knowledge}
\label{sec:learn}
\begin{figure*}[tp]
   \begin{center}
      \includegraphics[width=\textwidth]{fig/learn_arch}
   \end{center}
   \caption{Learning phase flow chart} 
   \label{fig:learnflow}
\end{figure*}
In this section, we will introduce the componants that consist the learning phase.
The main goal in the learning phase is to extract performance knowledge from training samples. Fig. \ref{fig:learnflow} shows the internal structure of the learning phase.

Training samples are matched score and expressive performance pairs (their format and preparation process is discussed in Chapter \ref{chap:corpus}). The raw data from the samples are too complex to process, so we need to extract important features from them. Two types of features will be extracted from the samples: first, the musicological cues from the scores are called score features; second, the measurable expression from the expressive performances are called the performance features. We want the system to learn how score features are \enquote{translated} into performeance features. This process can be analogize to a human performer reading the explicit and implicit cues from the score, and perform the music with certain expressive expression. The definition of the features used will be presented in Section \ref{sec:features}.


% The learning module has a input/output interface that is independent of the underlying algorithm, so different algorithm can be implemented without changing the overall structure of the system.

%TODO: sample loader
\subsection{Training Sample Loader}
   The training samples are loaded by the sample loader module. Since a training sample  consists of a score (musicXML format) and an expressive recording (MIDI format), the sample loader finds the two files, and load them into an intermediate representation (\texttt{music21.Stream} object provided by the \texttt{music21} library \cite{music21} from MIT). The music21 library will convert the musicXML and MIDI format into a Python Object hierarchy that is easy to access and manipulate by Python code. 

   One caveat here is the music21 library will quantize the time in MIDI, which will destroy the subtle onset and duration expressions. And the music21 library don't handle the \enquote{ticks per quarter note} information in the MIDI header \cite{midispec}, which is essential for the MIDI parser to interprete the correct time scale. So we must explicitly disable quantization and specify the \enquote{ticks per quarter note} value during MIDI loading.

\subsection{Features Extraction}
In order to keep the system architecture simple, feature extractors are designed to be independent to other feature extractors, so features can included or removed without affecting the rest of the system. Furthermore, this enables parallel feature extraction. But sometimes a feature inevitably depends on other features, for example, the \enquote{relative duration with the previous note} is calculated based on the \enquote{duration} feature. Since we want to avoid complex dependency management, the \enquote{relative duration with the previous note} feature extractor has to invoke the \enquote{duration} extractor, instead of waiting for the \enquote{duration} extractor to finish first. Therefore, the \enquote{duration} feature extracted will be computed twice. To avoid redundant computation of the feature extractors, we implemented a caching mechanism. Once the \enquote{duration} feature had been computed, no matter it is calculated during \enquote{duration} extraction or or during \enquote{relative dutaion with the previous note} extraction process, it's value will be cached during this execution session. So no matter how many feature extractors uses the \enquote{duration} feature, they can get the value directly from cache. This method can speed up the execution without needing to handling dependencies.

   The extracted features are aggregated and stored into a JavaScript Object Notation (JSON) file for the SVM-HMM module to load. By saving the features in a human-readable intermediate file, we can debug potential problems easily.%the learning module can then load this file as inpu.t. The learning algorithm can then do any pre-processing on the features, such as aggregation or quantization. The output of this module is the algorithm specific model description. For example, a linear regression algorithm will output the regression parameters. The algorithm is required to produce a model file containing the model description, but the system doesn't care about the internal format of the model description file, it will simply feed this model file to the generation module in the generation stage. So the developer of the learning module has to implement methods to write and read the model file themselves.

   %In the early stage of this research, linear regression is used. The results of linear regression is shown in \cite{Lyu2012}. In this thesis, Structural Support Vector Machin \cite{Joachims2009} is used instead. The detail of Structural SVM will be in the next Chapter.

\subsection{SVM-HMM Learning}
After all features are extracted, the next step is to learn performance knowledge from the features. In the early stage of this research, we have successfully applied linear regressio \cite{Lyu2012}. However, assuming this problem to be linear is clearly an oversimplification, so we switch to structural support vector machine with hidden Markov model output (SVM-HMM)\cite{svm2009, svm2005, svm2003} as our supervised learning algorithm. 

The SVM-HMM learning module loads the feature file from the previous stage, and aggregate the features to fit the required input format of the SVM-HMM learner program. However, most features from the previous stage are real values, but SVM-HMM only takes discrete performance features\footnote{SVM-HMM is initially designed for tasks like part-of-speech tagging, in which real value or binary features are used to predict discrete part-of-speech tags.}, so quantization is required. There are many possible way to quantize the features, each will result in different output, here we will present a quantizer design for demonstration purpose: for each performance feature, the mean and standard deviation from all training samples are calculated first. The range between mean minus or plus four standard deviations is divided into 128 uniform intervals. Values over than mean plus four standard deviations are quantized into the 128th bin, and values below mean minus four standard deviations are quantized into the 1st bin. The number of intervals decides how fine-grain the quantization is, if the number is too low, subtle expressions will be lost due to high quantization error. However, if the number is too large, there will be too few samples for each interval, which is bad from a statistical learning perspective. Also the training process will take a lot of CPU and memory resources without significant gain in prediction accuracy. The range of four standard deviation is chosen by trail and error, a narrower range will make most of the extreme values be quantized into the largest of smallest bin, so the performance will have a lot of saturated values. But a very large range will make the interval between each quantization bin too large, rising the quantization error. %Feature values below mean minus three standard deviations are quantized to the lowest bin, while values larger than mean plus three standard deviations are quantized to the highest bin. The range of three standard deviation and 1024 intervals are chosen by experience, which can be modified to fit different corpus. The mean, standard deviation and number of intervals information are stored in a file for the performing phase to dequantize the output.

The theoretical background of SVM-HMM is already mentioned in Section \ref{sec:svm-hmm}. We leverage Thorsten Joachims's implementation called $SVM^{hmm}$ \cite{Joachims2008}. $SVM^{hmm}$ is an implementation of structural SVMs for sequence tagging \cite{svm2003} using the training algorithm described in \cite{svm2005} and \cite{svm2009}. The $SVM^{hmm}$ package contains a SVM-HMM training program called \texttt{svm\_hmm\_learn} and a prediction program called \texttt{svm\_hmm\_classify}. For architectural simplicity, we train one model for each performance feature, each model uses all the score features to predict a single performance feature. The \texttt{svm\_hmm\_learn} read the features from a file  in the following format:
Each line represents features for a note in time order, format as
\begin{lstlisting}[style=nonumbers]
	PERF qid:EXNUM FEAT1:FEAT1_VAL FEAT2:FEAT2_VAL ... #comment
\end{lstlisting}
\texttt{PERF} is a quantized performance feature. The \texttt{EXNUM} after \texttt{qid:} identifies the phrases, all notes in a phrase will have the same \texttt{qid:EXNUM} identifier. Following the identifier are quantized score features, denote as \texttt{feature name : feature value}, separated by spaces. And text following a \texttt{\#} symbol is comment. %An example of the training file is shown in Fig. \ref{fig:expinput}

%\framebox{TODO: partial model}

%\begin{figure*}[tp]
%   \begin{center}
%      \includegraphics[width=\textwidth]{fig/TBDFigure}
%   \end{center}
%   \caption{Example input file} 
%   \label{fig:expinput}
%\end{figure*}

There are some key parameters needed to be adjusted for the training program. First the $C$ parameter in SVM, which controls the trade-off between lowering training error and maximizing margin. Larger C will result in lower training error, but the margin may be smaller. Second, the $\varepsilon$ parameter controls the required precision for termination. The smaller the $\varepsilon$, the higher precision, but it may require more time and computing resource. Finally, for the HMM part of the model, the order of dependencies of transition states and emission states needs to be specified. In our case, both are set to defaults: transition dependency is set to one, which stands for first-order Markov property, and emission dependency is set to zero. Since we train one models for each performance feature, each model can have their own set of parameters. The parameter selection experiments will be presented in Chapter \ref{chap:exp}.

Finally, the training program will output three model files (because we use three performance features) which contains SVM-HMM model parameters, such as the support vectors and other metadata. Since it takes considerable time (roughly a dozen minutes to a few hours) to train a model, depending on the amount of training samples and the power of the computer, the system can only support off-line learning. But the learning process only need to be run once. The performance knowledge model can be reused over and over again in the performing phase.



\section{Performing Expressively}
   \begin{figure*}[tp]
      \begin{center}
         \includegraphics[width=\textwidth]{fig/perf_arch}
      \end{center}
      \caption{Performing phase flow chart} 
      \label{fig:perfflow}
   \end{figure*}
The performing phase uses the performance knowledge model learned in the previous phase to generate expressive performances. The input is a score file to be performed, which should not be used as training sample to prevent overfitting. Score features will be extracted from it using the same routine as in the learning phase. The SVM-HMM generation module will use the learned model and the score features to predict the performance features. These features will than be de-quantized back to real values using the method described previously. An MIDI generation module will apply those performance features onto the score to produce a expressive MIDI file. The MIDI file itself is already a expressive performance, in order to listen to the sound, an software synthesizer can be used to render the MIDI file into WAV or MP3 format.
\subsection{SVM-HMM Generation}
The feature extraction and aggregation process in the performing phase is similar to the learning phase, but the \texttt{PERF} fields in the SVM-HMM input file are left blank for the algorithm to predict. The \texttt{svm\_hmm\_classify} program will take these inputs with the learned model file and predict the quantized labels of the performance features. These performance features are de-quantized back to the middle point of each bin. 
%If we already know the real performance feature value, say, we use part of the corpus as training set and the rest as testing set, the \texttt{PERF} can be set to the \enquote{answer} of the testing set, so the \texttt{svm\_hmm\_classify} program will calculate the accuracy of the prediction. 

      
\subsection{MIDI Generation and Synthesis}
  %Since the output of the SVM-HMM learner is the quantized label for the performance feature, dequantization is required to turn those values back to real-valued performance features. The dequantizer will load the quantization parameters from the learning stage to understand the range and intervals used in the quantizer. Each quantization label is dequantized into the mean value of the interval it belongs.

The predicted performance features are then applied onto the input score, i.e. the onset timings will be shifted, the duration extended or shortened, and the loudness shifted according to the predicted performance features. The resulting expressive performance will be transfromed into MIDI files using \texttt{music21} library \cite{music21}.%For example, if a performance feature represents the note's duration should last for 1.2 times of its nominal value, the duration in the score is multiplied by 1.2. After all the performance features are applied, the expressive version of the score is stored in MIDI format using the \texttt{music21} library.

%\framebox{TODO:Dramatization/post processing}
%\subsection{Audio Synthesis}
In order to actually hear the expressive performance, the MIDI file can be rendered by a software MIDI synthesizer. %Since the output is standard MIDI file, the user can choose any compatible software or hardware synthesizer. 
For example, \texttt{timidity++} software synthesizer for Linux can render the MIDI into a WAV (Waveform Audio Format) file, which can be compressed into MP3 (MPEG-2 Audio Layer III) by \texttt{lame} audio encoder. Alternatively, one can use hardware synthesizers, for example, RenCon \cite{RenCon} contest uses Yamaha Disklavier digital piano to render contestants' submission.

Because sub-note level expression is not the primary goal of this research, we choose standard MIDI grand piano sound to render the music. The system can be extended to used more advanced physical model or instrument-specific audio synthesizer. Some sub-note level features, such as special techniques for violins, can be added to the features list and be learned by the SVM-HMM model.
   
%\framebox{TODO: phrase concatenation}
%\framebox{REVIEW1}
\section{Features}\label{sec:features}

   %The system is trying to mimic the process of human performance: the musican reads the explict and implict cues from the score and transform them into musical expressions. So the features can be categorized into two category: score features and performance features. Score features are information contained in the score. Performance features corresponds to the musical expression. The basic time unit for both features are a note. 
   As mentioned in Section \ref{sec:learn}, there are two types of features, score features and performance features. We will present the features used in the system, and discuss the difficulties encountered.
\subsection{Score Features}
      Score features are musicological cues presented in the score. The purpose of score features are to simulate the high level information a performer may perceive when he/she reads the score. The basic time unit for these features are notes. Each note will have all features presented below.
      Score features includes:
      \begin{description}
         \item [Relative position in the phrase:]
            The relative position of a note in the phrase, its value ranges from 0\% to 100\%. %Since there are often salient musical expressions during the opening or closing of a phrase, this feature is used to capture the start or end of the phrase.
            This feature is intended to capture the special expression in the start or the end of a phrase, or time-variant expression like arch-type loudness variation.
         \item [Pitch:]
            The pitch of a note denoted by MIDI pitch number (resolution is down to semitone). %For a phrase of $n$ notes with pitch $P_1, P_2, \dots, P_n$, $$RP = \frac{P_i -min(P_1, P_2, \dots, P_n) }{max(P_1, P_2, \dots, P_n)-min(P_1, P_2, \dots, P_n) }$$  Where $P_i$ is the pitch of note at position $t$.

         \item [Interval from the previous note:] The interval between the current note and its previous note (in semitone). This represents the direction of the melodic line.See Fig. \ref{fig:interval} for example. $$\Delta P^- = P_{i} - P_{i-1} $$ 
         \item [Interval to the next note:] The interval between the current note and its previous note (in semitone). See Fig. \ref{fig:interval} for example. $$\Delta P^+ = P_{i+1} - P_i$$ 
         
      \begin{figure}[tp]
         \begin{center}
            \includegraphics[width=0.4\textwidth]{fig/interval_arrow}
         \end{center}
         \caption{Interval from/to neighbor notes}
         \label{fig:interval}
      \end{figure}

         
         \item [Note duration:] The duration of a note (quarter notes). 

            Grace notes have no duration in musicXML specification \cite{midispec}. The reason for this is that grace notes are considered very short ornaments that does not occupy real beat position. But zero duration is hard to handle in math formulation. So we assigned a duration of a sixty-fourth note, because it's far shorter than all the notes in our corpus.
%REVISE DONE TO HERE
         \item [Relative Duration with the previous note:] The duration of a note divided by the duration of its previous note. See Fig. \ref{fig:duration} for example.
For a phrase of $n$ notes with duration $D_1, D_2, \dots, D_n$, $$RD^- = \frac{D_i}{D_{i-1}} $$             This feature is intended to locate local changes in tempo, such as a series of rapid consecutive notes followed by a long note, which will cause a discontinuity in this feature.
         \item [Relative duration with the next note:] The duration of a note divided by duration of its next note. See Fig. \ref{fig:duration} for example.
$$RD^+ = \frac{D_i}{D_{i+1}} $$ 

      \begin{figure}[tp]
         \begin{center}
            \includegraphics[width=0.4\textwidth]{fig/duration}
         \end{center}
         \caption{Relative duration with the previous/next note}
         \label{fig:duration}
      \end{figure}
   \item [Metric position:] The position (beat) of a note in a measure. For example, under a time signature of $^4_4$, if a measure consists of five notes, they will have metric position of 1, 2, 2.5, 3 and 4, respectively. 
      
      Metric position usually implies beat strength. In most tonal music, there exist a hierarchy of beat strength. For example, for a time signature of $^4_4$, the first note is usually the strongest, the third note is the second strongest, and the second and fourth notes are the least strong. %\cite?

   \begin{figure}[tp]
      \begin{center}
         \includegraphics[width=0.4\textwidth]{fig/metrical}
      \end{center}
      \caption{Metric position}
      \label{fig:metrical}
   \end{figure}
      \end{description}

      %TODO: link to narmour group
%   \begin{figure*}[tp]
%      \begin{center}
%         \includegraphics[width=0.8\textwidth]{fig/TBDFigure}
%      \end{center}
%      \caption{Example Score Features}
%      \label{fig:expScoreFeat}
%   \end{figure*}



\subsection{Performance Features}
   Performance features are the expressive expressions we would like to learn from a performance. Performance features are extracted by calculating how the expression deviates from the nominal notation in the score.
      Performance features includes:
      \begin{description}
         \item [Onset time deviation:] 
            A human performer usually adds conscious or unconsious rubato ot their performance. The onset time deviation is the difference of onset timing between the performance and the score. Namely,
            $$ \Delta O = O_i^{perf} - O_i^{score} $$ Where $O_i^{perf}$ is the onset time of note $i$ in the performance, $O_i^{score}$ is the onset time of note $i$ in the score. 

            However, the above formula assumes the performance is played at the exact same tempo assigned by the score. However, performers can't always keep up with the speed of the score because of limited piano skill, or they may speed up or slow down certain sections as expression. Therefore, the performance should be linearly scaled to avoid systematic bias, We will present a solution to this issue in Section \ref{sec:normalize}.
         \item [Loudness:] The loudness of a note. Measured by MIDI velocity level 0 to 127.

         \item [Relative duration:]
            The performed duration of a note divided by the nominal duration on the score.
            $$ RD = \frac{ D_i^{perf}}{D_i^{score}}$$
      \end{description}

%\begin{figure*}[tp]
%   \begin{center}
%      %TODO:Fig.:Example JSON code
%      \includegraphics[width=\textwidth]{fig/TBDFigure}
%   \end{center}
%   \caption{Example Performance Features}
%   \label{fig:expPerfFeat}
%\end{figure*}
   % \section{Melodic Similarity and Sample Selection}
   % Once a score is given to the system for playing, all the samples in the database will be ranked by the melodic similarity with the given score. Here we use the melodic distance function provided by the MIDI Toolbo \cite{Eerola2004}, which is defined as follows: 
   % \begin{enumerate}
   %    \item Melodic contour is calculated by connecting each note's pitch, forming a piece-wise linear contour.
   %    \item Subtract the contour by it's mean to preserve only the relative part.
   %    \item If the two phrases has different length, re-sample both phrases with fixed intervals so both of the phrase will have contour vector of the same length.
   %    \item The L1 norm (a.k.a Taxicab distance) of these two contour vector is the similarity measure. 
   % \end{enumerate}
   % The reason I choose melodic contour is because it yields best results in finding melodic similarity, which is shown in \cite{Hoffmann-engl2005}.

      %TODO: not included features: e.g. notation
   \subsection{Normalizing Onset Deviation}
   \label{sec:normalize}
\begin{figure}[tp]
   \begin{center}
      %TODO:Fig.:Normalization Schemes
      \includegraphics[width=0.8\textwidth]{fig/prob_onset_diff}

   \end{center}
   \caption{Systematic bias in onset deviation }
   \label{fig:normalizationprob}
\end{figure}
%\begin{figure}[tp]
%\begin{center}
%%TODO:Fig.:Normalization Schemes
%\includegraphics[width=\textwidth]{fig/onsetdifffix}
%
%\end{center}
%\caption{Normalization Schemes}
%\label{fig:normalization}
%\end{figure}
In the previous section, we mentioned that the onset deviation feature will have problems when the performer did not play at the exact tempo indicated by the score. As illustrated in Fig. \ref{fig:normalizationprob}, if the performance is played slower than expected, the deviation will grow larger and larger over time same, and vice versa is it's played faster. The systematic bias caused by the difference in total duration will mix up with the local deviation, For a long phrase, the onset deviation of the last notes can be as larger as a dozen quarter notes. These kind of extremely large values will be learned by the model and cause erroneous predictions. A note may be delayed for a few quarter notes, causing it the notes to be played in the wrong order.
 
In other words, the onset deviation actually contains two type of deviation: a global/systematic deviation cause by the difference between performed and nominal tempo, and a local deviation cause by note-level expression. Since the intention of the onset deviation feature is to capture the note-level expression, the performance must be linearly scaled to cancel out the global deviation.

%The relative onset deviation feature must be normalized to get meaningful result. For example, a phrase is played very fast by the performer, say, the played length is only 70\% of the notation. If the first note is aligned, the onset timing bias will grow linearly until the end of the phrase, the last note will have a onset timing bias of about 30\% of the phrase. But from the definition of this feature, the timing bias should be roughly less than half of  a note's length, and it's mean should be around zero. This 30\% value will introduce a relative large value in the training corpus, and thus there may be very large output value from the generation phase. If the model predict that a note should be played early or later for 30\% of the phrase length, the melody will be messed up. 

   Initially, we tried two possible way of normalization: 
   \begin{enumerate}
      \item Align the onset of the first notes, and align the onset of the last notes.
      \item Align the onset of the first notes, and align the end (MIDI note-off event) of the last notes.
      %\item Don't align the onset of the first notes, align the onset of the last notes
      %\item Don't align the onset of the first notes, align the end of the last notes
   \end{enumerate}
    %The incentive for not aligning the first note is that the performer may intend to use an early start or delayed start as an expression, if the first note is aligned by it's onset, the first note in every phrase will have a onset timing bias feature of value zero. In other words, the early/delayed start expression is lost. %But each normalization method are equally reasonable theoretically, so we need to use empirical data to verify them. The experiment is explained in section \ref{TODO:experiment}. The experiment result showed that [TODO: result]
   However, neither of the method can robustly eliminate extreme values.  Therefore, we proposed an automated approach to find the best scaling ratio such that the normalized onset deviations in the performances fits best with those in the score. The measure of fitting is defined as the Euclidean distance between the normalized performance onset sequences and the score onset sequences, represented as vectors. %Note that the two vectors must have the same size, because the recordings are required to match note-to-note with the score. 
 Brent's Method \cite{brent1973} is used to find this optimal ratio. To speed up the optimization and prevent unreasonable local minima value, a search range of $[initial\ guess \times 0.5 , initial\ guess \times 2]$ is imposed on the optimizer. The $initial\ guess$ is used as a rough estimate of the ratio, calculated by aligning the first and last onsets. Than we assume the actual ratio will not be smaller than half of $initial\ guess$ and not larger than twice of $initial\ guess$. The two numbers 0.5 and 2 are chosen by trail and error, and most of the empirical data supports this decision. We will demonstrate the effectiveness of this solution in Section \ref{sec:onsetnormexp}.

   %So we have defined a automatic method to dynamically adjust the normalization ratio to eliminate systematical error in the onset deviation feature. Comparing this method to not using any normalization (see Fig. \ref{fig:afternorm}), the method can produce very low onset deviations, roughly centered around zero, while the result from not using any normalization will show a clear trend of increasing deviations.
 

